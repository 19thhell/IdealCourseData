[{"detail": [{"content": "High Performance Computing at NYU - High Performance Computing at NYU - NYU Wikis                                                                                                                                                                                                                                                                                                                                                                            Skip to content    Skip to breadcrumbs    Skip to header menu    Skip to action menu    Skip to quick search                                                                    Spaces                                      Create                          Quick Search                     Help                                                            Online Help                             Keyboard Shortcuts                            Feed Builder                            What\u2019s new                             Available Gadgets                            About Confluence                                                                                    Log in                                                      High Performance Computing at NYU    Pages  Blog    Child pages   Pages   High Performance Computing at NYU   About NYU HPC  Accounts  The Clusters   Scheduler  Using the Clusters  Bowery and scratch Maintenance Downtime  Status of HPC  scratch Automated Purge Policy  archive Downtime  /scratch Disk Upgrade  Bastion Host Unavailability  Clusters and Storage  Accessing software via Environment Modules  Account Eligibility  Compute and Storage Facilities  File permissions and Access Control Lists  HPC at NYU - Access  FAQ  HPC at NYU - Front page content  HPC Stakeholders  Interpreting pbstop  Logging in to the NYU HPC Clusters  Logging in from a Windows workstation  HPC Newsletters  Quick Links  Research Gallery  Running jobs on the NYU HPC clusters  Sharing files on the NYU HPC clusters  Storage  Transferring files to and from the HPC clusters  Wiki Map  Tutorials  Keeping directories in sync with rsync  Lustre Migration Updates  Newsletter Figures  Upcoming HPC Classes  Compiling your own software  Content - Transferring files to and from the HPC clusters  35 more child pages    Browse pages  Configure Space tools                                                                     Tools                                                     A t tachments (1)                                           Page History                                           Restrictions                                                               Page Information                                           Link to this Page\u2026                                           View in Hierarchy                                           View Source                                           Export to PDF                                           Export to Word                                                                                                    Pages                              Skip to end of banner                  JIRA links         Go to start of banner                          High Performance Computing at NYU                                                                                                                                                                 Skip to end of metadata                                         Created by  Meredith B. Rendall , last modified by  Stephen Leak on Apr 22, 2015                  Go to start of metadata                                 Upcoming HPC Events            Icon                Looking for one-on-one HPC help ? Email us to book an HPC Consultation                       Icon                Spotted any errors or omissions in our new Wiki? Tell us about it!             Upcoming HPC classes (details here )                                         Getting started in the NYU HPC environment                                                                     Using NYU HPC Effectively                                                Quick Links    Mercer downtime April 20-22  HPC at NYU  HPC Newsletter   Getting started on Mercer   Logging in   Windows   Mac / Linux   Clusters and Storage   Transferring data to/from the clusters   Writing and submitting jobs with qsub   Available software   Building Software   Tutorials  Research Gallery  FAQs   NYU HPC Cheat Sheet   Wiki Map  (Tip: click &quot;&lt;&lt;&quot; at bottom left to close Confluence sidebar)    \u00a0 \u00a0         Mercer is (mostly) back          Icon                Mercer /scratch update is complete, but some data is still being migrated. Some nodes, including most of the 20-core nodes, are still being used to complete the data migration. If your data is not fully migrated and verified you will see a notice on login, please read it and contact hpc@nyu.edu . During the migration we are checking that your $SCRATCH files are replicated on the new $SCRATCH filesystem, however you should also verify for yourself that all data is correct. The old filesystem is being kept offline and read-only, so we can help recover files from there if necessary Over the next several days migration will be completed and more nodes will be returned to the cluster. If you experience any errors, please let us know Thanks for your patience during this major upgrade HPC Team               Welcome to High Performance Computing (HPC) at New York University.\u00a0 NYU HPC, within ITS, operates and supports high performance computing resources and assists the NYU research community in their use. HPC resources are open to NYU faculty and staff, and faculty-sponsored students, and may be used for class instruction . ITS is also available to partner with faculty as they seek funding for research with substantial technology components - see HPC Stakeholders and also ITS Research Computing . We can also assist in access to and collaboration with a number of national and state facilities .      Getting and Renewing Access        Click here to expand...               Icon                For how to log in, see Logging in to the NYU HPC Clusters                       Icon                Please do not submit HPC account requests or renewals during the downtime on April 20-22. Account creation will be unavailable during this time.             Who is eligible for an HPC account? NYU HPC resources are available at no charge to full-time NYU faculty (other than NYU Medical School) and to all other NYU staff and students with full-time NYU faculty sponsorship (more...) Getting an account on the NYU HPC clusters First you need a valid NYU NetID. Your HPC sponsor can request one for you\u00a0 here . You also need a valid NYU Google account to receive emails, as does your HPC sponsor -\u00a0 contact us \u00a0if you need assistance with this. Next you need a\u00a0 faculty sponsor . Finally, log into the\u00a0 NYU Identity Management service \u00a0and follow the link to &quot;Request HPC account&quot;. Renewing your HPC account Each year, non-faculty users must renew their HPC account by filling in the account renewal form from the\u00a0 NYU Identity Management service . See \u00a0 Renewing your HPC account with IIQ \u00a0for a walk-through of the process. Information for faculty who sponsor HPC users You can request a NetID for your student or collaborator\u00a0 here . The request form has additional information about affiliates. Each year, your sponsored users must renew their account. You will need to approve the renewal by logging into the\u00a0 NYU Identity Management service . We have a walkthrough of the process, with screenshots, here . Getting an account with one of NYU partners NYU partners with many state and national facilities with a variety of HPC systems and expertise.\u00a0 Contact us \u00a0for assistance setting up a collaboration with any these. The Open Science Data Cloud Provides 1TB free storage for science data. We encourage researchers to publish datasets associated with published research as &quot;Public Data&quot; on OSDC The NY State High Performance Computing Consortium (hpc^2) Provides \u00a0high performance computing resources for New York State industry and academic institutions: Rensselaer Polytechnic Institute Stony Brook University - Dave Ecker University at Buffalo Brookhaven National Lab NYSERNet The Extreme Science and Engineering Discovery Environment (XSEDE) The most advanced, powerful, and robust collection of integrated advanced digital resources and services in the world; a single virtual system that scientists can use to interactively share computing resources, data, and expertise. Open Science Grid A national, distributed computing grid for data-intensive research. The Common Solutions Group for cooperative exploration of common solutions to IT challenges in higher education The Open Science Project is dedicated to writing and releasing free and Open Source\u00a0 scientific software.\u00a0 NYSERNet is a private not-for-profit corporation created to foster science and education in New York State The National Science Foundation An independent federal agency created by Congress in 1950 &quot;to promote the progress of science; to advance the national health, prosperity, and welfare; to secure the national defense.&quot; Oak Ridge National Laboratory The Department of Energy's largest science and energy laboratory. Argonne National Laboratory One of the U.S. Department of Energy's largest research centers. It is also the nation's first national laboratory, chartered in 1946. TOP500 Supercomputer Sites A project started in 1993 to provide a reliable basis for tracking and detecting trends in high-performance computing.\u00a0     HPC Stakeholders        Click here to expand...     ITS supports and encourages a model of hosting and managing clusters for research groups or departments in return for making unused cluster cycles available to the general NYU research community. These research groups and departments are our\u00a0 HPC Stakeholders , for whom NYU HPC manages hardware and provides priority access. Our current stakeholders are\u00a0 CGSB ,\u00a0 CNS ,\u00a0 CDS ,\u00a0 Kussell Lab \u00a0and\u00a0 CAOS . If you are interested in becoming a stakeholder, please contact us at\u00a0 hpc@nyu.edu \u00a0for details\u00a0 before \u00a0you purchase your cluster. We can discuss your needs and work with you in the planning and purchase of hardware. ( more... )     Compute and Storage Facilites        Click here to expand...     NYU HPC has retired its older clusters ( Union Square and Cardiac ) , and has merged the hardware that was Bowery into its newest and primary cluster, Mercer .  Mercer has 4 login and 394 compute nodes:  Number of nodes CPU type and speed Number of cores per node GPUs per node Total memory per node Memory available to jobs Node names Node set name \u00a0 112 Intel Xeon E-2690v2 ( Ivy Bridge) x86_64 3.0GHz (2014) 20 \u00a0 64GB 62GB compute-14-* to compute-20-*  ivybridge_20p_64GB_3000MHz  ivybridge  48 Ivy Bridge x86_64 3.0GHz (2014) 20 \u00a0 192GB 189GB compute-21-* to compute-23-* ivybridge_20p_192GB_3000MHz  68 Westmere x86_64 2.67GHz (2010) 12 \u00a0 24GB 23GB compute-4-* to compute-8-7 westmere_12p_24GB_2670MHz  dell_westmere  8 Westmere x86_64 2.67GHz (2010) 12 \u00a0 48GB 46GB compute-8-8 to compute-8-15 westmere_12p_48GB_2670MHz  16 Westmere x86_64 2.67GHz (2010) 12 \u00a0 96GB 93GB compute-9-* westmere_12p_96GB_2670MHz  64 Westmere x86_64 3.07GHz (2011) 12 \u00a0 48GB 46GB compute-12-* and compute-13-* westmere_12p_48GB_3070MHz \u00a0 64 Nehalem x86_64 2.67GHz (2009) 8 \u00a0 24GB 23GB compute-0-* to compute-3-* nehalem_8p_24GB_2670MHz \u00a0 1 Nehalem x86_64 2.27GHz (2009) 16 \u00a0 256GB 250GB compute-10-0 nehalem_16p_256GB_2270MHz \u00a0 1 Westmere x86_64 2.67GHz (2011) 32 \u00a0 1TB 1000GB compute-10-1 westmere_32p_1024GB_2670MHz \u00a0 4 Westmere x86_64 2.67GHz (2011) 12 1 x NVidia Tesla M7020 24GB 23GB compute-11-* westmere_12p_24GB_2670MHz_Tesla  tesla  9 Sandy Bridge x86_64 2.0GHz (2014) 16 4 x NVidia Titan 128GB 126GB gpu-23-* sandybridge_16p_128GB_2000MHz_Titan  titan            Icon                To restrict a job to a specific subset of nodes, you can request the node set name as a feature, eg: #PBS -l feature= ivybridge_20p_64GB_3000MHz             You can see a map of nodes and usage with pbstop. The diagram below indicates which nodes belong to which of the above nodesets: \u00a0  Hydra is a 3-node cluster with 24 Intel Sandy Bridge cores and 8 GPUs per node . 75% of Hydra is reserved for use by the NYU Center for Data Science ( CDS ) (see HPC Stakeholders ).  Dumbo is an older, 70-node Intel Xeon cluster running Hadoop. Our Hadoop support is still at &quot;pilot&quot; stage, and is &quot;best effort&quot; rather than guaranteed service. We plan to update the hardware for Hadoop in the near future. BuTinah , located at and managed by NYU Abu Dhabi , has 4 login and 537 compute nodes each with 12 Intel Westmere cores and either 48 GB or 192 GB memory. Access to the primary clusters is as indicated in the following diagram. The diagram also indicates what file storage NYU HPC provides .    The NYU HPC clusters have five filesystems for users' files. Each filesystem is configured differently to serve a different purpose: \u00a0  Space Environment Variable Space Purpose Visibility Backed up? Flushed? Allocation  Cost for Additional Storage  Total Size  File System  /home $HOME Program development space; storing small files you want to keep long term , e.g. source code, scripts. login and compute nodes. Starting with the installation of Mercer we have a unified /home filesystem served from same 7420 storage system as /archive and /work  Yes ASCII filenames only \u00a0 No 20GB (unified /home, mounted on Mercer) \u00a0 N/A 600TB (unified /home , space shared with /archive and /work ) NFS ZFS  /archive $ARCHIVE Long-term storage, mounted only on login nodes. Best for large files, please tar collections of small files when archiving . Groups may request a common aggregate archive space . login nodes only. Common to all clusters. Yes ASCII filenames only No 2TB $500/year for 1TB 600TB shared with /work and unified /home  ZFS  /scratch $SCRATCH Computational work space. Best suited to large, infrequent reads and writes. Files are deleted after 60 days without use . login and compute nodes. Common to all clusters.  No  Files not accessed for 60 days 5TB; inode quota: 1 million Policy  N/A 410TB Lustre  /work $WORK  Medium term, non-backed up storage mounted on login and compute nodes . login and compute nodes. No No 500GB N/A 600TB shared with /archive and unified /home ZFS  /state/partition1 $PBS_JOBTMP Small, node-local filesystem cleaned up at the end of each Torque job. For small, frequent reads and writes. Environment variable is defined in batch jobs (via qsub wrapper) compute nodes only. Local to each compute node. No End of each job Varies. Generally &gt;100GB N/A Varies ext3 \u00a0 $PBS_MEMDISK Optional, node-local memory filesystem. Like $PBS_JOBTMP but smaller and faster. See here for usage . compute nodes only. Local to each compute node. No End of each job Default 8GB. Specific amount can be requested (but must fit within node memory) N/A Varies tmpfs or ramfs   \u00a0 Only files and directories with ASCII-only filenames are backed up . Our backup system does not handle unicode in file or directory names, such files and directories (including all files and directories under them) will be bypassed. Important : Of all the space, only\u00a0 /scratch \u00a0should be used for computational purposes. Please do not write to /home when running jobs as it can easily be filled up. *Note:\u00a0 \u00a0Capacity of the\u00a0 /home \u00a0file system varies from cluster to cluster. Unlike\u00a0 /scratch \u00a0and\u00a0 /archive , the\u00a0 /home \u00a0file system is not mounted across clusters. Each cluster has its own\u00a0 /home , its own user base and\u00a0 /home \u00a0allocation policy.\u00a0 \u00a0 To purchase additional storage, send email to\u00a0 hpc@nyu.edu . See Clusters and Storage for more information.     Logging in to the NYU HPC Clusters \u00a0        Click here to expand...     The HPC clusters are not directly visible to the internet, you must first log in to a bastion host named hpc.nyu.edu : (note that from the clusters you can still access the Internet - see Transferring files to and from the HPC clusters ). The diagram below illustrates the login path.  In a nutshell Logging in is a two-step process: First login to the bastion host, hpc.nyu.edu. From a Mac or Linux workstation, this is a simple terminal command:    Windows users will need to use Putty, see here for instructions .           Icon                You can't do anything on the bastion host, except ssh to the cluster             Next login to the cluster. For Mercer, this is done with:     The full story First you need to ensure your workstation has the necessary software and settings to connect to the clusters, and to use graphical interfaces. We have instructions for Windows and Mac users. Next, here's how to log in from a \u00a0 Windows / Linux / Mac \u00a0workstation. SSH tunneling for easier login and data transfer The two-stage access can be inconvenient, especially when transferring files to and from the clusters. Secure direct access and file transfer is possible by setting up SSH tunneling from your workstation to the HPC clusters. We have instructions on setting this up for Windows , Mac and Linux .        Logging in with passwordless SSH     Once you are using SSH tunneling, it is possible to configure your workstation and accounts to avoid typing your password each time. Instructions for this will be added here soon.     What can I do on the login node? (Can I run &lt;insert-software-name-here&gt;?) The login nodes are for preparing, submitting and monitoring scripts, analyzing results, moving data around and code development and simple compilation. They are not suitable for running computational workloads - for this use the batch system . Compiling a large source codebase, especially with heavy use of optimization or -ipo (interprocedural optimization), can use much memory and CPU time. In such circumstances it is best to use the batch system for compilation too, perhaps via an interactive batch job .     Finding and Using Software        Click here to expand...     A variety of commercial and open-source software is available on the NYU HPC clusters, and can be accessed via Environment Modules .         Important          Icon                The login nodes are not suitable for computational work, they are a limited and shared resource for preparing and submitting computational jobs, developing and compiling software, and managing data. Computational work should instead be run via the batch system .                   Using NYU software on your own computer     NYU HPC hosts licenses for a number of commercial software packages which are suitable for workstation as well as HPC use, such as Matlab, COMSOL and Mathematica. \u00a0 Contact us \u00a0about accessing these packages.           Getting new software installed on the HPC clusters     If you need a free or open source software package which is not currently available on the HPC clusters, contact us . Usually we can install it for you, or suggest an alternative which is already available. Our ability to buy and install commercial software depends on the cost and on how widely it will be used. We may also be able to host licenses or share costs with you in return for making the software available also to the NYU research community, so if you need a specific commercial package\u00a0 contact us to discuss it.           Compiling and developing software     Intel and GNU compilers are available on the clusters. For most code, we recommend the Intel compilers\u00a0 For debugging we have the GNU debugger gdb , the Intel debugger idb and Totalview by Roguewave. Debugging is best performed with an interactive batch session . There is more about compiling and debugging on the old wiki pages .           Usage examples on Mercer     \u00a0There are usage examples for many popular software packages in\u00a0 /share/apps/examples \u00a0on Mercer: batch \u00a0- An example batch job blcr \u00a0\u00a0- Checkpoint-Restart facility for long jobs comsol \u00a0\u00a0- Computational Fluid Dynamics c-sharp \u00a0\u00a0- Language for the .NET/mono runtime environment fluent \u00a0\u00a0- Computational Fluid Dynamics / Multiphysics package gaussian \u00a0- Chemistry package matlab \u00a0\u00a0- For mathematical exploration namd \u00a0\u00a0- Molecular dynamics qchem-amber \u00a0\u00a0- Molecular dynamics r \u00a0\u00a0- Interpreted language for statistics work resource-usage \u00a0\u00a0- Shows minute-by-minute CPU and memory usage of a program stata \u00a0- Statistics package         Managing data: Storage , collaboration and moving data around        Click here to expand...     Filesystems, their optimal usage and your space allocation are described under Storage . Quotas On Mercer, enter 'myquota' at the prompt to see how much space you have used and available on each filesystem. \u00a0 Security and collaboration: file permissions and ACL on NYU HPC clusters By default, only you can edit, or even see, your files. You can grant permission for your colleagues to see or edit files with setfacl , and you can check the permissions on a file or directory with getfacl . An access control list (or ACL) gives per-file, per-directory and per-user control over who can read, write and execute files.\u00a0 You can see the ACL for a file or directory with the\u00a0 getfacl \u00a0command :   $ getfacl myfile.txt   To modify permissions for files or directories, use\u00a0 setfacl . For a detailed description, see ' man setfacl '. In the example below, I give read permission on\u00a0 dummy.txt \u00a0to user\u00a0 bob123 :   $ setfacl -m u:bob123:r myfile.txt    \u00a0 For setting execute permission on files \u00a0- useful for scripts, and for allowing directories to be entered -\u00a0 chmod \u00a0is still used. ( more... ) Transferring files to and from the HPC clusters To transfer data between your workstation and the NYU HPC clusters, you must set up and start an SSH tunnel on the workstation. We have instructions for this for Windows , Mac and Linux workstations. Once you have an SSH tunnel, you can transfer files to and from the HPC clusters - including \u00a0 BuTinah at NYUAD .     Running jobs with qsub: How to use the batch system        Click here to expand...     Working on the HPC clusters is not the same as working at a desktop workstation: in order to provide high performance computing to many users simultaneously, computational work must be packaged into a job - a script specifying what resources the job will need and the commands necessary to perform the work - and submitted to the system to be run without further input from the user. The system then schedules and runs the job on a dedicated portion of the cluster. (Note that there is a way to work interactively within this model, for work which cannot be scripted, such as debugging). Job Scheduling On the NYU clusters,\u00a0 Torque \u00a0and\u00a0 Moab \u00a0manage the running and scheduling of jobs. As a user you will interact mostly with Torque, which accepts and runs job scripts and\u00a0manages and monitors the cluster's compute resources. Moab does the heavy thinking: the planning of which job should be run where and when.           Icon                Avoid requesting vastly more CPUs, memory or walltime than you actually need . Jobs needing fewer resources are easier to schedule - in our scheduling diagram , a job requiring just 1 CPU for 1 hour could be inserted into the gap on Node 1 CPU 4. Smaller jobs are also more likely to receive priority when being scheduled. Note that a small overestimate, such as 10%-20%, is wise, lest your job run out of time and be killed before it finishes, but requesting several times what you need will result in longer queueing time for your job and less efficient system utilization for everybody.           ( more... ) Login and Compute Nodes Note that certain filesystems are visible to the login or compute nodes but not both: specifically at NYU\u00a0 /archive \u00a0is not visible to the compute nodes, while /state/partition1 is visible and local only to individual compute nodes.          Important          Icon                Do not run computationally-heavy or long-running jobs on the login nodes ! Not only will you have poor performance, the heavy resource usage of such jobs impacts others ability to use the login nodes for their intended purposes. If you need to run a job interactively (for example, when debugging), please do so through an interactive batch session .           ( more... ) Queues Not all jobs can be run at once - the cluster is finite! - so when jobs are submitted they are placed into a queue.\u00a0When a &quot;space&quot; becomes available in the schedule Moab looks down the queue for the first job that will fit into the space.\u00a0 Jobs are not necessarily placed at the end of the queue - Moab uses the priority (discussed here ) to determine where in the queue a job should be placed.           Icon                At NYU HPC shorter jobs are given higher priority             There is more than one queue. Each queue is configured for different types of jobs and has resource limits and priorities set accordingly. If you do not specify a queue to submit to, Torque will use the resources requested to select a queue for you. Frequently this is the best option , however in some circumstances you are better off explicitly specifying a queue. \u00a0You can see the list of queues with the command &quot; qstat -q &quot;, and you can see more detail about a specific queue with &quot; qstat -Qf\u00a0 queue-name &quot;.\u00a0 ( more... ) Writing a Job Script ( more... ) Submitting a Job Jobs are submitted with the\u00a0 qsub \u00a0command:   $ qsub options job-script   The options tell Torque information about the job, such as what resources will be needed. These can be specified in the job-script as PBS directives, or on the command line as options, or both (in which case the command line options take precedence should the two contradict each other). For each option there is a corresponding PBS directive with the syntax:   #PBS option   For example, you can specify that a job needs 2 nodes and 8 cores on each node by adding to the script the directive:    or as a command-line option to qsub when you submit the job:\u00a0   $ qsub -l nodes=2:ppn=8 my_script.q   ( more... ) Monitoring Jobs To see the status of a single job - or a list of specific jobs - pass the Job IDs to\u00a0 qstat , as in the following example:\u00a0   $ qstat 3593014 3593016 Job id  Name    User   Time Use S Queue ------------- ---------------- --------------- -------- - ----- 3593014  model_scen_1  ab123   7:23:47 R s48 3593016  model_scen_1  ab123   7:23:26 R s48   Most of the fields in the output are self-explanatory. The second-last column &quot;S&quot; is the job status , which can be : Q meaning &quot;Queued&quot; H meaning &quot;Held&quot; - this may be the result of a manual hold or of a job dependency R meaning &quot;Running&quot; C meaning &quot;Completed&quot;. After the job finishes, it will remain with &quot;completed&quot; status for a short time before being removed from\u00a0the batch system.  Other, less common job status flags are described in the manual ( man qsub ). The program\u00a0 pbstop , available on the login nodes, shows which jobs are currently running on which nodes and cores of a cluster. Jobs belonging to a single user can be highlighted by launching\u00a0 pbstop \u00a0with the\u00a0 -u \u00a0switch:   pbstop -u &lt;NetID&gt;   (of course, replace\u00a0 &lt;NetID&gt; \u00a0with your NYU NetID). Or, you can use the alias &quot;me&quot;:   pbstop -u me   When you start pbstop you see something like the annotated screenshot below. You might need to resize your terminal to make it all fit: \u00a0  ( more... ) Canceling a Job To kill a running job, or remove a queued job from the queue, use qdel :   $ qdel jobid   To cancel ALL of your jobs:   $ qdel all        Tutorials , FAQs and how to get help        Click here to expand...     For help with any aspect of scientific or high performance computing on the NYU HPC clusters, email us at hpc@nyu.edu . We are developing a set of tutorials to help NYU HPC users make the most of the facilities. Tutorials are suitable for self-directed learning and are also periodically run as classes in the library. NYU Data Services also provides tutorials for a range of scientific software - for dates and times of upcoming HPC classes see the calendar on the left, or see NYU Data Services for a wider schedule of classes. Currently available HPC tutorials are: Getting started on Mercer - how to login and migrate your workflow from our older clusters onto our newest cluster, Mercer. Tutorial 1: Introduction to Unix/Linux Tutorial 2: Getting Started in the NYU HPC environment Tutorial 3: Using NYU HPC Effectively The NYU HPC qsub tutorial is also available, covering: Declare the date/time a job becomes eligible for execution Defining the working directory path to be used for the job Manipulate the output files Mail job status at the start and end of a job Submit a job to a specific queue Submitting a job that is dependent on the output of another Submitting multiple jobs in a loop that depend on output of another job Opening an interactive shell to the compute node Passing an environment variable to your job Passing your environment to your job Submitting an array job: Managing groups of jobs  FAQ Something went wrong! Why does running &quot;ls&quot; on /scratch take so long? I can't login When trying to login, I get warnings about &quot;HOST IDENTIFICATION HAS CHANGED&quot; What happened to my data on /scratch? In the library, my wireless connection keeps dropping out. How can I fix it? I'm getting a &quot;module: command not found&quot; error Warning: no access to tty (Bad file descriptor), Thus no job control in this shell I get an error &quot;Warning: no display specified.&quot; when I use -X flag with ssh Who killed my job, and why? I got an email &quot;Please do not run jobs on login nodes&quot; Running jobs What resources can and should I request? Can I make sure a job gets executed only after another one completes? How do I log in to a specific node? How can I make sure my job is running smoothly? My job will take longer than 48 hours, what should I do? My job needs (MySQL, some other service) to be running I want to run a job at 9am every day Using software How do I run ... (esp, needs a license) a STATA job? a Gaussian job? a Matlab job? I can't find (some software package) Can you install (some software package)? How can I view a PDF file on Mercer? Managing data How much of my file/space quota have I used? How do I give my colleague access to my files? How do I get the best transfer speed to or from BuTinah? I have a huge amount of data that I want to compress for storage or transfer     Monthly Maintenance Window        Click here to expand...     To provide the best possible service, ITS must regularly update and perform routine maintenance on its systems and networks. Some of these activities require that the affected systems and networks be shut down. While this work is essential, we also recognize that it presents an inconvenience. To enable those who use these systems to better plan for maintenance, we have guidelines for scheduling routine maintenance and upgrades to the HPC clusters as described below. A MONTHLY SCHEDULED MAINTENANCE OF UP TO 12 HOURS WILL BE TAKEN, IF NEEDED, BEGINNING AT 8AM ON THE FIRST MONDAY OF EACH MONTH Major scheduled maintenance and upgrade activities will take place, if needed, once per month.\u00a0 These will be scheduled for the first Monday of each month at 8am to noon to start these scheduled maintenance and upgrade activities. The maintenance period may often be brief or not used at all, but can last up to 12 hours if this amount of time is needed to complete the work. We have chosen early morning on the first Monday of each month for our maintenance work as it has been the time period during the week which has low usage on our clusters. A notification will be sent to all HPC account holders announcing any scheduled maintenance work in advance. A WEEKLY SCHEDULED MAINTENANCE OF UP TO FOUR HOURS (MONDAY 8 AM to NOON) MAY BE USED TO ADDRESS SMALLER MAINTENANCE AND UPGRADE NEEDS. This time will not be used if not needed.      Featured Research   The link between Atlantic Ocean warming and Antarctic climate change In recent decades Antarctica, especially the Antarctic Peninsula, has experienced dramatic climate change, one observed phenomenon being a dipolar pattern of high sea-level pressure south of Australia and low sea-level pressure in the Amundsen Sea during the souther winter. The mechanism driving this has been identified by researchers at NYU's Center for Atmospheric and Ocean Science as warming in the north and tropical Atlantic. Xichen Li, David Holland, Edwin Gerber and Changhyun Yoo recently published a paper in Nature showing that this pattern of Antarctic sea-level pressure correlates with sea surface temperature in the north and tropical Atlantic, moreover that the Atlantic sea surface temperature is driving the Antarctic sea-level pressure and that the low pressure thus established in the Amundsen Sea enhances warm-air advection and warm-water transport to the Antarctic Peninsula, thus contributing to the observed climate change in that region. Xichen Li and colleagues used regression and maximum covariance analysis to establish the correlation between sea surface temperature in the north and tropical Atlantic and sea-level pressure in the Antarctic, as illustrated below. However correlation does not imply causation, and physical experiments testing the effect on Antarctica of altering temperatures in the Atlantic are not practical (or ethical!). But by running numerical simulations on NYUs HPC clusters , using the Community Atmosphere Model (CAM4) from NCAR they were able to show that by forcing warming in the north and tropical Atlantic causes sea-level patterns in Antarctica corresponding to those observed. ( read more )  \u00a0 More featured research using NYU HPC                                                          No labels                                                                                                                                           Overview      Content Tools      Activity                                                     Powered by Atlassian Confluence 5.6.6 , Team Collaboration Software      Printed by Atlassian Confluence 5.6.6, Team Collaboration Software.      Report a bug      Atlassian News              Atlassian"}]}]