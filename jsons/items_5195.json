[{"detail": [{"content": "Image Classifier Demo                                                               Image Classifier Demo                   Demo        About        Terms                                        Image Classifier Demo           Upload your images to have them classified by a machine! Upload multiple images using the  button below or dropping them on this page. The predicted objects out of 1,000 categories  will be refreshed automatically. Images are resized such that the smallest dimension becomes  256, then the center 256x256 crop is used. More about this NYU demo can be  found here . The model in the demo is early work towards the  following paper:    \"Visualizing and Understanding Convolutional Networks\" , Matthew Zeiler and  Rob Fergus.           To see a live real-time demo of the current best object recognition system trained on 10,000  categories, checkout www.clarifai.com .                                                                 Upload Images                                                                          Remove All             Show help tips         I agree to the Terms of Use                                                                                                Demo Notes          If your images have objects that are not in the 1,000    categories of ImageNet, the model will not know about    them.     Other objects can be added from all 20,000+ ImageNet   categories (it may be slow to load the autocomplete results...just wait a little).     The maximum file size for uploads in this demo is 10 MB .      Only image files ( JPEG, JPG, GIF, PNG ) are allowed in this demo .      You can drag &amp; drop files from your desktop on this webpage    with Google Chrome, Mozilla Firefox and Apple Safari.     Some mobile browsers are known to work, others will   not. Try updating your browser or contact us with the   problem.     All images for your current IP and browsing session   are shown above and not shown to others.     This demo is powered by research out of New York   University. Click here to find out more         If you encounter problems, please contact       zeiler@cs.nyu.edu         Demo created by: Matthew Zeiler             &copy; Copyright 2013                &times;                            Download                   Slideshow                   Previous              Next"}]},
{"detail": [{"content": "Image Classifier Demo                                                               Image Classifier Demo                   Demo        About        Terms                                        Image Classifier Demo           Upload your images to have them classified by a machine! Upload multiple images using the  button below or dropping them on this page. The predicted objects out of 1,000 categories  will be refreshed automatically. Images are resized such that the smallest dimension becomes  256, then the center 256x256 crop is used. More about this NYU demo can be  found here . The model in the demo is early work towards the  following paper:    \"Visualizing and Understanding Convolutional Networks\" , Matthew Zeiler and  Rob Fergus.           To see a live real-time demo of the current best object recognition system trained on 10,000  categories, checkout www.clarifai.com .                                                                 Upload Images                                                                          Remove All             Show help tips         I agree to the Terms of Use                                                                                                Demo Notes          If your images have objects that are not in the 1,000    categories of ImageNet, the model will not know about    them.     Other objects can be added from all 20,000+ ImageNet   categories (it may be slow to load the autocomplete results...just wait a little).     The maximum file size for uploads in this demo is 10 MB .      Only image files ( JPEG, JPG, GIF, PNG ) are allowed in this demo .      You can drag &amp; drop files from your desktop on this webpage    with Google Chrome, Mozilla Firefox and Apple Safari.     Some mobile browsers are known to work, others will   not. Try updating your browser or contact us with the   problem.     All images for your current IP and browsing session   are shown above and not shown to others.     This demo is powered by research out of New York   University. Click here to find out more         If you encounter problems, please contact       zeiler@cs.nyu.edu         Demo created by: Matthew Zeiler             &copy; Copyright 2013                &times;                            Download                   Slideshow                   Previous              Next"}, {"content": "\"CBLL, Research Projects, Computational and Biological Learning Lab, Courant Institute, NYU\"                                CBLL HOME   VLG Group   News/Events   Seminars      People   Research   Publications   Talks   Demos   Datasets   Software   Courses   Links      Group Meetings      Join CBLL      Y. LeCun's website   CS at Courant   Courant Institute   NYU                       NORB: Generic Object Recognition in Images                                        Time Period : September 2003 - present.  Participants : Fu Jie Huang, Yann LeCun (Courant Institute/CBLL), Leon Bottou (NEC Labs). Talks :    Slides : End-to-End Learning of Object Categorization  with Invariance to Pose, Illumination, and Clutter .  Slides of a talk delivered at CVPR Workshop on Object Recognition,   Washington DC, June 2004.  [DjVu (2.1MB)] .    Publications :     [LeCun, Huang, Bottou, 2004] .   Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting   Proceedings of CVPR 2004.    Dataset Download the  NORB dataset .  Support : this project is supported by National Science  Foundation under grants numbers 0535166, and 0325463.      The recognition of generic object categories with invariance to pose, lighting, diverse backgrounds, and the presence of clutter is one of the major challenges of Computer Vision.    We are developing learning systems that can recognize generic object purely from their shape, independently of pose, illumination, and surrounding clutter.   The NORB dataset (NYU Object Recognition Benchmark) contains stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions (for a total of 194,400 individual images).   The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars . Five instances of each category were used for training, and the other 5 for testing.       The picture shows the 25 objects used for training (left panel)  and the 25 different objects used for testing (right panel). There are five object categories: animals, human figures, airplanes, trucks and cars.     Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used to train and test nearest neighbor methods, Support Vector Machines, and Convolutional Networks, operating on raw pixels or on PCA-derived features.        The NORB Dataset      Experiments were conducted with four datasets generated from the normalized object images. The first two datasets were for pure categorization experiments (a somewhat unrealistic task), while the last two were for simultaneous detection/segmentation/recognition experiments.   All datasets used 5 instances of each category for training and the 5 remaining instances for testing. In the normalized dataset, 972 images of each instance were used: 9elevations, 18 azimuths (0 to 340 degrees every 20 degrees), and 6 illuminations, for a total of 24,300 training samples and 24,300 test samples. In the various  jittered datasets, each of the 972 images of each instance were used to generate additional examples by randomly perturbing the position ([-3, +3] pixels), scale (ratio in [0.8, 1.1]), image-plane angle ([-5, 5] degrees), brightness ([-20, 20] shifts of gray levels), contrast ([0.8, 1.3] gain) of the objects during the compositing process. Ten drawings of these random parameters were drawn to generate training sets, and one or two drawings to generate test sets.     [click picture to enlarge]     Image capturing setup.          In the textured and cluttered datasets, the objects were placed on randomly picked background images. In those experiments, a 6-th category was added: background images with no objects (results are reported for this 6-way classification). In the textured  set, the backgrounds were placed at a fixed disparity, akin to a back wall orthogonal to the camera axis at a fixed distance. In the cluttered datasets, the disparities were adjusted and randomly picked so that the objects appeared placed on highly textured horizontal surfaces at small random distance from that surface. In addition, a randomly picked ``distractor'' object from the training set was placed at the periphery of the image.          Examples of the various lighting conditions for two elevations)           normalized-uniform set : 5 classes, centered, unperturbed objects on uniform  backgrounds. 24,300 training samples, 24,300 testing samples.    jittered-uniform set : 5 classes, random perturbations, uniform backgrounds.  243,000 training samples (10 drawings) and 24,300 test samples (1 drawing)    jittered-textured set : 6 classes (including one background class)  random perturbation, natural background textures at fixed disparity.  291,600 training samples (10 drawings), 58,320 testing samples (2 drawings).    jittered-cluttered set : 6 classes (including one background class),   random perturbation, highly cluttered background images at random disparities,   and randomly placed distractor objects around the periphery.   291,600 training samples (10 drawings), 58,320 testing samples (2 drawings).         [click picture to enlarge]     Compositing process. top left: raw image; top right: chroma-keyed object mask; bottom left: cast shadow coefficient mask; bottom right: composite image with cast shadow.        Occlusions of the central object by the distractor occur occasionally in the jittered cluttered set. Most experiments were performed in binocular mode (using left and right images), but some were performed in monocular mode. In monocular experiments, the training set and test set were composed of all left and right images used in the corresponding binocular experiment. Therefore, while the number of training samples was twice higher, the total amount of training data was identical. Examples from the jittered-textured and jittered-cluttered training set are shown below      Examples from the jittered-textured       [click picture to enlarge]     examples from the jittered-cluttered dataset. This dataset is   availabe for download.          Methods     On the Normalized-Uniform Dataset      Classifier Error Rate   Linear Classifier, binocular 30.2% error   K-Nearest Neighbors on raw stereo images 18.4% error   K-Nearest Neighbors on 95 PCA features 16.6 error   Pairwise Support Vector Machine on raw stereo images NO CONVERGENCE   Pairwise SVM on 48x48 monocular images 13.9% error   Pairwise SVM on 32x32 monocular images 12.6% error   Pairwise SVM on 95 PCA features 13.3 error   Convolutional Network \"LeNet7\" 6.6% error   Convolutional Network \"LeNet7\" with pose manifold 6.2% error          The first 60 principal components extracted from the normalized-uniform training set. Unlike with eigen-faces these \"eigen-toys\" are not recognizable and have symmetries because the objects are seen from every angle in the training set.      On the Jittered-Cluttered Dataset      Classifier Error Rate   Convolutional Network \"LeNet7\", binocular 7.8% error   Convolutional Network \"LeNet7\", monocular 20.8% error         [click picture to enlarge]      Architecture of the convolutional net \"LeNet 7\". This network has 90,857 trainable parameters and 4.66 Million connections.  Each output unit is influenced by a receptive field of 96x96 pixels  on the input.         Learned kernels from the first layer of the binocular convolutional network.        Learned kernels from the third layer of the binocular convolutional network.        Results and Examples    The convolutional network can be very efficiently applied to all locations on a large input image. For example, applying LeNet 7 to a single 96x96 window requires 4.66 Million multiply-accumulate operations. But applying LeNet 7 to every 96x96 windows, shifted every 12 pixels, over a 240x240 image (169 windows) requires only 47.5 Million multiply-accumulate operations. Applying a non-convolutional classifier with the same complexity to every such 96x96 window would consume 788 Million operations (4.66 million times 169).   The network can be applied to images at multiple scales to  ensure scale invariance.   A system was built around LeNet 7, that can detect and recognize objects in natural images. The system runs in real time (a few frames per second) on a laptop connected to a USB camera. Examples of outputs from that system are shown below.     Scenes with objects from the NORB dataset                   Various scenes with other objects                                                   Natural Scenes     NOTE: The system was not trained on natural images.                                                                        A few mistakes                     Examples with the Internal State of the Convolutional Network                                                                                                     ."}]},
{"detail": [{"content": "Image Classifier Demo                                                               Image Classifier Demo                   Demo        About        Terms                                        Image Classifier Demo           Upload your images to have them classified by a machine! Upload multiple images using the  button below or dropping them on this page. The predicted objects out of 1,000 categories  will be refreshed automatically. Images are resized such that the smallest dimension becomes  256, then the center 256x256 crop is used. More about this NYU demo can be  found here . The model in the demo is early work towards the  following paper:    \"Visualizing and Understanding Convolutional Networks\" , Matthew Zeiler and  Rob Fergus.           To see a live real-time demo of the current best object recognition system trained on 10,000  categories, checkout www.clarifai.com .                                                                 Upload Images                                                                          Remove All             Show help tips         I agree to the Terms of Use                                                                                                Demo Notes          If your images have objects that are not in the 1,000    categories of ImageNet, the model will not know about    them.     Other objects can be added from all 20,000+ ImageNet   categories (it may be slow to load the autocomplete results...just wait a little).     The maximum file size for uploads in this demo is 10 MB .      Only image files ( JPEG, JPG, GIF, PNG ) are allowed in this demo .      You can drag &amp; drop files from your desktop on this webpage    with Google Chrome, Mozilla Firefox and Apple Safari.     Some mobile browsers are known to work, others will   not. Try updating your browser or contact us with the   problem.     All images for your current IP and browsing session   are shown above and not shown to others.     This demo is powered by research out of New York   University. Click here to find out more         If you encounter problems, please contact       zeiler@cs.nyu.edu         Demo created by: Matthew Zeiler             &copy; Copyright 2013                &times;                            Download                   Slideshow                   Previous              Next"}, {"content": "\"CBLL, Research Projects, Computational and Biological Learning Lab, Courant Institute, NYU\"                                CBLL HOME   VLG Group   News/Events   Seminars      People   Research   Publications   Talks   Demos   Datasets   Software   Courses   Links      Group Meetings      Join CBLL      Y. LeCun's website   CS at Courant   Courant Institute   NYU                       NORB: Generic Object Recognition in Images                                        Time Period : September 2003 - present.  Participants : Fu Jie Huang, Yann LeCun (Courant Institute/CBLL), Leon Bottou (NEC Labs). Talks :    Slides : End-to-End Learning of Object Categorization  with Invariance to Pose, Illumination, and Clutter .  Slides of a talk delivered at CVPR Workshop on Object Recognition,   Washington DC, June 2004.  [DjVu (2.1MB)] .    Publications :     [LeCun, Huang, Bottou, 2004] .   Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting   Proceedings of CVPR 2004.    Dataset Download the  NORB dataset .  Support : this project is supported by National Science  Foundation under grants numbers 0535166, and 0325463.      The recognition of generic object categories with invariance to pose, lighting, diverse backgrounds, and the presence of clutter is one of the major challenges of Computer Vision.    We are developing learning systems that can recognize generic object purely from their shape, independently of pose, illumination, and surrounding clutter.   The NORB dataset (NYU Object Recognition Benchmark) contains stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions (for a total of 194,400 individual images).   The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars . Five instances of each category were used for training, and the other 5 for testing.       The picture shows the 25 objects used for training (left panel)  and the 25 different objects used for testing (right panel). There are five object categories: animals, human figures, airplanes, trucks and cars.     Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used to train and test nearest neighbor methods, Support Vector Machines, and Convolutional Networks, operating on raw pixels or on PCA-derived features.        The NORB Dataset      Experiments were conducted with four datasets generated from the normalized object images. The first two datasets were for pure categorization experiments (a somewhat unrealistic task), while the last two were for simultaneous detection/segmentation/recognition experiments.   All datasets used 5 instances of each category for training and the 5 remaining instances for testing. In the normalized dataset, 972 images of each instance were used: 9elevations, 18 azimuths (0 to 340 degrees every 20 degrees), and 6 illuminations, for a total of 24,300 training samples and 24,300 test samples. In the various  jittered datasets, each of the 972 images of each instance were used to generate additional examples by randomly perturbing the position ([-3, +3] pixels), scale (ratio in [0.8, 1.1]), image-plane angle ([-5, 5] degrees), brightness ([-20, 20] shifts of gray levels), contrast ([0.8, 1.3] gain) of the objects during the compositing process. Ten drawings of these random parameters were drawn to generate training sets, and one or two drawings to generate test sets.     [click picture to enlarge]     Image capturing setup.          In the textured and cluttered datasets, the objects were placed on randomly picked background images. In those experiments, a 6-th category was added: background images with no objects (results are reported for this 6-way classification). In the textured  set, the backgrounds were placed at a fixed disparity, akin to a back wall orthogonal to the camera axis at a fixed distance. In the cluttered datasets, the disparities were adjusted and randomly picked so that the objects appeared placed on highly textured horizontal surfaces at small random distance from that surface. In addition, a randomly picked ``distractor'' object from the training set was placed at the periphery of the image.          Examples of the various lighting conditions for two elevations)           normalized-uniform set : 5 classes, centered, unperturbed objects on uniform  backgrounds. 24,300 training samples, 24,300 testing samples.    jittered-uniform set : 5 classes, random perturbations, uniform backgrounds.  243,000 training samples (10 drawings) and 24,300 test samples (1 drawing)    jittered-textured set : 6 classes (including one background class)  random perturbation, natural background textures at fixed disparity.  291,600 training samples (10 drawings), 58,320 testing samples (2 drawings).    jittered-cluttered set : 6 classes (including one background class),   random perturbation, highly cluttered background images at random disparities,   and randomly placed distractor objects around the periphery.   291,600 training samples (10 drawings), 58,320 testing samples (2 drawings).         [click picture to enlarge]     Compositing process. top left: raw image; top right: chroma-keyed object mask; bottom left: cast shadow coefficient mask; bottom right: composite image with cast shadow.        Occlusions of the central object by the distractor occur occasionally in the jittered cluttered set. Most experiments were performed in binocular mode (using left and right images), but some were performed in monocular mode. In monocular experiments, the training set and test set were composed of all left and right images used in the corresponding binocular experiment. Therefore, while the number of training samples was twice higher, the total amount of training data was identical. Examples from the jittered-textured and jittered-cluttered training set are shown below      Examples from the jittered-textured       [click picture to enlarge]     examples from the jittered-cluttered dataset. This dataset is   availabe for download.          Methods     On the Normalized-Uniform Dataset      Classifier Error Rate   Linear Classifier, binocular 30.2% error   K-Nearest Neighbors on raw stereo images 18.4% error   K-Nearest Neighbors on 95 PCA features 16.6 error   Pairwise Support Vector Machine on raw stereo images NO CONVERGENCE   Pairwise SVM on 48x48 monocular images 13.9% error   Pairwise SVM on 32x32 monocular images 12.6% error   Pairwise SVM on 95 PCA features 13.3 error   Convolutional Network \"LeNet7\" 6.6% error   Convolutional Network \"LeNet7\" with pose manifold 6.2% error          The first 60 principal components extracted from the normalized-uniform training set. Unlike with eigen-faces these \"eigen-toys\" are not recognizable and have symmetries because the objects are seen from every angle in the training set.      On the Jittered-Cluttered Dataset      Classifier Error Rate   Convolutional Network \"LeNet7\", binocular 7.8% error   Convolutional Network \"LeNet7\", monocular 20.8% error         [click picture to enlarge]      Architecture of the convolutional net \"LeNet 7\". This network has 90,857 trainable parameters and 4.66 Million connections.  Each output unit is influenced by a receptive field of 96x96 pixels  on the input.         Learned kernels from the first layer of the binocular convolutional network.        Learned kernels from the third layer of the binocular convolutional network.        Results and Examples    The convolutional network can be very efficiently applied to all locations on a large input image. For example, applying LeNet 7 to a single 96x96 window requires 4.66 Million multiply-accumulate operations. But applying LeNet 7 to every 96x96 windows, shifted every 12 pixels, over a 240x240 image (169 windows) requires only 47.5 Million multiply-accumulate operations. Applying a non-convolutional classifier with the same complexity to every such 96x96 window would consume 788 Million operations (4.66 million times 169).   The network can be applied to images at multiple scales to  ensure scale invariance.   A system was built around LeNet 7, that can detect and recognize objects in natural images. The system runs in real time (a few frames per second) on a laptop connected to a USB camera. Examples of outputs from that system are shown below.     Scenes with objects from the NORB dataset                   Various scenes with other objects                                                   Natural Scenes     NOTE: The system was not trained on natural images.                                                                        A few mistakes                     Examples with the Internal State of the Convolutional Network                                                                                                     ."}, {"content": "NORB Object Recognition Dataset, Fu Jie Huang, Yann LeCun, New York University        THE NORB DATASET, V1.0       the small set (with normalized object sizes and uniform background)      Fu Jie Huang , Yann LeCun  Courant Institute, New York University  July 2004      last updated: October,2005   This database is intended for experiments in 3D object reocgnition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340  every 20 degrees).   The training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5).     TERMS / COPYRIGHT    This database is provided for research purposes. It cannot be sold. Publications that include results obtained with this database should reference the following paper:    Y. LeCun, F.J. Huang, L. Bottou, Learning Methods for Generic Object Recognition with  Invariance to Pose and Lighting. CVPR 2004. online version    CONTENT    The files are gzipped for download purpose. After uncompressed, they are in a simple  binary matrix format, with file postfix \".mat\". The file format is explained in a  later section.   The \"-dat\" files store the image sequences. The \"-cat\" files store the corresponding  category of the images. Each \"-dat\" file stores 29,160 image pairs (6 categories, 5 instances, 6 lightings, 9 elevations, and 18 azimuths). The 6-th category is for  images without objects, which can be used to train a system to reject images as none  of the 5 object categories. Each corresponding \"-cat\" file contains 29,160 category  labels (0 for animal, 1 for human, 2 for plane, 3 for truck, 4 for car, 5 for blank).   Each \"-info\" file stores 29,160 10-dimensional vectors, which contain additional  information about the corresponding images. The first 4 elements in the vector are:   - 1. the instance in the category (0 to 9)   - 2. the elevation (0 to 8, which mean cameras are 30, 35,40,45,50,55,60,65,70    degrees from the horizontal respectively)   - 3. the azimuth (0,2,4,...,34, multiply by 10 to get the azimuth in degrees)   - 4. the lighting condition (0 to 5)  and the next 6 elements describe the peturbations added to the object when superposed  onto a cluttered background. (see next section)   For regular training and testing, \"-dat\" and \"-cat\" files are sufficient. \"-info\"  files are provided in case some other forms of classification or preprocessing are  needed.     JITTERED OBJECTS AND CLUTTERED BACKGROUND    After capturing, each image has been processed so that the object is centered in  the image (the center of mass of object pixels are in the center of the image),  scaled so that the bounding box is roughly 80x80 pixels, and placed on a uniform  background, including the cast shadow.   And then 3 sources of variations are added to the data set:  - the objects are peturbed  - the objects are superposed onto complex background  - distractor objects are added to the background   The objects are randomly peturbed in 5 ways. They are scaled by factors  between 0.78 to 1.0; in-plane rotated -5 to +5 degrees; and shifted -6 to  +6 pixels horizontally and vertically. The image intensities (in the range  of 0 to 255) are a random value between -20 to +20; image contrasts are  scaled in the range of 0.8 to 1.3. The peturbations are stored in the last 6  elements in the \"-info\" files:   - 5. horizontal shift (-6 to +6)   - 6. vertical shift (-6 to +6)   - 7. lumination change (-20 to +20)   - 8. contrast (0.8 to 1.3)   - 9. object scale (0.78 to 1.0)   - 10. rotation (-5 to +5 degrees)    The complex background images are extracted from a subset of natural scene  images from Corel image library. The images contain scenes with large region  contrasts such as lake against moutain, and irregular region boundaries.   One distractor object is added to each image. The distractor is located toward  the boundary of the image, but can clutter the main object in the center.   There are images with only background and distractor objects. These images belong  to their own category, as indicated in the category files.     FILE FORMAT    The files are stored in the so-called \"binary matrix\" file format, which  is a simple format for vectors and multidimensional matrices of various  element types. Binary matrix files begin with a file header which describes the type and size of the matrix, and then comes the binary image of the matrix.   The header is best described by a C structure:   struct header {   int magic;  // 4 bytes   int ndim;  // 4 bytes, little endian   int dim[3];  };    Note that when the matrix has less than 3 dimensions, say, it's a 1D vector,  then dim[1] and dim[2] are both 1. When the matrix has more than 3 dimensions,  the header will be followed by further dimension size information. Otherwise,  after the file header comes the matrix data, which is stored with the index  in the last dimension changes the fastest.   The magic number encodes the element type of the matrix:   - 0x1E3D4C51 for a single precision matrix   - 0x1E3D4C52 for a packed matrix   - 0x1E3D4C53 for a double precision matrix   - 0x1E3D4C54 for an integer matrix   - 0x1E3D4C55 for a byte matrix   - 0x1E3D4C56 for a short matrix    Since the files are generated on an Intel machine, they use the little-endian  scheme to encode the 4-byte integers. Pay attention when you read the files on  machines that use big-endian.   - The \"-dat\" files store a 4D tensor of dimensions 29160x2x108x108.  - The \"-cat\" files store a 1D vector of dimension 29,160.  - The \"-info\" files store a 2D matrix of dimensions 29160x10.    Here's a piece of Matlab code to show how to read some example files. (to avoid the endian confusion, we read bytes of the header):    >> fid=fopen('norb-5x46789x9x18x6x2x108x108-training-10-dat.mat','r');   >> fread(fid,4,'uchar'); % result = [85 76 61 30], it's a byte matrix   >> fread(fid,4,'uchar'); % result = [4 0 0 0], ndim = 4   >> fread(fid,4,'uchar'); % result = [232 113 0 0], dim0 = 29160 (=113*256+232)   >> fread(fid,4,'uchar'); % result = [2 0 0 0],  dim1 = 2   >> fread(fid,4,'uchar'); % result = [108 0 0 0], dim2 = 108   >> fread(fid,4,'uchar'); % result = [108 0 0 0], dim3 = 108   >> imshow(transpose(reshape(fread(fid,108*108),108,108)),[0 255]); % show the first image     >> fid=fopen('norb-5x46789x9x18x6x2x108x108-training-10-cat.mat','r');   >> fread(fid,4,'uchar'); % [84 76 61 30], integer matrix   >> fread(fid,4,'uchar'); % [1 0 0 0] ndim = 1   >> fread(fid,4,'uchar'); % [232 113 0 0] dim0 = 29160 (=113*256+232)   >> fread(fid,4,'uchar'); % [1 0 0 0] (ignore this)   >> fread(fid,4,'uchar'); % [1 0 0 0] (ignore this)   >> fread(fid,10,'int'); % [0 1 2 3 4 5 0 1 2 3] (on little-endian CPU)     >> fid=fopen('norb-5x46789x9x18x6x2x108x108-training-10-info.mat','r');   >> fread(fid,4,'uchar'); % [84 76 61 30], integer matrix   >> fread(fid,4,'uchar'); % [2 0 0 0] ndim = 2   >> fread(fid,4,'uchar'); % [232 113 0 0] dim0 = 29160 (=113*256+232)   >> fread(fid,4,'uchar'); % [10 0 0 0] dim1 = 10   >> fread(fid,4,'uchar'); % [1 0 0 0] (ignore this)   >> fread(fid,10,'int'); % [8 5 10 4 -3 0 -6 1 0 -4] (on little-endian CPU)     Here is a screen shot of first 30 image pairs read from \"norb-5x46789x9x18x6x2x108x108-training-10-dat.mat\",  arranged topdown and left-to-right (column major). The caption below each pair shows the content from the  corresponding \"-cat.mat\" and \"-info.mat\" files. They are \"category / instance / elevation / azimuth / lighting\".  For the background images, the later 4 numbers are all -1.        DOWNLOAD     Please note that your web browser may uncompress the files without telling you . Check the file size to see if it's uncompressed!  The file size of \"*-cat.mat.gz\" is 0.4 KB, uncompressed to 116 KB   The file size of \"*-dat.mat.gz\" is 514 MB, uncompressed to 680 MB  The file size of \"*-info.mat.gz\" is 157 KB, uncompressed to 1.1 MB    readme (same as this page)   norb-5x01235x9x18x6x2x108x108-testing-01-cat.mat.gz   norb-5x01235x9x18x6x2x108x108-testing-01-dat.mat.gz   norb-5x01235x9x18x6x2x108x108-testing-01-info.mat.gz   norb-5x01235x9x18x6x2x108x108-testing-02-cat.mat.gz   norb-5x01235x9x18x6x2x108x108-testing-02-dat.mat.gz   norb-5x01235x9x18x6x2x108x108-testing-02-info.mat.gz   norb-5x46789x9x18x6x2x108x108-training-01-cat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-01-dat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-01-info.mat.gz   norb-5x46789x9x18x6x2x108x108-training-02-cat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-02-dat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-02-info.mat.gz   norb-5x46789x9x18x6x2x108x108-training-03-cat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-03-dat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-03-info.mat.gz   norb-5x46789x9x18x6x2x108x108-training-04-cat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-04-dat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-04-info.mat.gz   norb-5x46789x9x18x6x2x108x108-training-05-cat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-05-dat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-05-info.mat.gz   norb-5x46789x9x18x6x2x108x108-training-06-cat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-06-dat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-06-info.mat.gz   norb-5x46789x9x18x6x2x108x108-training-07-cat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-07-dat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-07-info.mat.gz   norb-5x46789x9x18x6x2x108x108-training-08-cat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-08-dat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-08-info.mat.gz   norb-5x46789x9x18x6x2x108x108-training-09-cat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-09-dat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-09-info.mat.gz   norb-5x46789x9x18x6x2x108x108-training-10-cat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-10-dat.mat.gz   norb-5x46789x9x18x6x2x108x108-training-10-info.mat.gz"}]}]