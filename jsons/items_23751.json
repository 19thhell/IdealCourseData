[{"detail": [{"content": "From illusions to inference: NYU course on human perception                         From illusions to inference:  Adventures in human perception                                               Why does one monster appear larger? (It is not.)   Roger N. Shepard's Terror Subterra                Why do we see A as darker than B?  (They are the same shade of gray.)   Edward Adelson's Checkerboard Shadow Illusion                    What is this guy saying? Then replay it with your eyes closed.   McGurk and MacDonald, 1976            3D sidewalk painting by Julian Beever   http://www.julianbeever.net             How can you fool a wine expert? (Quite easily.)  Why do healthy people hear spoken words in noise?  Why do we eat more when we see there is more food left?     Our sensory perception easily falls prey to illusions and biases. It is tempting to think of these as failures of our brain, but they might not be! In fact, they reveal the difficult challenges that our brain faces when interpreting the world, and the clever (and sometimes not so clever) solutions that it comes up with.    In this course, we will use a wide variety of well-known and lesser-known illusions (visual, auditory, tactile, vestibular, and multisensory) to work towards the central concept of inference : the notion that the brain constantly forms hypotheses about the outside world and tries to figure out which of them is most probable.    We will draw parallels with fun examples from online shopping to medical diagnosis to spam filtering to election forecasting to searching for submarines. We will have guest lectures by outside experts: one about neurological disorders of perception, about illusions in aviation, and about 3D illusory street art.            Topics will include...         Wine tastings in a French castle   Jay Johnson's Broadway show   How an illusion can crash a plane   Finding the U.S.S. Scorpion   The tale of the bottomless soup bowl   3D sidewalk drawings   Ames' Room : what you see is not what you get   Is Led Zeppelin satanic ?   How Nate Silver got famous How Netflix knows what you like McGurk effect   The train illusion   Borromini gallery   Misheard lyrics   James Turrell's \"Iltar\"   Cutaneous rabbit illusion   Visual variant of Alzheimer's   How Gmail filters your spam   The Alice in Wonderland syndrome   How we see how clothes feel   How babies learn syllables   Dalmatian dogs   The first scientist and his vision of vision   ...           Spring 2015 semester      PSYCH-UA 300.007    Non-majors welcome.  No prerequisites.      Time: Mondays 4-6:15 PM Location: Meyer 815       Instructor:  Wei Ji Ma, Ph.D.    Associate Professor in Neural Science and Psychology   Lab website Email: weijima@nyu.edu     Grading (tentative):  33.3% class participation  33.3% homework  33.3% final    Class participation will in part be measured using Turning Technologies, which will be provided for free for the duration of the class .    Homework will be based on the previous week's lecture (and lecture notes) and on assigned reading. Syllabus in PDF format"}]},
{"detail": [{"content": "From illusions to inference: NYU course on human perception                         From illusions to inference:  Adventures in human perception                                               Why does one monster appear larger? (It is not.)   Roger N. Shepard's Terror Subterra                Why do we see A as darker than B?  (They are the same shade of gray.)   Edward Adelson's Checkerboard Shadow Illusion                    What is this guy saying? Then replay it with your eyes closed.   McGurk and MacDonald, 1976            3D sidewalk painting by Julian Beever   http://www.julianbeever.net             How can you fool a wine expert? (Quite easily.)  Why do healthy people hear spoken words in noise?  Why do we eat more when we see there is more food left?     Our sensory perception easily falls prey to illusions and biases. It is tempting to think of these as failures of our brain, but they might not be! In fact, they reveal the difficult challenges that our brain faces when interpreting the world, and the clever (and sometimes not so clever) solutions that it comes up with.    In this course, we will use a wide variety of well-known and lesser-known illusions (visual, auditory, tactile, vestibular, and multisensory) to work towards the central concept of inference : the notion that the brain constantly forms hypotheses about the outside world and tries to figure out which of them is most probable.    We will draw parallels with fun examples from online shopping to medical diagnosis to spam filtering to election forecasting to searching for submarines. We will have guest lectures by outside experts: one about neurological disorders of perception, about illusions in aviation, and about 3D illusory street art.            Topics will include...         Wine tastings in a French castle   Jay Johnson's Broadway show   How an illusion can crash a plane   Finding the U.S.S. Scorpion   The tale of the bottomless soup bowl   3D sidewalk drawings   Ames' Room : what you see is not what you get   Is Led Zeppelin satanic ?   How Nate Silver got famous How Netflix knows what you like McGurk effect   The train illusion   Borromini gallery   Misheard lyrics   James Turrell's \"Iltar\"   Cutaneous rabbit illusion   Visual variant of Alzheimer's   How Gmail filters your spam   The Alice in Wonderland syndrome   How we see how clothes feel   How babies learn syllables   Dalmatian dogs   The first scientist and his vision of vision   ...           Spring 2015 semester      PSYCH-UA 300.007    Non-majors welcome.  No prerequisites.      Time: Mondays 4-6:15 PM Location: Meyer 815       Instructor:  Wei Ji Ma, Ph.D.    Associate Professor in Neural Science and Psychology   Lab website Email: weijima@nyu.edu     Grading (tentative):  33.3% class participation  33.3% homework  33.3% final    Class participation will in part be measured using Turning Technologies, which will be provided for free for the duration of the class .    Homework will be based on the previous week's lecture (and lecture notes) and on assigned reading. Syllabus in PDF format"}, {"content": "David Sontag             David Sontag's Home Page    E-mail: dsontag {@ | at} cs.nyu.edu   Phone: 212-998-3498 (office) Clinical machine learning group website    I am an Assistant Professor of  Computer Science and Data Science at NYU. Computer Science is part of the Courant Institute of Mathematical Sciences . My research focuses on machine learning and probabilistic inference, with a particular focus on applications to clinical medicine. For example, we are developing algorithms to learn probabilistic models for medical diagnosis directly from unstructured clinical data, automatically discovering and predicting latent (hidden) variables. We collaborate with the   Emergency Medicine Informatics Research Lab at Beth Israel Deaconess Medical Center and with Independence Blue Cross .     Previously, I was a post-doc  at Microsoft  Research New England . My Ph.D. is in Computer Science from MIT , where I worked with Tommi Jaakkola on approximate inference and learning in probabilistic models. My bachelors degree is from UC  Berkeley , in Computer  Science , where I worked with Stuart Russell 's First-Order Probabilistic Logic group.        Teaching  Fall 2014: Inference and Representation (DS-GA-1005 and CSCI-GA.2569)  Spring 2014: Machine Learning and Computational Statistics (DS-GA-1003 and CSCI-GA.2567)  Fall 2013: Introduction to Machine Learning (CSCI-UA.0480-002)  Spring 2013: Probabilistic Graphical Models (CSCI-GA.3033-006)        Publications      Theses:   D. Sontag. Approximate     Inference in Graphical Models using LP     Relaxations . Ph.D. thesis , Massachusetts Institute of Technology, 2010.   George M. Sprowls Award for the best doctoral theses in Computer      Science at MIT (2010) . BibTex   D. Sontag. Cutting Plane Algorithms for Variational Inference in Graphical Models . Master's thesis , Massachusetts Institute of Technology, 2007. BibTex    Machine learning:    New:    Download Python code for learning topic models (corresponds to ICML '13 paper). See also David Mimno's Mallet-compatible Java implementation .    New:    Download code for learning Bayesian network structure (corresponds to UAI '13 SparsityBoost paper).   Download C++ code for MAP inference in graphical models (corresponds to  UAI '12 paper; see readme file).   Download C++ code implementing our UAI '08 paper (see readme file).     Y. Jernite, S. Rush, D. Sontag. A Fast Variational Approach for Learning Markov Random Field Language Models. To appear in the 32nd International Conference on Machine Learning (ICML) , July 2015.  A. Globerson, T. Roughgarden, D. Sontag, C. Yildirim. How Hard is Inference for Structured Prediction? To appear in the 32nd International Conference on Machine Learning (ICML) , July 2015. [ arXiv ]  Y. Halpern, Y.D. Choi, S. Horng, D. Sontag. Using Anchors to Estimate Clinical State without Labeled Data . American Medical Informatics Association (AMIA) Annual Symposium , Nov. 2014. [ Slides ] BibTex   N. Silberman, D. Sontag, R. Fergus. Instance Segmentation of Indoor Scenes using a Coverage Loss . To appear in the European Conference on Computer Vision (ECCV) , Sept. 2014. BibTex   X. Wang, D. Sontag, F. Wang. Unsupervised Learning of Disease Progression Models . ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) , Aug. 2014. [ Slides ] BibTex   H. Bui, T. Huynh, D. Sontag. Lifted Tree-Reweighted Variational Inference . Uncertainty in Artificial Intelligence (UAI) , July 2014. Addendum BibTex   A. Weller, K. Tang, D. Sontag, T. Jebara. Understanding the Bethe approximation: when and how can it go wrong? Uncertainty in Artificial Intelligence (UAI) , July 2014. BibTex   Y. Jernite, Y. Halpern, D. Sontag. Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests . Neural Information Processing Systems (NIPS) 26 , Dec. 2013. Supplementary [ Code ] BibTex   Y. Jernite, Y. Halpern, S. Horng, D. Sontag. Predicting Chief Complaints at Triage Time in the Emergency Department . NIPS 2013 Workshop on Machine Learning for Clinical Data Analysis and Healthcare , Dec. 2013. BibTex   E. Brenner, D. Sontag. SparsityBoost: A New Scoring Function for Learning Bayesian Network Structure . Uncertainty in Artificial Intelligence (UAI) 29 , July 2013. BibTex   Y. Halpern, D. Sontag. Unsupervised Learning of Noisy-Or Bayesian Networks . Uncertainty in Artificial Intelligence (UAI) 29 , July 2013. BibTex   S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, M. Zhu. A Practical Algorithm for Topic Modeling with Provable Guarantees . 30th International Conference on Machine Learning (ICML) , 2013. Supplementary BibTex   D. Sontag, D. K. Choe, Y. Li. Efficiently Searching for Frustrated Cycles in MAP Inference . Uncertainty in Artificial Intelligence (UAI) 28 , Aug. 2012. Supplementary BibTex   Y. Halpern, S. Horng, L. A. Nathanson, N. I. Shapiro, D. Sontag. A Comparison of Dimensionality Reduction Techniques for Unstructured Clinical Text . ICML 2012 Workshop on Clinical Data Analysis , July 2012. BibTex   D. Sontag, K. Collins-Thompson, P. N. Bennett, R. W. White, S. Dumais, B. Billerbeck. Probabilistic Models for Personalizing Web Search . Fifth ACM International Conference on Web Search and Data Mining (WSDM) , Feb. 2012. [Slides]   BibTex    D. Sontag, D. Roy. Complexity of Inference in Latent Dirichlet Allocation . Neural Information Processing Systems (NIPS)  24 , Dec. 2011. [Slides]   BibTex    K. Collins-Thompson, P. N. Bennett, R. W. White, S. de la Chica, D. Sontag. Personalizing Web Search Results by Reading Level . Twentieth ACM International Conference on Information and Knowledge Management (CIKM 2011) , Oct. 2011.  BibTex    D. Sontag, A. Globerson, T. Jaakkola. Introduction to Dual Decomposition for Inference . Optimization for Machine Learning , editors S. Sra, S. Nowozin, and S. J. Wright: MIT Press, 2011.  BibTex   D. Sontag, O. Meshi, T. Jaakkola,  A. Globerson. More data means less  inference: A pseudo-max approach to structured learning .  Neural Information Processing Systems (NIPS)  23 , Dec. 2010. Supplementary   BibTex   T. Koo, A. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual Decomposition for Parsing with Non-Projective Head Automata . Empirical Methods in Natural Language Processing (EMNLP) , 2010. Best paper award . BibTex   A. Rush, D. Sontag, M. Collins, and T. Jaakkola.  On Dual Decomposition and       Linear Programming       Relaxations for       Natural Language       Processing . Empirical Methods in Natural Language Processing (EMNLP) , 2010. BibTex   O. Meshi, D. Sontag, T. Jaakkola, A. Globerson. Learning Efficiently with Approximate Inference via Dual Losses . 27th International Conference on Machine Learning (ICML) , July 2010.  BibTex   T. Jaakkola, D. Sontag, A. Globerson,  M. Meila. Learning  Bayesian Network Structure using LP Relaxations . 13th International Conference on Artificial Intelligence  and Statistics (AI-STATS) ,  2010. BibTex   D. Sontag, T. Jaakkola. Tree Block Coordinate Descent for MAP in Graphical Models . 12th International Conference on Artificial Intelligence and Statistics (AI-STATS) , April 2009. BibTex    D. Sontag, A. Globerson, T. Jaakkola. Clusters and Coarse Partitions in LP Relaxations . Neural Information Processing Systems (NIPS) 21 , Dec. 2008. BibTex    D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, T. Jaakkola. Tightening LP Relaxations for MAP using Message Passing . Uncertainty in Artificial Intelligence (UAI) 24 , July 2008. Best paper award . BibTex    D. Sontag, T. Jaakkola. New Outer Bounds on the Marginal Polytope . Neural Information Processing Systems (NIPS) 20 , Dec. 2007. Outstanding student paper award . Addendum BibTex   B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic Models with Unknown Objects . In Lise Getoor and Ben Taskar, eds. Statistical Relational Learning. Cambridge, MA: MIT Press , 2007.  B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic Models with Unknown Objects . Proc. 19th International Joint Conference on Artificial Intelligence (IJCAI): 1352-1359 , 2005. BibTex   B. Milch, B. Marthi, D. Sontag, S. Russell, D. L. Ong, and A. Kolobov. Approximate Inference for Infinite Contingent Bayesian Networks . 10th International Workshop on Artificial Intelligence and Statistics , 2005. BibTex    Computer networking:   D. Sontag, Y. Zhang, A. Phanishayee, D. Andersen, D. Karger. Scaling All-Pairs Overlay Routing .  Fifth ACM International Conference on emerging  Networking EXperiments and Technologies (CoNEXT) , Dec. 2009. Code BibTex     Computational biology:   D. Sontag, R. Singh, B. Berger. Probabilistic Modeling of Systematic Errors in Two-Hybrid Experiments .  Pacific Symposium on Biocomputing (PSB), 2007 . Supplementary information BibTex"}]},
{"detail": [{"content": "From illusions to inference: NYU course on human perception                         From illusions to inference:  Adventures in human perception                                               Why does one monster appear larger? (It is not.)   Roger N. Shepard's Terror Subterra                Why do we see A as darker than B?  (They are the same shade of gray.)   Edward Adelson's Checkerboard Shadow Illusion                    What is this guy saying? Then replay it with your eyes closed.   McGurk and MacDonald, 1976            3D sidewalk painting by Julian Beever   http://www.julianbeever.net             How can you fool a wine expert? (Quite easily.)  Why do healthy people hear spoken words in noise?  Why do we eat more when we see there is more food left?     Our sensory perception easily falls prey to illusions and biases. It is tempting to think of these as failures of our brain, but they might not be! In fact, they reveal the difficult challenges that our brain faces when interpreting the world, and the clever (and sometimes not so clever) solutions that it comes up with.    In this course, we will use a wide variety of well-known and lesser-known illusions (visual, auditory, tactile, vestibular, and multisensory) to work towards the central concept of inference : the notion that the brain constantly forms hypotheses about the outside world and tries to figure out which of them is most probable.    We will draw parallels with fun examples from online shopping to medical diagnosis to spam filtering to election forecasting to searching for submarines. We will have guest lectures by outside experts: one about neurological disorders of perception, about illusions in aviation, and about 3D illusory street art.            Topics will include...         Wine tastings in a French castle   Jay Johnson's Broadway show   How an illusion can crash a plane   Finding the U.S.S. Scorpion   The tale of the bottomless soup bowl   3D sidewalk drawings   Ames' Room : what you see is not what you get   Is Led Zeppelin satanic ?   How Nate Silver got famous How Netflix knows what you like McGurk effect   The train illusion   Borromini gallery   Misheard lyrics   James Turrell's \"Iltar\"   Cutaneous rabbit illusion   Visual variant of Alzheimer's   How Gmail filters your spam   The Alice in Wonderland syndrome   How we see how clothes feel   How babies learn syllables   Dalmatian dogs   The first scientist and his vision of vision   ...           Spring 2015 semester      PSYCH-UA 300.007    Non-majors welcome.  No prerequisites.      Time: Mondays 4-6:15 PM Location: Meyer 815       Instructor:  Wei Ji Ma, Ph.D.    Associate Professor in Neural Science and Psychology   Lab website Email: weijima@nyu.edu     Grading (tentative):  33.3% class participation  33.3% homework  33.3% final    Class participation will in part be measured using Turning Technologies, which will be provided for free for the duration of the class .    Homework will be based on the previous week's lecture (and lecture notes) and on assigned reading. Syllabus in PDF format"}, {"content": "David Sontag             David Sontag's Home Page    E-mail: dsontag {@ | at} cs.nyu.edu   Phone: 212-998-3498 (office) Clinical machine learning group website    I am an Assistant Professor of  Computer Science and Data Science at NYU. Computer Science is part of the Courant Institute of Mathematical Sciences . My research focuses on machine learning and probabilistic inference, with a particular focus on applications to clinical medicine. For example, we are developing algorithms to learn probabilistic models for medical diagnosis directly from unstructured clinical data, automatically discovering and predicting latent (hidden) variables. We collaborate with the   Emergency Medicine Informatics Research Lab at Beth Israel Deaconess Medical Center and with Independence Blue Cross .     Previously, I was a post-doc  at Microsoft  Research New England . My Ph.D. is in Computer Science from MIT , where I worked with Tommi Jaakkola on approximate inference and learning in probabilistic models. My bachelors degree is from UC  Berkeley , in Computer  Science , where I worked with Stuart Russell 's First-Order Probabilistic Logic group.        Teaching  Fall 2014: Inference and Representation (DS-GA-1005 and CSCI-GA.2569)  Spring 2014: Machine Learning and Computational Statistics (DS-GA-1003 and CSCI-GA.2567)  Fall 2013: Introduction to Machine Learning (CSCI-UA.0480-002)  Spring 2013: Probabilistic Graphical Models (CSCI-GA.3033-006)        Publications      Theses:   D. Sontag. Approximate     Inference in Graphical Models using LP     Relaxations . Ph.D. thesis , Massachusetts Institute of Technology, 2010.   George M. Sprowls Award for the best doctoral theses in Computer      Science at MIT (2010) . BibTex   D. Sontag. Cutting Plane Algorithms for Variational Inference in Graphical Models . Master's thesis , Massachusetts Institute of Technology, 2007. BibTex    Machine learning:    New:    Download Python code for learning topic models (corresponds to ICML '13 paper). See also David Mimno's Mallet-compatible Java implementation .    New:    Download code for learning Bayesian network structure (corresponds to UAI '13 SparsityBoost paper).   Download C++ code for MAP inference in graphical models (corresponds to  UAI '12 paper; see readme file).   Download C++ code implementing our UAI '08 paper (see readme file).     Y. Jernite, S. Rush, D. Sontag. A Fast Variational Approach for Learning Markov Random Field Language Models. To appear in the 32nd International Conference on Machine Learning (ICML) , July 2015.  A. Globerson, T. Roughgarden, D. Sontag, C. Yildirim. How Hard is Inference for Structured Prediction? To appear in the 32nd International Conference on Machine Learning (ICML) , July 2015. [ arXiv ]  Y. Halpern, Y.D. Choi, S. Horng, D. Sontag. Using Anchors to Estimate Clinical State without Labeled Data . American Medical Informatics Association (AMIA) Annual Symposium , Nov. 2014. [ Slides ] BibTex   N. Silberman, D. Sontag, R. Fergus. Instance Segmentation of Indoor Scenes using a Coverage Loss . To appear in the European Conference on Computer Vision (ECCV) , Sept. 2014. BibTex   X. Wang, D. Sontag, F. Wang. Unsupervised Learning of Disease Progression Models . ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) , Aug. 2014. [ Slides ] BibTex   H. Bui, T. Huynh, D. Sontag. Lifted Tree-Reweighted Variational Inference . Uncertainty in Artificial Intelligence (UAI) , July 2014. Addendum BibTex   A. Weller, K. Tang, D. Sontag, T. Jebara. Understanding the Bethe approximation: when and how can it go wrong? Uncertainty in Artificial Intelligence (UAI) , July 2014. BibTex   Y. Jernite, Y. Halpern, D. Sontag. Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests . Neural Information Processing Systems (NIPS) 26 , Dec. 2013. Supplementary [ Code ] BibTex   Y. Jernite, Y. Halpern, S. Horng, D. Sontag. Predicting Chief Complaints at Triage Time in the Emergency Department . NIPS 2013 Workshop on Machine Learning for Clinical Data Analysis and Healthcare , Dec. 2013. BibTex   E. Brenner, D. Sontag. SparsityBoost: A New Scoring Function for Learning Bayesian Network Structure . Uncertainty in Artificial Intelligence (UAI) 29 , July 2013. BibTex   Y. Halpern, D. Sontag. Unsupervised Learning of Noisy-Or Bayesian Networks . Uncertainty in Artificial Intelligence (UAI) 29 , July 2013. BibTex   S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, M. Zhu. A Practical Algorithm for Topic Modeling with Provable Guarantees . 30th International Conference on Machine Learning (ICML) , 2013. Supplementary BibTex   D. Sontag, D. K. Choe, Y. Li. Efficiently Searching for Frustrated Cycles in MAP Inference . Uncertainty in Artificial Intelligence (UAI) 28 , Aug. 2012. Supplementary BibTex   Y. Halpern, S. Horng, L. A. Nathanson, N. I. Shapiro, D. Sontag. A Comparison of Dimensionality Reduction Techniques for Unstructured Clinical Text . ICML 2012 Workshop on Clinical Data Analysis , July 2012. BibTex   D. Sontag, K. Collins-Thompson, P. N. Bennett, R. W. White, S. Dumais, B. Billerbeck. Probabilistic Models for Personalizing Web Search . Fifth ACM International Conference on Web Search and Data Mining (WSDM) , Feb. 2012. [Slides]   BibTex    D. Sontag, D. Roy. Complexity of Inference in Latent Dirichlet Allocation . Neural Information Processing Systems (NIPS)  24 , Dec. 2011. [Slides]   BibTex    K. Collins-Thompson, P. N. Bennett, R. W. White, S. de la Chica, D. Sontag. Personalizing Web Search Results by Reading Level . Twentieth ACM International Conference on Information and Knowledge Management (CIKM 2011) , Oct. 2011.  BibTex    D. Sontag, A. Globerson, T. Jaakkola. Introduction to Dual Decomposition for Inference . Optimization for Machine Learning , editors S. Sra, S. Nowozin, and S. J. Wright: MIT Press, 2011.  BibTex   D. Sontag, O. Meshi, T. Jaakkola,  A. Globerson. More data means less  inference: A pseudo-max approach to structured learning .  Neural Information Processing Systems (NIPS)  23 , Dec. 2010. Supplementary   BibTex   T. Koo, A. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual Decomposition for Parsing with Non-Projective Head Automata . Empirical Methods in Natural Language Processing (EMNLP) , 2010. Best paper award . BibTex   A. Rush, D. Sontag, M. Collins, and T. Jaakkola.  On Dual Decomposition and       Linear Programming       Relaxations for       Natural Language       Processing . Empirical Methods in Natural Language Processing (EMNLP) , 2010. BibTex   O. Meshi, D. Sontag, T. Jaakkola, A. Globerson. Learning Efficiently with Approximate Inference via Dual Losses . 27th International Conference on Machine Learning (ICML) , July 2010.  BibTex   T. Jaakkola, D. Sontag, A. Globerson,  M. Meila. Learning  Bayesian Network Structure using LP Relaxations . 13th International Conference on Artificial Intelligence  and Statistics (AI-STATS) ,  2010. BibTex   D. Sontag, T. Jaakkola. Tree Block Coordinate Descent for MAP in Graphical Models . 12th International Conference on Artificial Intelligence and Statistics (AI-STATS) , April 2009. BibTex    D. Sontag, A. Globerson, T. Jaakkola. Clusters and Coarse Partitions in LP Relaxations . Neural Information Processing Systems (NIPS) 21 , Dec. 2008. BibTex    D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, T. Jaakkola. Tightening LP Relaxations for MAP using Message Passing . Uncertainty in Artificial Intelligence (UAI) 24 , July 2008. Best paper award . BibTex    D. Sontag, T. Jaakkola. New Outer Bounds on the Marginal Polytope . Neural Information Processing Systems (NIPS) 20 , Dec. 2007. Outstanding student paper award . Addendum BibTex   B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic Models with Unknown Objects . In Lise Getoor and Ben Taskar, eds. Statistical Relational Learning. Cambridge, MA: MIT Press , 2007.  B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic Models with Unknown Objects . Proc. 19th International Joint Conference on Artificial Intelligence (IJCAI): 1352-1359 , 2005. BibTex   B. Milch, B. Marthi, D. Sontag, S. Russell, D. L. Ong, and A. Kolobov. Approximate Inference for Infinite Contingent Bayesian Networks . 10th International Workshop on Artificial Intelligence and Statistics , 2005. BibTex    Computer networking:   D. Sontag, Y. Zhang, A. Phanishayee, D. Andersen, D. Karger. Scaling All-Pairs Overlay Routing .  Fifth ACM International Conference on emerging  Networking EXperiments and Technologies (CoNEXT) , Dec. 2009. Code BibTex     Computational biology:   D. Sontag, R. Singh, B. Berger. Probabilistic Modeling of Systematic Errors in Two-Hybrid Experiments .  Pacific Symposium on Biocomputing (PSB), 2007 . Supplementary information BibTex"}, {"content": "Inference and Representation, Fall 2014        Inference and Representation        DS-GA-1005 and CSCI-GA.2569 Fall   201 4                    Overview                     This course covers graphical models, causal inference, and     advanced topics in statistical machine learning. A     graphical model is a probabilistic model, where the     conditional dependencies between the random variables are     specified via a graph. Graphical models provide a flexible     framework for modeling large collections of variables with     complex interactions, as evidenced by their wide domain of     application, including for example machine learning,     computer vision, speech and computational biology. This     course will provide a comprehensive survey of learning and     inference methods in graphical models.                                  Prerequisite: DS-GA-1003/CSCI-GA.2567                                     ( Machine                                      Learning and Computational Statistics ),                                or permission of instructor. Please note        that seats are limited and exceptions to the        prerequisite are only likely to be granted        to PhD students.                             General information      Lecture : Tuesdays, 7:10-9:00pm, in     Warren Weaver Hall 101      Recitation/Laboratory      (required for all students) : Thursdays,     5:10-6:00pm in Silver                                  Center 401                              Instructor :                                     David Sontag                       dsontag {@ | at} cs.nyu.edu                    Lab instructor :                      Yacine Jernite                             jernite {@ | at} cs.nyu.edu                    Grader :                             Prasoon Goyal                             pg1338 {@ | at} nyu.edu                                   Office hours: Tuesdays,     10:30-11:30am. Location:           Center for Data Science, 726 Broadway, 7th Floor            Grading : problem sets (55%) + midterm     exam (20%) + final exam (20%) + participation (5%). Problem Set policy           Book (required) : Kevin Murphy, Machine                                                                   Learning: a Probabilistic Perspective , MIT Press,     2012. I recommend the latest (4th) printing, as the     earlier editions had many typos. You can     tell which printing you have as follows: check the     inside cover, below the \"Library of Congress\"     information. If it says \"10 9 8 ... 4\" you've got the     (correct) fourth print.                Mailing list: To subscribe to the class list,     follow instructions here .                      Draft Schedule   (Readings refer to the Murphy book)                     Week      Date      Topic      Readings      Assignments             1     Sept 2         Introduction, Bayesian networks     [ Slides ]         Chapter 10 (Bayesian networks)     Sec. 17.3, 17.4, 17.5.1 (HMMs)           How to write     a spelling corrector (optional)           Introduction                                     to Probabilistic Topic Models (optional)                   ps1 due Sept. 15 at     10pm      [ Solutions ]                      2     Sept 9         Undirected graphical models [ Slides ]          Conditional random fields         Sec. 9-9.2.5 (exponential family)     Sec. 19-19.4.4, 19.6 (MRFs, CRFs)           An Introduction     to Conditional Random Fields (section 2)           Original                                     paper introducing CRFs (optional)           Application                       to camera lens blur (optional)           Application                       to predicting wind flow (optional)                                   3         Sept 16          Learning and causal inference     [ Slides ] [ BN                      structure slides ]          Maximum likelihood learning, structure learning, causal    networks     Sec. 26-26.6 (structure learning &amp;    causal inference)     Sec. 2.8.2 (KL-divergence)           Overview                            of causal inference in statistics by Judea Pearl (sec.    3.2; optional)           Paper on     causal inference used for computational advertising     (optional)           Science                            paper on learning gene regulatory networks (optional)           Learning                   causal networks using interventions (optional)         ps2 due    Sept. 26 at 5pm     [ Solutions ]                 4         Sept 23         Exact inference [ Slides ]          Variable elimination, treewidth         Chapter 20              ps3 ( data ) due Oct. 10 at     5pm                 5         Sept 30         Belief propagation     [ Slides ] [ Notes ]         Sec. 22.2 (loopy belief propagation)           Using loopy BP     for stereo vision (optional)                              6         Oct 7         Integer linear programming      (MAP inference) [ Slides ]          Linear programming relaxations, dual decomposition     Sec. 22.6 (MAP inference)           Derivation     relating dual decomposition &amp; LP relaxations           Introduction                                     to Dual Decomposition for Inference (optional)           Integer               Programming for Bayesian Network Structure Learning     (optional)                                  Oct 21          (no class Oct 14, Fall Recess; will hold office    hours on 10/14)          Midterm exam      (in class)                                           7         Oct 28         Variational inference [ Slides ]          Mean-field approximation     Sec. 21.1-21.4           Graphical                                  models, exponential families, and variational inference     (optional)           Stochastic             Variational Inference (optional)              ps4 ( data ) due Nov. 17 at     5pm                      8         Nov 4         Monte-Carlo methods for inference [ Slides 1 ] [ Slides          2 ]          Importance sampling, Metropolis Hastings, Gibbs sampling         Sec. 23.1-23.4     Sec. 17.2.3     Sec. 24.1-24.4                                   9         Nov 11         More variational inference [ Slides ]           TRW, loopy BP         Sec. 22.3-22.4                                   10         Nov 18     Discovering latent variables using the     method-of-moments      [ Slides 1 ]    [ Slides 2 ]            Guest         lecture by Daniel         Hsu          A Method of     Moments for Mixture Models and Hidden Markov Models     (Sec. 1 &amp; 2)           Method        of Moments for Topic Modeling (optional)                         11     Nov 25     Structured prediction     [ Slides ]         Sec. 19.7 (also, review 19.6 again)           Tutorial       on Structured Prediction (optional)           Original                                  paper introducing structured perceptron (optional)           Cutting-Plane                                  Training of Structural SVMs (optional)           Block-Coordinate                                  Frank-Wolfe Optimization for Structural SVMs     (optional)          ps5 ( data ) due Dec. 8 at     5pm                      12     Dec 2         Learning Markov networks      [ Slides 1 ] [ Slides      2 ]          Pseudo-likelihood, deep generative models          Sec. 19.5, 26.7-26.8     Sec. 27.7, 28.2           Notes on     pseudo-likelihood           An Introduction     to Conditional Random Fields (section 4; optional)           Approximate maximum     entropy learning in MRFs (optional)                         13     Dec 9         Advanced topics in learning           Probabilistic programming                  Talk     for recent ICML 2014 paper on deep    generative models (optional)           Probabilistic     programming: overview and bibliography (optional)           Probabilistic     programming: BUGS (optional)           Probabilistic programming:     Church book (optional)                                  Dec 16          Final exam (in class)                                     Acknowledgements : Many thanks to the Toyota Technological  Institute, Hebrew University, UC Berkeley, and Stanford University  for sharing material used in slides and homeworks                          Reference materials           Optional course text: Probabilistic      Graphical Models: Principles and Techniques by     Daphne Koller and Nir Friedman, MIT Press (2009).      Machine learning books            Trevor Hastie, Rob Tibshirani, and Jerry Friedman, Elements of      Statistical Learning , Second Edition, Springer,      2009. (Can be downloaded as PDF file.)       David Barber, Bayesian                                                                   Reasoning and Machine Learning , Cambridge      University Press, 2012. (Can be downloaded as PDF      file.)           Probability            Chapter 2 of Murphy             Review                                                                    notes from Stanford's machine learning class       Sam Roweis's probability                                                                    review           Optimization            Convex                                                                    Optimization by Stephen Boyd and Lieven      Vandenberghe. (Can be downloaded as PDF file.)                                  Problem                                      Set policy            I expect you to try solving each problem set on your own.    However, when being stuck on a problem, I encourage     you to collaborate with other students in the class, subject    to the following rules:               You may discuss a problem with any student in this     class, and work together on solving it. This can involve     brainstorming and verbally discussing the problem, going     together through possible solutions, but should not      involve one student telling another a complete solution.                Once you solve the homework, you must write up      your solutions on your own , without looking at     other people's write-ups or giving your write-up to     others.                In your solution for each problem, you must write      down the names of any person with whom you     discussed it. This will not affect your grade.                Do not consult solution manuals or other people's     solutions from similar courses.           Late submission policy : During the    semester you are allowed at most two extensions on the    homework assignment. Each extension is for at most 48 hours    and carries a penalty of 25% off your assignment."}]}][{"detail": [{"content": "From illusions to inference: NYU course on human perception                         From illusions to inference:  Adventures in human perception                                               Why does one monster appear larger? (It is not.)   Roger N. Shepard's Terror Subterra                Why do we see A as darker than B?  (They are the same shade of gray.)   Edward Adelson's Checkerboard Shadow Illusion                    What is this guy saying? Then replay it with your eyes closed.   McGurk and MacDonald, 1976            3D sidewalk painting by Julian Beever   http://www.julianbeever.net             How can you fool a wine expert? (Quite easily.)  Why do healthy people hear spoken words in noise?  Why do we eat more when we see there is more food left?     Our sensory perception easily falls prey to illusions and biases. It is tempting to think of these as failures of our brain, but they might not be! In fact, they reveal the difficult challenges that our brain faces when interpreting the world, and the clever (and sometimes not so clever) solutions that it comes up with.    In this course, we will use a wide variety of well-known and lesser-known illusions (visual, auditory, tactile, vestibular, and multisensory) to work towards the central concept of inference : the notion that the brain constantly forms hypotheses about the outside world and tries to figure out which of them is most probable.    We will draw parallels with fun examples from online shopping to medical diagnosis to spam filtering to election forecasting to searching for submarines. We will have guest lectures by outside experts: one about neurological disorders of perception, about illusions in aviation, and about 3D illusory street art.            Topics will include...         Wine tastings in a French castle   Jay Johnson's Broadway show   How an illusion can crash a plane   Finding the U.S.S. Scorpion   The tale of the bottomless soup bowl   3D sidewalk drawings   Ames' Room : what you see is not what you get   Is Led Zeppelin satanic ?   How Nate Silver got famous How Netflix knows what you like McGurk effect   The train illusion   Borromini gallery   Misheard lyrics   James Turrell's \"Iltar\"   Cutaneous rabbit illusion   Visual variant of Alzheimer's   How Gmail filters your spam   The Alice in Wonderland syndrome   How we see how clothes feel   How babies learn syllables   Dalmatian dogs   The first scientist and his vision of vision   ...           Spring 2015 semester      PSYCH-UA 300.007    Non-majors welcome.  No prerequisites.      Time: Mondays 4-6:15 PM Location: Meyer 815       Instructor:  Wei Ji Ma, Ph.D.    Associate Professor in Neural Science and Psychology   Lab website Email: weijima@nyu.edu     Grading (tentative):  33.3% class participation  33.3% homework  33.3% final    Class participation will in part be measured using Turning Technologies, which will be provided for free for the duration of the class .    Homework will be based on the previous week's lecture (and lecture notes) and on assigned reading. Syllabus in PDF format"}]},
{"detail": [{"content": "From illusions to inference: NYU course on human perception                         From illusions to inference:  Adventures in human perception                                               Why does one monster appear larger? (It is not.)   Roger N. Shepard's Terror Subterra                Why do we see A as darker than B?  (They are the same shade of gray.)   Edward Adelson's Checkerboard Shadow Illusion                    What is this guy saying? Then replay it with your eyes closed.   McGurk and MacDonald, 1976            3D sidewalk painting by Julian Beever   http://www.julianbeever.net             How can you fool a wine expert? (Quite easily.)  Why do healthy people hear spoken words in noise?  Why do we eat more when we see there is more food left?     Our sensory perception easily falls prey to illusions and biases. It is tempting to think of these as failures of our brain, but they might not be! In fact, they reveal the difficult challenges that our brain faces when interpreting the world, and the clever (and sometimes not so clever) solutions that it comes up with.    In this course, we will use a wide variety of well-known and lesser-known illusions (visual, auditory, tactile, vestibular, and multisensory) to work towards the central concept of inference : the notion that the brain constantly forms hypotheses about the outside world and tries to figure out which of them is most probable.    We will draw parallels with fun examples from online shopping to medical diagnosis to spam filtering to election forecasting to searching for submarines. We will have guest lectures by outside experts: one about neurological disorders of perception, about illusions in aviation, and about 3D illusory street art.            Topics will include...         Wine tastings in a French castle   Jay Johnson's Broadway show   How an illusion can crash a plane   Finding the U.S.S. Scorpion   The tale of the bottomless soup bowl   3D sidewalk drawings   Ames' Room : what you see is not what you get   Is Led Zeppelin satanic ?   How Nate Silver got famous How Netflix knows what you like McGurk effect   The train illusion   Borromini gallery   Misheard lyrics   James Turrell's \"Iltar\"   Cutaneous rabbit illusion   Visual variant of Alzheimer's   How Gmail filters your spam   The Alice in Wonderland syndrome   How we see how clothes feel   How babies learn syllables   Dalmatian dogs   The first scientist and his vision of vision   ...           Spring 2015 semester      PSYCH-UA 300.007    Non-majors welcome.  No prerequisites.      Time: Mondays 4-6:15 PM Location: Meyer 815       Instructor:  Wei Ji Ma, Ph.D.    Associate Professor in Neural Science and Psychology   Lab website Email: weijima@nyu.edu     Grading (tentative):  33.3% class participation  33.3% homework  33.3% final    Class participation will in part be measured using Turning Technologies, which will be provided for free for the duration of the class .    Homework will be based on the previous week's lecture (and lecture notes) and on assigned reading. Syllabus in PDF format"}, {"content": "David Sontag             David Sontag's Home Page    E-mail: dsontag {@ | at} cs.nyu.edu   Phone: 212-998-3498 (office) Clinical machine learning group website    I am an Assistant Professor of  Computer Science and Data Science at NYU. Computer Science is part of the Courant Institute of Mathematical Sciences . My research focuses on machine learning and probabilistic inference, with a particular focus on applications to clinical medicine. For example, we are developing algorithms to learn probabilistic models for medical diagnosis directly from unstructured clinical data, automatically discovering and predicting latent (hidden) variables. We collaborate with the   Emergency Medicine Informatics Research Lab at Beth Israel Deaconess Medical Center and with Independence Blue Cross .     Previously, I was a post-doc  at Microsoft  Research New England . My Ph.D. is in Computer Science from MIT , where I worked with Tommi Jaakkola on approximate inference and learning in probabilistic models. My bachelors degree is from UC  Berkeley , in Computer  Science , where I worked with Stuart Russell 's First-Order Probabilistic Logic group.        Teaching  Fall 2014: Inference and Representation (DS-GA-1005 and CSCI-GA.2569)  Spring 2014: Machine Learning and Computational Statistics (DS-GA-1003 and CSCI-GA.2567)  Fall 2013: Introduction to Machine Learning (CSCI-UA.0480-002)  Spring 2013: Probabilistic Graphical Models (CSCI-GA.3033-006)        Publications      Theses:   D. Sontag. Approximate     Inference in Graphical Models using LP     Relaxations . Ph.D. thesis , Massachusetts Institute of Technology, 2010.   George M. Sprowls Award for the best doctoral theses in Computer      Science at MIT (2010) . BibTex   D. Sontag. Cutting Plane Algorithms for Variational Inference in Graphical Models . Master's thesis , Massachusetts Institute of Technology, 2007. BibTex    Machine learning:    New:    Download Python code for learning topic models (corresponds to ICML '13 paper). See also David Mimno's Mallet-compatible Java implementation .    New:    Download code for learning Bayesian network structure (corresponds to UAI '13 SparsityBoost paper).   Download C++ code for MAP inference in graphical models (corresponds to  UAI '12 paper; see readme file).   Download C++ code implementing our UAI '08 paper (see readme file).     Y. Jernite, S. Rush, D. Sontag. A Fast Variational Approach for Learning Markov Random Field Language Models. To appear in the 32nd International Conference on Machine Learning (ICML) , July 2015.  A. Globerson, T. Roughgarden, D. Sontag, C. Yildirim. How Hard is Inference for Structured Prediction? To appear in the 32nd International Conference on Machine Learning (ICML) , July 2015. [ arXiv ]  Y. Halpern, Y.D. Choi, S. Horng, D. Sontag. Using Anchors to Estimate Clinical State without Labeled Data . American Medical Informatics Association (AMIA) Annual Symposium , Nov. 2014. [ Slides ] BibTex   N. Silberman, D. Sontag, R. Fergus. Instance Segmentation of Indoor Scenes using a Coverage Loss . To appear in the European Conference on Computer Vision (ECCV) , Sept. 2014. BibTex   X. Wang, D. Sontag, F. Wang. Unsupervised Learning of Disease Progression Models . ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) , Aug. 2014. [ Slides ] BibTex   H. Bui, T. Huynh, D. Sontag. Lifted Tree-Reweighted Variational Inference . Uncertainty in Artificial Intelligence (UAI) , July 2014. Addendum BibTex   A. Weller, K. Tang, D. Sontag, T. Jebara. Understanding the Bethe approximation: when and how can it go wrong? Uncertainty in Artificial Intelligence (UAI) , July 2014. BibTex   Y. Jernite, Y. Halpern, D. Sontag. Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests . Neural Information Processing Systems (NIPS) 26 , Dec. 2013. Supplementary [ Code ] BibTex   Y. Jernite, Y. Halpern, S. Horng, D. Sontag. Predicting Chief Complaints at Triage Time in the Emergency Department . NIPS 2013 Workshop on Machine Learning for Clinical Data Analysis and Healthcare , Dec. 2013. BibTex   E. Brenner, D. Sontag. SparsityBoost: A New Scoring Function for Learning Bayesian Network Structure . Uncertainty in Artificial Intelligence (UAI) 29 , July 2013. BibTex   Y. Halpern, D. Sontag. Unsupervised Learning of Noisy-Or Bayesian Networks . Uncertainty in Artificial Intelligence (UAI) 29 , July 2013. BibTex   S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, M. Zhu. A Practical Algorithm for Topic Modeling with Provable Guarantees . 30th International Conference on Machine Learning (ICML) , 2013. Supplementary BibTex   D. Sontag, D. K. Choe, Y. Li. Efficiently Searching for Frustrated Cycles in MAP Inference . Uncertainty in Artificial Intelligence (UAI) 28 , Aug. 2012. Supplementary BibTex   Y. Halpern, S. Horng, L. A. Nathanson, N. I. Shapiro, D. Sontag. A Comparison of Dimensionality Reduction Techniques for Unstructured Clinical Text . ICML 2012 Workshop on Clinical Data Analysis , July 2012. BibTex   D. Sontag, K. Collins-Thompson, P. N. Bennett, R. W. White, S. Dumais, B. Billerbeck. Probabilistic Models for Personalizing Web Search . Fifth ACM International Conference on Web Search and Data Mining (WSDM) , Feb. 2012. [Slides]   BibTex    D. Sontag, D. Roy. Complexity of Inference in Latent Dirichlet Allocation . Neural Information Processing Systems (NIPS)  24 , Dec. 2011. [Slides]   BibTex    K. Collins-Thompson, P. N. Bennett, R. W. White, S. de la Chica, D. Sontag. Personalizing Web Search Results by Reading Level . Twentieth ACM International Conference on Information and Knowledge Management (CIKM 2011) , Oct. 2011.  BibTex    D. Sontag, A. Globerson, T. Jaakkola. Introduction to Dual Decomposition for Inference . Optimization for Machine Learning , editors S. Sra, S. Nowozin, and S. J. Wright: MIT Press, 2011.  BibTex   D. Sontag, O. Meshi, T. Jaakkola,  A. Globerson. More data means less  inference: A pseudo-max approach to structured learning .  Neural Information Processing Systems (NIPS)  23 , Dec. 2010. Supplementary   BibTex   T. Koo, A. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual Decomposition for Parsing with Non-Projective Head Automata . Empirical Methods in Natural Language Processing (EMNLP) , 2010. Best paper award . BibTex   A. Rush, D. Sontag, M. Collins, and T. Jaakkola.  On Dual Decomposition and       Linear Programming       Relaxations for       Natural Language       Processing . Empirical Methods in Natural Language Processing (EMNLP) , 2010. BibTex   O. Meshi, D. Sontag, T. Jaakkola, A. Globerson. Learning Efficiently with Approximate Inference via Dual Losses . 27th International Conference on Machine Learning (ICML) , July 2010.  BibTex   T. Jaakkola, D. Sontag, A. Globerson,  M. Meila. Learning  Bayesian Network Structure using LP Relaxations . 13th International Conference on Artificial Intelligence  and Statistics (AI-STATS) ,  2010. BibTex   D. Sontag, T. Jaakkola. Tree Block Coordinate Descent for MAP in Graphical Models . 12th International Conference on Artificial Intelligence and Statistics (AI-STATS) , April 2009. BibTex    D. Sontag, A. Globerson, T. Jaakkola. Clusters and Coarse Partitions in LP Relaxations . Neural Information Processing Systems (NIPS) 21 , Dec. 2008. BibTex    D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, T. Jaakkola. Tightening LP Relaxations for MAP using Message Passing . Uncertainty in Artificial Intelligence (UAI) 24 , July 2008. Best paper award . BibTex    D. Sontag, T. Jaakkola. New Outer Bounds on the Marginal Polytope . Neural Information Processing Systems (NIPS) 20 , Dec. 2007. Outstanding student paper award . Addendum BibTex   B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic Models with Unknown Objects . In Lise Getoor and Ben Taskar, eds. Statistical Relational Learning. Cambridge, MA: MIT Press , 2007.  B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic Models with Unknown Objects . Proc. 19th International Joint Conference on Artificial Intelligence (IJCAI): 1352-1359 , 2005. BibTex   B. Milch, B. Marthi, D. Sontag, S. Russell, D. L. Ong, and A. Kolobov. Approximate Inference for Infinite Contingent Bayesian Networks . 10th International Workshop on Artificial Intelligence and Statistics , 2005. BibTex    Computer networking:   D. Sontag, Y. Zhang, A. Phanishayee, D. Andersen, D. Karger. Scaling All-Pairs Overlay Routing .  Fifth ACM International Conference on emerging  Networking EXperiments and Technologies (CoNEXT) , Dec. 2009. Code BibTex     Computational biology:   D. Sontag, R. Singh, B. Berger. Probabilistic Modeling of Systematic Errors in Two-Hybrid Experiments .  Pacific Symposium on Biocomputing (PSB), 2007 . Supplementary information BibTex"}]},
{"detail": [{"content": "From illusions to inference: NYU course on human perception                         From illusions to inference:  Adventures in human perception                                               Why does one monster appear larger? (It is not.)   Roger N. Shepard's Terror Subterra                Why do we see A as darker than B?  (They are the same shade of gray.)   Edward Adelson's Checkerboard Shadow Illusion                    What is this guy saying? Then replay it with your eyes closed.   McGurk and MacDonald, 1976            3D sidewalk painting by Julian Beever   http://www.julianbeever.net             How can you fool a wine expert? (Quite easily.)  Why do healthy people hear spoken words in noise?  Why do we eat more when we see there is more food left?     Our sensory perception easily falls prey to illusions and biases. It is tempting to think of these as failures of our brain, but they might not be! In fact, they reveal the difficult challenges that our brain faces when interpreting the world, and the clever (and sometimes not so clever) solutions that it comes up with.    In this course, we will use a wide variety of well-known and lesser-known illusions (visual, auditory, tactile, vestibular, and multisensory) to work towards the central concept of inference : the notion that the brain constantly forms hypotheses about the outside world and tries to figure out which of them is most probable.    We will draw parallels with fun examples from online shopping to medical diagnosis to spam filtering to election forecasting to searching for submarines. We will have guest lectures by outside experts: one about neurological disorders of perception, about illusions in aviation, and about 3D illusory street art.            Topics will include...         Wine tastings in a French castle   Jay Johnson's Broadway show   How an illusion can crash a plane   Finding the U.S.S. Scorpion   The tale of the bottomless soup bowl   3D sidewalk drawings   Ames' Room : what you see is not what you get   Is Led Zeppelin satanic ?   How Nate Silver got famous How Netflix knows what you like McGurk effect   The train illusion   Borromini gallery   Misheard lyrics   James Turrell's \"Iltar\"   Cutaneous rabbit illusion   Visual variant of Alzheimer's   How Gmail filters your spam   The Alice in Wonderland syndrome   How we see how clothes feel   How babies learn syllables   Dalmatian dogs   The first scientist and his vision of vision   ...           Spring 2015 semester      PSYCH-UA 300.007    Non-majors welcome.  No prerequisites.      Time: Mondays 4-6:15 PM Location: Meyer 815       Instructor:  Wei Ji Ma, Ph.D.    Associate Professor in Neural Science and Psychology   Lab website Email: weijima@nyu.edu     Grading (tentative):  33.3% class participation  33.3% homework  33.3% final    Class participation will in part be measured using Turning Technologies, which will be provided for free for the duration of the class .    Homework will be based on the previous week's lecture (and lecture notes) and on assigned reading. Syllabus in PDF format"}, {"content": "David Sontag             David Sontag's Home Page    E-mail: dsontag {@ | at} cs.nyu.edu   Phone: 212-998-3498 (office) Clinical machine learning group website    I am an Assistant Professor of  Computer Science and Data Science at NYU. Computer Science is part of the Courant Institute of Mathematical Sciences . My research focuses on machine learning and probabilistic inference, with a particular focus on applications to clinical medicine. For example, we are developing algorithms to learn probabilistic models for medical diagnosis directly from unstructured clinical data, automatically discovering and predicting latent (hidden) variables. We collaborate with the   Emergency Medicine Informatics Research Lab at Beth Israel Deaconess Medical Center and with Independence Blue Cross .     Previously, I was a post-doc  at Microsoft  Research New England . My Ph.D. is in Computer Science from MIT , where I worked with Tommi Jaakkola on approximate inference and learning in probabilistic models. My bachelors degree is from UC  Berkeley , in Computer  Science , where I worked with Stuart Russell 's First-Order Probabilistic Logic group.        Teaching  Fall 2014: Inference and Representation (DS-GA-1005 and CSCI-GA.2569)  Spring 2014: Machine Learning and Computational Statistics (DS-GA-1003 and CSCI-GA.2567)  Fall 2013: Introduction to Machine Learning (CSCI-UA.0480-002)  Spring 2013: Probabilistic Graphical Models (CSCI-GA.3033-006)        Publications      Theses:   D. Sontag. Approximate     Inference in Graphical Models using LP     Relaxations . Ph.D. thesis , Massachusetts Institute of Technology, 2010.   George M. Sprowls Award for the best doctoral theses in Computer      Science at MIT (2010) . BibTex   D. Sontag. Cutting Plane Algorithms for Variational Inference in Graphical Models . Master's thesis , Massachusetts Institute of Technology, 2007. BibTex    Machine learning:    New:    Download Python code for learning topic models (corresponds to ICML '13 paper). See also David Mimno's Mallet-compatible Java implementation .    New:    Download code for learning Bayesian network structure (corresponds to UAI '13 SparsityBoost paper).   Download C++ code for MAP inference in graphical models (corresponds to  UAI '12 paper; see readme file).   Download C++ code implementing our UAI '08 paper (see readme file).     Y. Jernite, S. Rush, D. Sontag. A Fast Variational Approach for Learning Markov Random Field Language Models. To appear in the 32nd International Conference on Machine Learning (ICML) , July 2015.  A. Globerson, T. Roughgarden, D. Sontag, C. Yildirim. How Hard is Inference for Structured Prediction? To appear in the 32nd International Conference on Machine Learning (ICML) , July 2015. [ arXiv ]  Y. Halpern, Y.D. Choi, S. Horng, D. Sontag. Using Anchors to Estimate Clinical State without Labeled Data . American Medical Informatics Association (AMIA) Annual Symposium , Nov. 2014. [ Slides ] BibTex   N. Silberman, D. Sontag, R. Fergus. Instance Segmentation of Indoor Scenes using a Coverage Loss . To appear in the European Conference on Computer Vision (ECCV) , Sept. 2014. BibTex   X. Wang, D. Sontag, F. Wang. Unsupervised Learning of Disease Progression Models . ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) , Aug. 2014. [ Slides ] BibTex   H. Bui, T. Huynh, D. Sontag. Lifted Tree-Reweighted Variational Inference . Uncertainty in Artificial Intelligence (UAI) , July 2014. Addendum BibTex   A. Weller, K. Tang, D. Sontag, T. Jebara. Understanding the Bethe approximation: when and how can it go wrong? Uncertainty in Artificial Intelligence (UAI) , July 2014. BibTex   Y. Jernite, Y. Halpern, D. Sontag. Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests . Neural Information Processing Systems (NIPS) 26 , Dec. 2013. Supplementary [ Code ] BibTex   Y. Jernite, Y. Halpern, S. Horng, D. Sontag. Predicting Chief Complaints at Triage Time in the Emergency Department . NIPS 2013 Workshop on Machine Learning for Clinical Data Analysis and Healthcare , Dec. 2013. BibTex   E. Brenner, D. Sontag. SparsityBoost: A New Scoring Function for Learning Bayesian Network Structure . Uncertainty in Artificial Intelligence (UAI) 29 , July 2013. BibTex   Y. Halpern, D. Sontag. Unsupervised Learning of Noisy-Or Bayesian Networks . Uncertainty in Artificial Intelligence (UAI) 29 , July 2013. BibTex   S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, M. Zhu. A Practical Algorithm for Topic Modeling with Provable Guarantees . 30th International Conference on Machine Learning (ICML) , 2013. Supplementary BibTex   D. Sontag, D. K. Choe, Y. Li. Efficiently Searching for Frustrated Cycles in MAP Inference . Uncertainty in Artificial Intelligence (UAI) 28 , Aug. 2012. Supplementary BibTex   Y. Halpern, S. Horng, L. A. Nathanson, N. I. Shapiro, D. Sontag. A Comparison of Dimensionality Reduction Techniques for Unstructured Clinical Text . ICML 2012 Workshop on Clinical Data Analysis , July 2012. BibTex   D. Sontag, K. Collins-Thompson, P. N. Bennett, R. W. White, S. Dumais, B. Billerbeck. Probabilistic Models for Personalizing Web Search . Fifth ACM International Conference on Web Search and Data Mining (WSDM) , Feb. 2012. [Slides]   BibTex    D. Sontag, D. Roy. Complexity of Inference in Latent Dirichlet Allocation . Neural Information Processing Systems (NIPS)  24 , Dec. 2011. [Slides]   BibTex    K. Collins-Thompson, P. N. Bennett, R. W. White, S. de la Chica, D. Sontag. Personalizing Web Search Results by Reading Level . Twentieth ACM International Conference on Information and Knowledge Management (CIKM 2011) , Oct. 2011.  BibTex    D. Sontag, A. Globerson, T. Jaakkola. Introduction to Dual Decomposition for Inference . Optimization for Machine Learning , editors S. Sra, S. Nowozin, and S. J. Wright: MIT Press, 2011.  BibTex   D. Sontag, O. Meshi, T. Jaakkola,  A. Globerson. More data means less  inference: A pseudo-max approach to structured learning .  Neural Information Processing Systems (NIPS)  23 , Dec. 2010. Supplementary   BibTex   T. Koo, A. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual Decomposition for Parsing with Non-Projective Head Automata . Empirical Methods in Natural Language Processing (EMNLP) , 2010. Best paper award . BibTex   A. Rush, D. Sontag, M. Collins, and T. Jaakkola.  On Dual Decomposition and       Linear Programming       Relaxations for       Natural Language       Processing . Empirical Methods in Natural Language Processing (EMNLP) , 2010. BibTex   O. Meshi, D. Sontag, T. Jaakkola, A. Globerson. Learning Efficiently with Approximate Inference via Dual Losses . 27th International Conference on Machine Learning (ICML) , July 2010.  BibTex   T. Jaakkola, D. Sontag, A. Globerson,  M. Meila. Learning  Bayesian Network Structure using LP Relaxations . 13th International Conference on Artificial Intelligence  and Statistics (AI-STATS) ,  2010. BibTex   D. Sontag, T. Jaakkola. Tree Block Coordinate Descent for MAP in Graphical Models . 12th International Conference on Artificial Intelligence and Statistics (AI-STATS) , April 2009. BibTex    D. Sontag, A. Globerson, T. Jaakkola. Clusters and Coarse Partitions in LP Relaxations . Neural Information Processing Systems (NIPS) 21 , Dec. 2008. BibTex    D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, T. Jaakkola. Tightening LP Relaxations for MAP using Message Passing . Uncertainty in Artificial Intelligence (UAI) 24 , July 2008. Best paper award . BibTex    D. Sontag, T. Jaakkola. New Outer Bounds on the Marginal Polytope . Neural Information Processing Systems (NIPS) 20 , Dec. 2007. Outstanding student paper award . Addendum BibTex   B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic Models with Unknown Objects . In Lise Getoor and Ben Taskar, eds. Statistical Relational Learning. Cambridge, MA: MIT Press , 2007.  B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic Models with Unknown Objects . Proc. 19th International Joint Conference on Artificial Intelligence (IJCAI): 1352-1359 , 2005. BibTex   B. Milch, B. Marthi, D. Sontag, S. Russell, D. L. Ong, and A. Kolobov. Approximate Inference for Infinite Contingent Bayesian Networks . 10th International Workshop on Artificial Intelligence and Statistics , 2005. BibTex    Computer networking:   D. Sontag, Y. Zhang, A. Phanishayee, D. Andersen, D. Karger. Scaling All-Pairs Overlay Routing .  Fifth ACM International Conference on emerging  Networking EXperiments and Technologies (CoNEXT) , Dec. 2009. Code BibTex     Computational biology:   D. Sontag, R. Singh, B. Berger. Probabilistic Modeling of Systematic Errors in Two-Hybrid Experiments .  Pacific Symposium on Biocomputing (PSB), 2007 . Supplementary information BibTex"}, {"content": "Inference and Representation, Fall 2014        Inference and Representation        DS-GA-1005 and CSCI-GA.2569 Fall   201 4                    Overview                     This course covers graphical models, causal inference, and     advanced topics in statistical machine learning. A     graphical model is a probabilistic model, where the     conditional dependencies between the random variables are     specified via a graph. Graphical models provide a flexible     framework for modeling large collections of variables with     complex interactions, as evidenced by their wide domain of     application, including for example machine learning,     computer vision, speech and computational biology. This     course will provide a comprehensive survey of learning and     inference methods in graphical models.                                  Prerequisite: DS-GA-1003/CSCI-GA.2567                                     ( Machine                                      Learning and Computational Statistics ),                                or permission of instructor. Please note        that seats are limited and exceptions to the        prerequisite are only likely to be granted        to PhD students.                             General information      Lecture : Tuesdays, 7:10-9:00pm, in     Warren Weaver Hall 101      Recitation/Laboratory      (required for all students) : Thursdays,     5:10-6:00pm in Silver                                  Center 401                              Instructor :                                     David Sontag                       dsontag {@ | at} cs.nyu.edu                    Lab instructor :                      Yacine Jernite                             jernite {@ | at} cs.nyu.edu                    Grader :                             Prasoon Goyal                             pg1338 {@ | at} nyu.edu                                   Office hours: Tuesdays,     10:30-11:30am. Location:           Center for Data Science, 726 Broadway, 7th Floor            Grading : problem sets (55%) + midterm     exam (20%) + final exam (20%) + participation (5%). Problem Set policy           Book (required) : Kevin Murphy, Machine                                                                   Learning: a Probabilistic Perspective , MIT Press,     2012. I recommend the latest (4th) printing, as the     earlier editions had many typos. You can     tell which printing you have as follows: check the     inside cover, below the \"Library of Congress\"     information. If it says \"10 9 8 ... 4\" you've got the     (correct) fourth print.                Mailing list: To subscribe to the class list,     follow instructions here .                      Draft Schedule   (Readings refer to the Murphy book)                     Week      Date      Topic      Readings      Assignments             1     Sept 2         Introduction, Bayesian networks     [ Slides ]         Chapter 10 (Bayesian networks)     Sec. 17.3, 17.4, 17.5.1 (HMMs)           How to write     a spelling corrector (optional)           Introduction                                     to Probabilistic Topic Models (optional)                   ps1 due Sept. 15 at     10pm      [ Solutions ]                      2     Sept 9         Undirected graphical models [ Slides ]          Conditional random fields         Sec. 9-9.2.5 (exponential family)     Sec. 19-19.4.4, 19.6 (MRFs, CRFs)           An Introduction     to Conditional Random Fields (section 2)           Original                                     paper introducing CRFs (optional)           Application                       to camera lens blur (optional)           Application                       to predicting wind flow (optional)                                   3         Sept 16          Learning and causal inference     [ Slides ] [ BN                      structure slides ]          Maximum likelihood learning, structure learning, causal    networks     Sec. 26-26.6 (structure learning &amp;    causal inference)     Sec. 2.8.2 (KL-divergence)           Overview                            of causal inference in statistics by Judea Pearl (sec.    3.2; optional)           Paper on     causal inference used for computational advertising     (optional)           Science                            paper on learning gene regulatory networks (optional)           Learning                   causal networks using interventions (optional)         ps2 due    Sept. 26 at 5pm     [ Solutions ]                 4         Sept 23         Exact inference [ Slides ]          Variable elimination, treewidth         Chapter 20              ps3 ( data ) due Oct. 10 at     5pm                 5         Sept 30         Belief propagation     [ Slides ] [ Notes ]         Sec. 22.2 (loopy belief propagation)           Using loopy BP     for stereo vision (optional)                              6         Oct 7         Integer linear programming      (MAP inference) [ Slides ]          Linear programming relaxations, dual decomposition     Sec. 22.6 (MAP inference)           Derivation     relating dual decomposition &amp; LP relaxations           Introduction                                     to Dual Decomposition for Inference (optional)           Integer               Programming for Bayesian Network Structure Learning     (optional)                                  Oct 21          (no class Oct 14, Fall Recess; will hold office    hours on 10/14)          Midterm exam      (in class)                                           7         Oct 28         Variational inference [ Slides ]          Mean-field approximation     Sec. 21.1-21.4           Graphical                                  models, exponential families, and variational inference     (optional)           Stochastic             Variational Inference (optional)              ps4 ( data ) due Nov. 17 at     5pm                      8         Nov 4         Monte-Carlo methods for inference [ Slides 1 ] [ Slides          2 ]          Importance sampling, Metropolis Hastings, Gibbs sampling         Sec. 23.1-23.4     Sec. 17.2.3     Sec. 24.1-24.4                                   9         Nov 11         More variational inference [ Slides ]           TRW, loopy BP         Sec. 22.3-22.4                                   10         Nov 18     Discovering latent variables using the     method-of-moments      [ Slides 1 ]    [ Slides 2 ]            Guest         lecture by Daniel         Hsu          A Method of     Moments for Mixture Models and Hidden Markov Models     (Sec. 1 &amp; 2)           Method        of Moments for Topic Modeling (optional)                         11     Nov 25     Structured prediction     [ Slides ]         Sec. 19.7 (also, review 19.6 again)           Tutorial       on Structured Prediction (optional)           Original                                  paper introducing structured perceptron (optional)           Cutting-Plane                                  Training of Structural SVMs (optional)           Block-Coordinate                                  Frank-Wolfe Optimization for Structural SVMs     (optional)          ps5 ( data ) due Dec. 8 at     5pm                      12     Dec 2         Learning Markov networks      [ Slides 1 ] [ Slides      2 ]          Pseudo-likelihood, deep generative models          Sec. 19.5, 26.7-26.8     Sec. 27.7, 28.2           Notes on     pseudo-likelihood           An Introduction     to Conditional Random Fields (section 4; optional)           Approximate maximum     entropy learning in MRFs (optional)                         13     Dec 9         Advanced topics in learning           Probabilistic programming                  Talk     for recent ICML 2014 paper on deep    generative models (optional)           Probabilistic     programming: overview and bibliography (optional)           Probabilistic     programming: BUGS (optional)           Probabilistic programming:     Church book (optional)                                  Dec 16          Final exam (in class)                                     Acknowledgements : Many thanks to the Toyota Technological  Institute, Hebrew University, UC Berkeley, and Stanford University  for sharing material used in slides and homeworks                          Reference materials           Optional course text: Probabilistic      Graphical Models: Principles and Techniques by     Daphne Koller and Nir Friedman, MIT Press (2009).      Machine learning books            Trevor Hastie, Rob Tibshirani, and Jerry Friedman, Elements of      Statistical Learning , Second Edition, Springer,      2009. (Can be downloaded as PDF file.)       David Barber, Bayesian                                                                   Reasoning and Machine Learning , Cambridge      University Press, 2012. (Can be downloaded as PDF      file.)           Probability            Chapter 2 of Murphy             Review                                                                    notes from Stanford's machine learning class       Sam Roweis's probability                                                                    review           Optimization            Convex                                                                    Optimization by Stephen Boyd and Lieven      Vandenberghe. (Can be downloaded as PDF file.)                                  Problem                                      Set policy            I expect you to try solving each problem set on your own.    However, when being stuck on a problem, I encourage     you to collaborate with other students in the class, subject    to the following rules:               You may discuss a problem with any student in this     class, and work together on solving it. This can involve     brainstorming and verbally discussing the problem, going     together through possible solutions, but should not      involve one student telling another a complete solution.                Once you solve the homework, you must write up      your solutions on your own , without looking at     other people's write-ups or giving your write-up to     others.                In your solution for each problem, you must write      down the names of any person with whom you     discussed it. This will not affect your grade.                Do not consult solution manuals or other people's     solutions from similar courses.           Late submission policy : During the    semester you are allowed at most two extensions on the    homework assignment. Each extension is for at most 48 hours    and carries a penalty of 25% off your assignment."}]}]