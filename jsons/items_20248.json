[{"detail": [{"content": "Job Queues on Mercer - High Performance Computing at NYU - NYU Wikis                                                                                                                                                                                                                                                                                                                                                                      Skip to content    Skip to breadcrumbs    Skip to header menu    Skip to action menu    Skip to quick search                                                                    Spaces                                      Create                          Quick Search                     Help                                                            Online Help                             Keyboard Shortcuts                            Feed Builder                            What\u2019s new                             Available Gadgets                            About Confluence                                                                                    Log in                                                      High Performance Computing at NYU    Pages  Blog    Child pages   Running jobs on the NYU HPC clusters   Job Queues on Mercer     Browse pages  Configure Space tools                                                                     Tools                                                     A t tachments (0)                                           Page History                                           Restrictions                                                               Page Information                                           Link to this Page\u2026                                           View in Hierarchy                                           View Source                                           Export to PDF                                           Export to Word                                                                                                    Pages                               High Performance Computing at NYU                               Running jobs on the NYU HPC clusters                              Skip to end of banner                 JIRA links         Go to start of banner                          Job Queues on Mercer                                                                                                                                                                 Skip to end of metadata                                         Created by  Stephen Leak , last modified on Mar 02, 2015                  Go to start of metadata                                          Icon                In almost all cases, do not specify a queue - the system will work out where to best place your job according to the resources requested              Queue name Job limit per user Resource limits per job Resource limit defaults Purpose  route \u00a0 \u00a0 \u00a0 Routing queue: jobs submitted without specifying a queue are processed here and routed to one of the other queues according to the resources requested. s48 1000 168 hrs walltime, 1 node 1 hr walltime, 2GB memory, 1 core Single-node jobs (serial or multithreaded). If your job will take longer than 48 hours, we recommend using checkpointing to guard against job failure or a node problem partway through. Read this page or contact us . \u00a0  p12 100 168 hrs walltime, 2+ nodes 1 hour walltime, 1 core per node Multi-node jobs This queue will not accept interactive jobs - you can use up to 2 nodes interactively for up to 4 hours via the interactive queue.  interactive 2 4 hrs walltime, 2 full nodes 1 hr walltime, 1 core, 2GB memory High-priority interactive use , especially debugging. Interactive jobs go here by default. Interactive jobs which do not meet the resource limits for this queue will go to the s48 queue, and consequently may take longer to start.  cgsb-s 1000 168 hours walltime, 48 GB memory, 12 cores on a single node 12 hrs walltime, 1 core Long-running CGSB jobs . (see HPC Stakeholders ) Jobs needing more than 96 hours and submitted by CGSB users will be routed to this queue and scheduled on the CGSB-owned nodes  sysadm 0 \u00a0 \u00a0 Maintenance reservations by system administrators: normal users do not have access to this queue  \u00a0 \u00a0                                                       No labels                                                                                                                                           Overview      Content Tools      Activity                                                     Powered by Atlassian Confluence 5.6.6 , Team Collaboration Software      Printed by Atlassian Confluence 5.6.6, Team Collaboration Software.      Report a bug      Atlassian News              Atlassian"}]},
{"detail": [{"content": "Job Queues on Mercer - High Performance Computing at NYU - NYU Wikis                                                                                                                                                                                                                                                                                                                                                                      Skip to content    Skip to breadcrumbs    Skip to header menu    Skip to action menu    Skip to quick search                                                                    Spaces                                      Create                          Quick Search                     Help                                                            Online Help                             Keyboard Shortcuts                            Feed Builder                            What\u2019s new                             Available Gadgets                            About Confluence                                                                                    Log in                                                      High Performance Computing at NYU    Pages  Blog    Child pages   Running jobs on the NYU HPC clusters   Job Queues on Mercer     Browse pages  Configure Space tools                                                                     Tools                                                     A t tachments (0)                                           Page History                                           Restrictions                                                               Page Information                                           Link to this Page\u2026                                           View in Hierarchy                                           View Source                                           Export to PDF                                           Export to Word                                                                                                    Pages                               High Performance Computing at NYU                               Running jobs on the NYU HPC clusters                              Skip to end of banner                 JIRA links         Go to start of banner                          Job Queues on Mercer                                                                                                                                                                 Skip to end of metadata                                         Created by  Stephen Leak , last modified on Mar 02, 2015                  Go to start of metadata                                          Icon                In almost all cases, do not specify a queue - the system will work out where to best place your job according to the resources requested              Queue name Job limit per user Resource limits per job Resource limit defaults Purpose  route \u00a0 \u00a0 \u00a0 Routing queue: jobs submitted without specifying a queue are processed here and routed to one of the other queues according to the resources requested. s48 1000 168 hrs walltime, 1 node 1 hr walltime, 2GB memory, 1 core Single-node jobs (serial or multithreaded). If your job will take longer than 48 hours, we recommend using checkpointing to guard against job failure or a node problem partway through. Read this page or contact us . \u00a0  p12 100 168 hrs walltime, 2+ nodes 1 hour walltime, 1 core per node Multi-node jobs This queue will not accept interactive jobs - you can use up to 2 nodes interactively for up to 4 hours via the interactive queue.  interactive 2 4 hrs walltime, 2 full nodes 1 hr walltime, 1 core, 2GB memory High-priority interactive use , especially debugging. Interactive jobs go here by default. Interactive jobs which do not meet the resource limits for this queue will go to the s48 queue, and consequently may take longer to start.  cgsb-s 1000 168 hours walltime, 48 GB memory, 12 cores on a single node 12 hrs walltime, 1 core Long-running CGSB jobs . (see HPC Stakeholders ) Jobs needing more than 96 hours and submitted by CGSB users will be routed to this queue and scheduled on the CGSB-owned nodes  sysadm 0 \u00a0 \u00a0 Maintenance reservations by system administrators: normal users do not have access to this queue  \u00a0 \u00a0                                                       No labels                                                                                                                                           Overview      Content Tools      Activity                                                     Powered by Atlassian Confluence 5.6.6 , Team Collaboration Software      Printed by Atlassian Confluence 5.6.6, Team Collaboration Software.      Report a bug      Atlassian News              Atlassian"}, {"content": "Queues - High Performance Computing at NYU - NYU Wikis                                                                                                                                                                                                                                                                                                                                                                      Skip to content    Skip to breadcrumbs    Skip to header menu    Skip to action menu    Skip to quick search                                                                    Spaces                                      Create                          Quick Search                     Help                                                            Online Help                             Keyboard Shortcuts                            Feed Builder                            What\u2019s new                             Available Gadgets                            About Confluence                                                                                    Log in                                                      High Performance Computing at NYU    Pages  Blog    Child pages   Using the Clusters   Queues     Browse pages  Configure Space tools                                                                     Tools                                                     A t tachments (0)                                           Page History                                           Restrictions                                                               Page Information                                           Link to this Page\u2026                                           View in Hierarchy                                           View Source                                           Export to PDF                                           Export to Word                                                                                                    Pages                               High Performance Computing at NYU                               Using the Clusters                              Skip to end of banner                 JIRA links         Go to start of banner                          Queues                                                                                                                                                                 Skip to end of metadata                                         Created by  Maureen A Butler , last modified by  Yanli Zhang on Dec 06, 2012                  Go to start of metadata                                            Icon                Click here for the updated HPC Wiki             Home About NYU HPC Accounts The Clusters Using the Clusters  Access Managing Data Available Software Compiling &amp; Debugging Running Jobs Queues Getting Started &amp; FAQs  Reference Research Gallery \u00a0           Queues High Performance Computing Centers use scheduling systems to run and monitor jobs submitted via various batch systems. NYU HPC uses MOAB/Torque for job scheduling. The queues are distributed in a way that favors shorter jobs. A simple MOAB fair share baseline is implemented to even out usage between users. Should you need more resources than the fair share allocations because of critical deadlines such as a grant application, a publication deadline, or class use, please email hpc@nyu.edu to make special arrangements. To see queues offered by a particular cluster, use the Torque/PBS command:   $ qstat -q   To request a specific queue, use the Torque/PBS operative below in your pbs script:   #PBS -q &lt;queue name&gt;   Same can be done from command line:   $ qsub -q &lt;queue name&gt;   This section outlines the queues associated with each of the clusters. #USQ Queues #Bowery Queues #Cardiac Queues  USQ Queues  Type Name of Queue Maximum Walltime Max Jobs \u00a0 Per User Max CPU Core/User * Maximum \u00a0Nodes Node Allocation \u00a0\u00a0\u00a0\u00a0 Type * * Active Priority  Serial ser2 48 hours \u00a0\u00a0 N/A 64,128 \u00a0 Shared Yes \u00a0  Serial serlong 96 hours \u00a0\u00a0 N/A 32,64 \u00a0 Shared Yes \u00a0  Interactive interactive 4 hours \u00a0\u00a0\u00a0 2 N/A 2 Shared Yes highest  Notes: *\u00a0 Max CPU Core/User defines the largest processor count available to any one user. The first number represents a soft limit and the second number a hard limit. These flexible dual limits are set to ensure efficient utilization of cluster resources. \u00a0 ** Exclusive nodes versus shared nodes. Due to the complexity of message passing used by parallel jobs, all NYU HPC parallel queues are setup for &quot;exclusive&quot; node use, which means only one job can run on a node at the same time. Serial jobs using serial queues on the other hand can share the same node, up to matching the CPU/core count. \u00a0 \u00a0 USQ is for running serial jobs. Please use Bowery for parallel jobs. Serial Queues ser2: Generic serial queue for jobs up to 48 hours.\u00a0 serlong: Generic serial queue for up to 96 hours.\u00a0 Interactive Queue      interactive: This queue allows code testing and debugging in the compute node environment. Using the interactive queue for code testing alleviates extra stress on the login nodes. Compute node-based debugging is a highly recommended practice. The priority of the interactive queue is set to be the highest for easy access. Bowery Queues  Type Name of Queue Maximum Walltime Default Walltime Max Jobs \u00a0 Per User Max CPU \u00a0 Core/User Maximum \u00a0Nodes Node Allocation Type Active  Parallel p12 12 hours 1 hour 6 288,576 N/A Exclusive Yes  Parallel p48 48 hours 1 hour N/A 64,128 N/A Exclusive Yes  Serial s48 48 hours 1 hour 500 36,72 1 Shared Yes  Interactive interactive 4 hours 1 hour 2 32 2 Shared Yes  Bigmem bigmem 48 hours 1 hour N/A 96,192 N/A Shared Yes  GPU cuda 48 hours 1 hour N/A N/A 2 Exclusive Yes   Parallel Queues p12 and p48:\u00a0 Soft and hard limits are set to control CPU core totals per user (288,576 and 64,128, respectively). The scheduler will allocate jobs based on cluster traffic to maximize usage and will allow override of soft limits for individual users when resources are available.\u00a0 All the p48 jobs can only be submitted to the 64 nodes from chassis 0 to 3 (compute-0-0 to compute-3-15), while p12 jobs can make use of all the compute nodes (compute-0-0 to compute-9-15). 64 out of 96 nodes from chassis 3 to 9 are owned by a private group but these nodes are available to the NYU public community when not being used by the group. The group's jobs have the highest priority on such nodes. Please specify &quot;ppn=8&quot; in your PBS script if you submit your jobs to the p48 queue. For the p12 queue, it is recommended to specify &quot;ppn=12&quot; for node balance purposes. Serial Queues The serial queue s48 on Bowery has 12 nodes on chassis 12 and all 32 nodes on chassis 13 for serial jobs. Each node has 12 cores with 48GB memory. If you need memory more than 48GB, please use bigmem queue. Interactive Queue      interactive: This queue allows code testing and debugging in the compute node environment. Using the interactive queue for code testing alleviates extra stress on the login nodes. Compute node-based debugging is a highly recommended practice. The priority of the interactive queue is set to be the highest for easy access. Bigmem Queue The bigmem queue has 16 nodes, each with 12 CPU cores and 96 GB memory, and one node with 16 CPU cores in total with 256 GB memory.\u00a0 It has been created for jobs with heavy memory usage requiring more than 24 GB of memory.\u00a0 If your memory requirement is 24 GB or less, please use other queues.\u00a0 Bigmem is one node with maximum ppn of 16. GPU Queue The cuda queue has 4 GPU nodes, each with 12 CPU cores, Nvidia Tesla M2090 GPU card and 24GB memory. It has been created for jobs that require GPU cores for computation (programs written in CUDA programming language). Please check this page for more information on using this queue to run cuda jobs on GPU nodes. Cardiac Queues Cardiac is a shared resource between a private owner and HPC/ITS (&quot;Public&quot;). MOAB FS, or &quot;fair share&quot; policy, is used to allocate resources between the private and public researchers to ensure priorities and to maximize utilization. The fair share policy aims to balance the 75/25% ownership ratio to flexibly increase the usage targets when private nodes are idle.  Type Name of Queue Maximum Walltime Max CPU\u00a0 \u00a0 Core/User Max Queued \u00a0 Per User Maximum \u00a0Nodes Node Allocation \u00a0\u00a0\u00a0\u00a0\u00a0 Type Active Ownership User Share  Parallel p12 12 hours \u00a0\u00a0 192,384 \u00a0 \u00a0 Exclusive Yes Public 25% +  Parallel p48 48 hours \u00a0\u00a0 96,192 \u00a0 \u00a0 Exclusive Yes Public 25% +  Serial ser2 48 hours \u00a0\u00a0 72,144 \u00a0 \u00a0 Shared Yes Public 25% +  Serial serlong 96 hours \u00a0\u00a0 32/64 \u00a0 \u00a0 Shared Yes Public 25% +  Interactive interactive 4 hours \u00a0\u00a0 2 \u00a0 2 Shared Yes Public N/A  Parallel card no limit no limit no limit \u00a0 Exclusive Yes Private 75%  Serial s-card no limit no limit no limit \u00a0 Shared Yes Private 75%   Public Queues Public queues on Cardiac are set to reflect general NYU HPC cluster usage policies. p12: Max walltime = 12 hours p48: Max walltime = 48 hours ser2: Max walltime = 48 hours serlong: Max walltime = 96 hours Interactive Queue      interactive: This queue allows code testing and debugging in the compute node environment. Using the interactive queue for code testing alleviates extra stress on the login nodes. Compute node-based debugging is a highly recommended practice. The priority of the interactive queue is set to be the highest for easy access.  Private Queues card: Parallel queue s-card: Serial queue                                                                             \u00a0 \u00a0    PBS Script Generator An interactive tool that generates PBS script based on user's input. Check\u00a0 this page \u00a0for more details.     Front-Line HPC Consulting HPC consultations are available once a week, Monday 1-3 PM. Appointments are required. Please make an appointment at hpc@nyu.edu .   \u00a0 \u00a0 \u00a0                                                        No labels                                                                                                                                           Overview      Content Tools      Activity                                                     Powered by Atlassian Confluence 5.6.6 , Team Collaboration Software      Printed by Atlassian Confluence 5.6.6, Team Collaboration Software.      Report a bug      Atlassian News              Atlassian"}]},
{"detail": [{"content": "Job Queues on Mercer - High Performance Computing at NYU - NYU Wikis                                                                                                                                                                                                                                                                                                                                                                      Skip to content    Skip to breadcrumbs    Skip to header menu    Skip to action menu    Skip to quick search                                                                    Spaces                                      Create                          Quick Search                     Help                                                            Online Help                             Keyboard Shortcuts                            Feed Builder                            What\u2019s new                             Available Gadgets                            About Confluence                                                                                    Log in                                                      High Performance Computing at NYU    Pages  Blog    Child pages   Running jobs on the NYU HPC clusters   Job Queues on Mercer     Browse pages  Configure Space tools                                                                     Tools                                                     A t tachments (0)                                           Page History                                           Restrictions                                                               Page Information                                           Link to this Page\u2026                                           View in Hierarchy                                           View Source                                           Export to PDF                                           Export to Word                                                                                                    Pages                               High Performance Computing at NYU                               Running jobs on the NYU HPC clusters                              Skip to end of banner                 JIRA links         Go to start of banner                          Job Queues on Mercer                                                                                                                                                                 Skip to end of metadata                                         Created by  Stephen Leak , last modified on Mar 02, 2015                  Go to start of metadata                                          Icon                In almost all cases, do not specify a queue - the system will work out where to best place your job according to the resources requested              Queue name Job limit per user Resource limits per job Resource limit defaults Purpose  route \u00a0 \u00a0 \u00a0 Routing queue: jobs submitted without specifying a queue are processed here and routed to one of the other queues according to the resources requested. s48 1000 168 hrs walltime, 1 node 1 hr walltime, 2GB memory, 1 core Single-node jobs (serial or multithreaded). If your job will take longer than 48 hours, we recommend using checkpointing to guard against job failure or a node problem partway through. Read this page or contact us . \u00a0  p12 100 168 hrs walltime, 2+ nodes 1 hour walltime, 1 core per node Multi-node jobs This queue will not accept interactive jobs - you can use up to 2 nodes interactively for up to 4 hours via the interactive queue.  interactive 2 4 hrs walltime, 2 full nodes 1 hr walltime, 1 core, 2GB memory High-priority interactive use , especially debugging. Interactive jobs go here by default. Interactive jobs which do not meet the resource limits for this queue will go to the s48 queue, and consequently may take longer to start.  cgsb-s 1000 168 hours walltime, 48 GB memory, 12 cores on a single node 12 hrs walltime, 1 core Long-running CGSB jobs . (see HPC Stakeholders ) Jobs needing more than 96 hours and submitted by CGSB users will be routed to this queue and scheduled on the CGSB-owned nodes  sysadm 0 \u00a0 \u00a0 Maintenance reservations by system administrators: normal users do not have access to this queue  \u00a0 \u00a0                                                       No labels                                                                                                                                           Overview      Content Tools      Activity                                                     Powered by Atlassian Confluence 5.6.6 , Team Collaboration Software      Printed by Atlassian Confluence 5.6.6, Team Collaboration Software.      Report a bug      Atlassian News              Atlassian"}, {"content": "Queues - High Performance Computing at NYU - NYU Wikis                                                                                                                                                                                                                                                                                                                                                                      Skip to content    Skip to breadcrumbs    Skip to header menu    Skip to action menu    Skip to quick search                                                                    Spaces                                      Create                          Quick Search                     Help                                                            Online Help                             Keyboard Shortcuts                            Feed Builder                            What\u2019s new                             Available Gadgets                            About Confluence                                                                                    Log in                                                      High Performance Computing at NYU    Pages  Blog    Child pages   Using the Clusters   Queues     Browse pages  Configure Space tools                                                                     Tools                                                     A t tachments (0)                                           Page History                                           Restrictions                                                               Page Information                                           Link to this Page\u2026                                           View in Hierarchy                                           View Source                                           Export to PDF                                           Export to Word                                                                                                    Pages                               High Performance Computing at NYU                               Using the Clusters                              Skip to end of banner                 JIRA links         Go to start of banner                          Queues                                                                                                                                                                 Skip to end of metadata                                         Created by  Maureen A Butler , last modified by  Yanli Zhang on Dec 06, 2012                  Go to start of metadata                                            Icon                Click here for the updated HPC Wiki             Home About NYU HPC Accounts The Clusters Using the Clusters  Access Managing Data Available Software Compiling &amp; Debugging Running Jobs Queues Getting Started &amp; FAQs  Reference Research Gallery \u00a0           Queues High Performance Computing Centers use scheduling systems to run and monitor jobs submitted via various batch systems. NYU HPC uses MOAB/Torque for job scheduling. The queues are distributed in a way that favors shorter jobs. A simple MOAB fair share baseline is implemented to even out usage between users. Should you need more resources than the fair share allocations because of critical deadlines such as a grant application, a publication deadline, or class use, please email hpc@nyu.edu to make special arrangements. To see queues offered by a particular cluster, use the Torque/PBS command:   $ qstat -q   To request a specific queue, use the Torque/PBS operative below in your pbs script:   #PBS -q &lt;queue name&gt;   Same can be done from command line:   $ qsub -q &lt;queue name&gt;   This section outlines the queues associated with each of the clusters. #USQ Queues #Bowery Queues #Cardiac Queues  USQ Queues  Type Name of Queue Maximum Walltime Max Jobs \u00a0 Per User Max CPU Core/User * Maximum \u00a0Nodes Node Allocation \u00a0\u00a0\u00a0\u00a0 Type * * Active Priority  Serial ser2 48 hours \u00a0\u00a0 N/A 64,128 \u00a0 Shared Yes \u00a0  Serial serlong 96 hours \u00a0\u00a0 N/A 32,64 \u00a0 Shared Yes \u00a0  Interactive interactive 4 hours \u00a0\u00a0\u00a0 2 N/A 2 Shared Yes highest  Notes: *\u00a0 Max CPU Core/User defines the largest processor count available to any one user. The first number represents a soft limit and the second number a hard limit. These flexible dual limits are set to ensure efficient utilization of cluster resources. \u00a0 ** Exclusive nodes versus shared nodes. Due to the complexity of message passing used by parallel jobs, all NYU HPC parallel queues are setup for &quot;exclusive&quot; node use, which means only one job can run on a node at the same time. Serial jobs using serial queues on the other hand can share the same node, up to matching the CPU/core count. \u00a0 \u00a0 USQ is for running serial jobs. Please use Bowery for parallel jobs. Serial Queues ser2: Generic serial queue for jobs up to 48 hours.\u00a0 serlong: Generic serial queue for up to 96 hours.\u00a0 Interactive Queue      interactive: This queue allows code testing and debugging in the compute node environment. Using the interactive queue for code testing alleviates extra stress on the login nodes. Compute node-based debugging is a highly recommended practice. The priority of the interactive queue is set to be the highest for easy access. Bowery Queues  Type Name of Queue Maximum Walltime Default Walltime Max Jobs \u00a0 Per User Max CPU \u00a0 Core/User Maximum \u00a0Nodes Node Allocation Type Active  Parallel p12 12 hours 1 hour 6 288,576 N/A Exclusive Yes  Parallel p48 48 hours 1 hour N/A 64,128 N/A Exclusive Yes  Serial s48 48 hours 1 hour 500 36,72 1 Shared Yes  Interactive interactive 4 hours 1 hour 2 32 2 Shared Yes  Bigmem bigmem 48 hours 1 hour N/A 96,192 N/A Shared Yes  GPU cuda 48 hours 1 hour N/A N/A 2 Exclusive Yes   Parallel Queues p12 and p48:\u00a0 Soft and hard limits are set to control CPU core totals per user (288,576 and 64,128, respectively). The scheduler will allocate jobs based on cluster traffic to maximize usage and will allow override of soft limits for individual users when resources are available.\u00a0 All the p48 jobs can only be submitted to the 64 nodes from chassis 0 to 3 (compute-0-0 to compute-3-15), while p12 jobs can make use of all the compute nodes (compute-0-0 to compute-9-15). 64 out of 96 nodes from chassis 3 to 9 are owned by a private group but these nodes are available to the NYU public community when not being used by the group. The group's jobs have the highest priority on such nodes. Please specify &quot;ppn=8&quot; in your PBS script if you submit your jobs to the p48 queue. For the p12 queue, it is recommended to specify &quot;ppn=12&quot; for node balance purposes. Serial Queues The serial queue s48 on Bowery has 12 nodes on chassis 12 and all 32 nodes on chassis 13 for serial jobs. Each node has 12 cores with 48GB memory. If you need memory more than 48GB, please use bigmem queue. Interactive Queue      interactive: This queue allows code testing and debugging in the compute node environment. Using the interactive queue for code testing alleviates extra stress on the login nodes. Compute node-based debugging is a highly recommended practice. The priority of the interactive queue is set to be the highest for easy access. Bigmem Queue The bigmem queue has 16 nodes, each with 12 CPU cores and 96 GB memory, and one node with 16 CPU cores in total with 256 GB memory.\u00a0 It has been created for jobs with heavy memory usage requiring more than 24 GB of memory.\u00a0 If your memory requirement is 24 GB or less, please use other queues.\u00a0 Bigmem is one node with maximum ppn of 16. GPU Queue The cuda queue has 4 GPU nodes, each with 12 CPU cores, Nvidia Tesla M2090 GPU card and 24GB memory. It has been created for jobs that require GPU cores for computation (programs written in CUDA programming language). Please check this page for more information on using this queue to run cuda jobs on GPU nodes. Cardiac Queues Cardiac is a shared resource between a private owner and HPC/ITS (&quot;Public&quot;). MOAB FS, or &quot;fair share&quot; policy, is used to allocate resources between the private and public researchers to ensure priorities and to maximize utilization. The fair share policy aims to balance the 75/25% ownership ratio to flexibly increase the usage targets when private nodes are idle.  Type Name of Queue Maximum Walltime Max CPU\u00a0 \u00a0 Core/User Max Queued \u00a0 Per User Maximum \u00a0Nodes Node Allocation \u00a0\u00a0\u00a0\u00a0\u00a0 Type Active Ownership User Share  Parallel p12 12 hours \u00a0\u00a0 192,384 \u00a0 \u00a0 Exclusive Yes Public 25% +  Parallel p48 48 hours \u00a0\u00a0 96,192 \u00a0 \u00a0 Exclusive Yes Public 25% +  Serial ser2 48 hours \u00a0\u00a0 72,144 \u00a0 \u00a0 Shared Yes Public 25% +  Serial serlong 96 hours \u00a0\u00a0 32/64 \u00a0 \u00a0 Shared Yes Public 25% +  Interactive interactive 4 hours \u00a0\u00a0 2 \u00a0 2 Shared Yes Public N/A  Parallel card no limit no limit no limit \u00a0 Exclusive Yes Private 75%  Serial s-card no limit no limit no limit \u00a0 Shared Yes Private 75%   Public Queues Public queues on Cardiac are set to reflect general NYU HPC cluster usage policies. p12: Max walltime = 12 hours p48: Max walltime = 48 hours ser2: Max walltime = 48 hours serlong: Max walltime = 96 hours Interactive Queue      interactive: This queue allows code testing and debugging in the compute node environment. Using the interactive queue for code testing alleviates extra stress on the login nodes. Compute node-based debugging is a highly recommended practice. The priority of the interactive queue is set to be the highest for easy access.  Private Queues card: Parallel queue s-card: Serial queue                                                                             \u00a0 \u00a0    PBS Script Generator An interactive tool that generates PBS script based on user's input. Check\u00a0 this page \u00a0for more details.     Front-Line HPC Consulting HPC consultations are available once a week, Monday 1-3 PM. Appointments are required. Please make an appointment at hpc@nyu.edu .   \u00a0 \u00a0 \u00a0                                                        No labels                                                                                                                                           Overview      Content Tools      Activity                                                     Powered by Atlassian Confluence 5.6.6 , Team Collaboration Software      Printed by Atlassian Confluence 5.6.6, Team Collaboration Software.      Report a bug      Atlassian News              Atlassian"}, {"content": "Running jobs - Queues - High Performance Computing at NYU - NYU Wikis                                                                                                                                                                                                                                                                                                                                                                            Skip to content    Skip to breadcrumbs    Skip to header menu    Skip to action menu    Skip to quick search                                                                    Spaces                                      Create                          Quick Search                     Help                                                            Online Help                             Keyboard Shortcuts                            Feed Builder                            What\u2019s new                             Available Gadgets                            About Confluence                                                                                    Log in                                                      High Performance Computing at NYU    Pages  Blog    Child pages   Running jobs on the NYU HPC clusters   Running jobs - Queues     Browse pages  Configure Space tools                                                                     Tools                                                     A t tachments (0)                                           Page History                                           Restrictions                                                               Page Information                                           Link to this Page\u2026                                           View in Hierarchy                                           View Source                                           Export to PDF                                           Export to Word                                                                                                    Pages                               High Performance Computing at NYU                               Running jobs on the NYU HPC clusters                              Skip to end of banner                 JIRA links         Go to start of banner                          Running jobs - Queues                                                                                                                                                                 Skip to end of metadata                                         Created by  Stephen Leak , last modified on Jul 17, 2014                  Go to start of metadata                                 Quick Links    Mercer downtime April 20-22  HPC at NYU  HPC Newsletter   Getting started on Mercer   Logging in   Windows   Mac / Linux   Clusters and Storage   Transferring data to/from the clusters   Writing and submitting jobs with qsub   Available software   Building Software   Tutorials  Research Gallery  FAQs   NYU HPC Cheat Sheet   Wiki Map  (Tip: click &quot;&lt;&lt;&quot; at bottom left to close Confluence sidebar)     Running jobs on the NYU HPC clusters Job Scheduling Overview Login and compute nodes  Queues  Writing a job script  Job stdout and stderr Which filesystems should I use?  Submitting a Job  Basic qsub options and directives Setting resource limits Running MPI jobs Running GPU jobs Working interactively Job dependencies and delaying starting Running many similar jobs Setting job priorities Where in the queue is my job, and why?  Monitoring jobs with qstat  qstat in more detail \u00a0  See what is running where with pbstop  When will my job start? Why hasn't my job started? \u00a0  Canceling a job    Job Queues Not all jobs can be run at once - the cluster is finite! - so when jobs are submitted they are placed into a queue.\u00a0When a &quot;space&quot; becomes available in the schedule Moab looks down the queue for the first job that will fit into the space.\u00a0 Jobs are not necessarily placed at the end of the queue - Moab uses the priority (discussed here ) to determine where in the queue a job should be placed.           Icon                At NYU HPC shorter jobs are given higher priority             There is more than one queue. Each queue is configured for different types of jobs and has resource limits and priorities set accordingly. If you do not specify a queue to submit to, Torque will use the resources requested to select a queue for you. Frequently this is the best option , however in some circumstances you are better off explicitly specifying a queue. \u00a0You can see the list of queues with the command &quot; qstat -q &quot;, and you can see more detail about a specific queue with &quot; qstat -Qf\u00a0 queue-name &quot;.\u00a0 The following example shows the queues available on Mercer, with some more detail about each queue in the table below. The output shows: The name of each queue The maximum memory, CPU time, Wallclock time and number of nodes that a job in each queue can use The number of currently queued and currently running jobs in each queue The queue job limits and state (these columns are of interest mostly to the system administrators)\u00a0    \u00a0           Icon                In almost all cases, do not specify a queue - the system will work out where to best place your job according to the resources requested              Queue name Job limit per user Resource limits per job Resource limit defaults Purpose  route \u00a0 \u00a0 \u00a0 Routing queue: jobs submitted without specifying a queue are processed here and routed to one of the other queues according to the resources requested. s48 1000 168 hrs walltime, 1 node 1 hr walltime, 2GB memory, 1 core Single-node jobs (serial or multithreaded). If your job will take longer than 48 hours, we recommend using checkpointing to guard against job failure or a node problem partway through. Read this page or contact us . \u00a0  p12 100 168 hrs walltime, 2+ nodes 1 hour walltime, 1 core per node Multi-node jobs This queue will not accept interactive jobs - you can use up to 2 nodes interactively for up to 4 hours via the interactive queue.  interactive 2 4 hrs walltime, 2 full nodes 1 hr walltime, 1 core, 2GB memory High-priority interactive use , especially debugging. Interactive jobs go here by default. Interactive jobs which do not meet the resource limits for this queue will go to the s48 queue, and consequently may take longer to start.  cgsb-s 1000 168 hours walltime, 48 GB memory, 12 cores on a single node 12 hrs walltime, 1 core Long-running CGSB jobs . (see HPC Stakeholders ) Jobs needing more than 96 hours and submitted by CGSB users will be routed to this queue and scheduled on the CGSB-owned nodes  sysadm 0 \u00a0 \u00a0 Maintenance reservations by system administrators: normal users do not have access to this queue  \u00a0 \u00a0                                                        No labels                                                                                                                                           Overview      Content Tools      Activity                                                     Powered by Atlassian Confluence 5.6.6 , Team Collaboration Software      Printed by Atlassian Confluence 5.6.6, Team Collaboration Software.      Report a bug      Atlassian News              Atlassian"}]}]