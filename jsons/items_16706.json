[{"detail": [{"content": "computation + cognition lab @ nyu: research     computation + cognition lab @ nyu                                                                                 research       overview       teaching       nyuconcats       resources/code       github       lab blog       psiturk *new*          publications       by year       by category          people       - everyone -          location      map                                                                                The goal of our research is to better understand the memory, learning, and decision processes          which allow humans to carry out intelligent and adaptive behaviors.          The name of our lab is \"computation and cognition\" which reflect the two main strands that         inform our work. Principally, we attempt to understand the nature of human cognition by         developing computational theories of human information processing. By comparing the operation         formal models and the behavior of humans, we can gain insight into the mechanisms that people         use to to solve problems and adapt their behavior. In addition, insights from these           models can inform the development of artificial systems capable of learning on their own.                                      We are particularly interested in how people learn from experience. Our research ranges         from studies of the different neural systems that contribute to         category and concept acquisition, to the interactions between categorization, memory, and         perceptual processes, to (more recently) studies of how people learn by interacting with          the world through exploration and information-search behaviors.                                      To give you a better sense of some of the basic scientific questions we are interested          in, we've highlighted below some of the main lines of work we are currently developing.          Our lab paper archive has a full list of papers         and abstracts.                                                            Our first, and in some sense primary, line of work deals with category and concept learning or how people learn          to generalize from the instances they see in the world. For example, how do young children learn which plants          belong to the category \"tree\" and which are \"flowers\"? Category learning is a critical cognitive ability which          underlies a vast array of abilities such as object recognition and language acquisition. Our research in this area          seeks a better understanding of the learning processes which support this ability and how they might be organized          or implemented in the brain. Our work in this area combines a variety of methodologies including behavioral          studies, neuro-imaging, and computational modeling.                               An example finding from our work:                               In one recent study, we looked at the influence that categories have on our perceptual abilities (Gureckis & Goldstone, 2008, in prep).         In this study we asked people to discriminate between a set of highly similar human faces. Then we taught them to assign         different subsets of the faces into categories. After learning, we again tested participant's discrimination ability.          The key finding is that, after learning, participants were less able to discriminate items that were assigned to the same         category and were better able to discrimination faces from different categories (a effect known as categorical perception).         More interestingly, we also found that the distribution of exemplars within each category also had an influence on people's         perception. Items that belonged to the same \"cluster\" of items inside a category were harder to discriminate after         learning than items that belong two different clusters within the same category. Thus, distinctions that were not         directly relevant for success in the learning task ended up influencing people's perception. We are currently following up         these studies to better understand the interplay of language, learning, and perception.                                       Representative Publications:                    Markant, D. and Gureckis, T.M. (in review) \"The impact of self-directed learning in a perceptual category learning task\"                              Gureckis, T.M. and Goldstone, R.L.. (2008) \"The effect of the internal          structure of categories on perception\" in Proceedings of the 30th Annual          Conference of Cognitive Science Society                                  Love, B.C., Medin, D.L., and Gureckis, T.M. (2004) SUSTAIN: A Network Model of Category Learning. Psychological Review, 11, 309-332.                             Gureckis, T.M. and Love, B.C. (2003). Towards a Unified Account of          Supervised and Unsupervised Learning. Journal of Experimental and          Theoretical Artifical Intelligence, 15, 1-24.                                                                 A second line of work considers how learning can be viewed as an active (or interactive) process.         Imagine a young child just beginning to learn about how the world works. Some of the time, an adult is          nearby to provide feedback or instruction, but often times the child must learn on their own by trial          and error. In some of our recent work, we've looked at how people learn effective behavioral strategies          through interacting with dynamic task environments where reward structure continually changes in response          to the actions of the individual. In contrast to approaches that treat learners as passive   observers, our goal in this work is to          develop theories of learning that can account for the active way in which we search, sample, and          explore our environment. On the computational side, this work applies the framework of          reinforcement learning (Sutton & Barto, 1998) to understand higher-level cognitive behavior and decision making.            Reinforcement learning is an agent-based approach to learning through interaction with the environment in          pursuit of goal-directed behavior. Reinforcement          learning is also a promising theoretical approach to studying higher-level human learning since it emphasizes the role of a         situated agent interacting with their environment. For example, RL models specify how learners should balance exploration          versus exploitation of resources in their environment, how they take into account delayed rewards while learning, and how          to assign credit to actions that later lead to successful outcomes.                    (barney photo notsogoodphotography )                                        Representative Publications:                    Markant, D. and Gureckis, T.M. (2010) \"Category Learning Through Active Sampling.\" in Proceedings of      the 32nd Annual Conference of the Cognitive Science Society                               Gureckis, T.M. and Markant, D. (2009) \"Modeling Information Search in a Spatial Concept Learning Game.\" in Proceedings of  the 31st Annual Conference of the Cognitive Science Society                               Gureckis, T.M. and Love, B.C. (2009) Learning in Noise: Dynamic          Decision-Making in an Uncertain Environment. Journal of Mathematical          Psychology                             Gureckis, T.M. and Love, B.C. (2009) Short Term Gains, Long Term          Pains: Reinforcement Learning in Dynamic Environments. Cognition                                                                                In recent years, there has been a growing interest in the neurobiological basis of our category learning          ability. In other words, how are categories represented in the brain, and how do different brain systems          influence what we learn? However, these efforts have tended to focus on the localization of particular          functions to particular brain regions in the absence of useful linking theories which detail how these          regions interact. One project we have been apart of over the past few years has been to consider the role          that models developed in cognitive science can play in helping to integrate these often disparate findings.          In general, we are very interested in how findings from cognitive neuroscience can constrain and improve theories of                behavior developed at the computational/behavioral level.                             Representative Publications:                             Gureckis, T.M., James, T.H., and Nosofsky, R.M. (2011).          Reevaluting the dissociation between implicit and explicit category learning: A fMRI Study. Journal of Cognitive Neuroscience.                             Love, B.C. and Gureckis, T.M. (2007). Models in Search of the Brain.          Cognitive and Affective Behavioral Neuroscience                                 Gureckis, T.M. and Love, B.C. (2006). Bridging Levels: Using a Cognitive Model to Connect Brain and Behavior in Category               Learning Proceedings of the 28th Annual Conference of Cognitive Science Society.                                 Gureckis, T.M. and Love, B.C. (2004). Common Mechanisms in Infant and  Adult Category Learning. Infancy, vol 5, no.2,                173-198.                                                                 From the ability to move our hands         when we are typing to our ability to sequence actions of our mouths to make words,          a key aspect of human cognition is our ability to learn about event arranged in time.          However, what are the representational primitives that support such behavior? In this line of         work, we have examined how the statistical structure of the environment interacts with the          representations of the world we adopt to determine which things are easier or harder for people          to learn.                                                 Representative Publications:                    Gureckis, T.M. and Love, B.C. (2009) Learning in Noise: Dynamic          Decision-Making in an Uncertain Environment. Journal of Mathematical          Psychology                             Gureckis, T.M. and Love, B.C. (2009) Short Term Gains, Long Term          Pains: Reinforcement Learning in Dynamic Environments. Cognition                             Gureckis, T.M. and Love, B.C. (2007) \"Behaviorism Reborn? Statistical          Learning as Simple Conditioning\" in Proceedings of the 29th Annual          Meeting of the Cognitive Science Society.                                 Gureckis, T.M. and Love, B.C. (2005). A Critical Look at the Mechanisms          Underlying Implicit Sequence Learning. Proceedings of the 27th Annual          Conference of Cognitive Science Society.                                                                            Our work is supported by grants from                 Copyright 2008-2013 - Todd Gureckis - New York University"}]},
{"detail": [{"content": "computation + cognition lab @ nyu: research     computation + cognition lab @ nyu                                                                                 research       overview       teaching       nyuconcats       resources/code       github       lab blog       psiturk *new*          publications       by year       by category          people       - everyone -          location      map                                                                                The goal of our research is to better understand the memory, learning, and decision processes          which allow humans to carry out intelligent and adaptive behaviors.          The name of our lab is \"computation and cognition\" which reflect the two main strands that         inform our work. Principally, we attempt to understand the nature of human cognition by         developing computational theories of human information processing. By comparing the operation         formal models and the behavior of humans, we can gain insight into the mechanisms that people         use to to solve problems and adapt their behavior. In addition, insights from these           models can inform the development of artificial systems capable of learning on their own.                                      We are particularly interested in how people learn from experience. Our research ranges         from studies of the different neural systems that contribute to         category and concept acquisition, to the interactions between categorization, memory, and         perceptual processes, to (more recently) studies of how people learn by interacting with          the world through exploration and information-search behaviors.                                      To give you a better sense of some of the basic scientific questions we are interested          in, we've highlighted below some of the main lines of work we are currently developing.          Our lab paper archive has a full list of papers         and abstracts.                                                            Our first, and in some sense primary, line of work deals with category and concept learning or how people learn          to generalize from the instances they see in the world. For example, how do young children learn which plants          belong to the category \"tree\" and which are \"flowers\"? Category learning is a critical cognitive ability which          underlies a vast array of abilities such as object recognition and language acquisition. Our research in this area          seeks a better understanding of the learning processes which support this ability and how they might be organized          or implemented in the brain. Our work in this area combines a variety of methodologies including behavioral          studies, neuro-imaging, and computational modeling.                               An example finding from our work:                               In one recent study, we looked at the influence that categories have on our perceptual abilities (Gureckis & Goldstone, 2008, in prep).         In this study we asked people to discriminate between a set of highly similar human faces. Then we taught them to assign         different subsets of the faces into categories. After learning, we again tested participant's discrimination ability.          The key finding is that, after learning, participants were less able to discriminate items that were assigned to the same         category and were better able to discrimination faces from different categories (a effect known as categorical perception).         More interestingly, we also found that the distribution of exemplars within each category also had an influence on people's         perception. Items that belonged to the same \"cluster\" of items inside a category were harder to discriminate after         learning than items that belong two different clusters within the same category. Thus, distinctions that were not         directly relevant for success in the learning task ended up influencing people's perception. We are currently following up         these studies to better understand the interplay of language, learning, and perception.                                       Representative Publications:                    Markant, D. and Gureckis, T.M. (in review) \"The impact of self-directed learning in a perceptual category learning task\"                              Gureckis, T.M. and Goldstone, R.L.. (2008) \"The effect of the internal          structure of categories on perception\" in Proceedings of the 30th Annual          Conference of Cognitive Science Society                                  Love, B.C., Medin, D.L., and Gureckis, T.M. (2004) SUSTAIN: A Network Model of Category Learning. Psychological Review, 11, 309-332.                             Gureckis, T.M. and Love, B.C. (2003). Towards a Unified Account of          Supervised and Unsupervised Learning. Journal of Experimental and          Theoretical Artifical Intelligence, 15, 1-24.                                                                 A second line of work considers how learning can be viewed as an active (or interactive) process.         Imagine a young child just beginning to learn about how the world works. Some of the time, an adult is          nearby to provide feedback or instruction, but often times the child must learn on their own by trial          and error. In some of our recent work, we've looked at how people learn effective behavioral strategies          through interacting with dynamic task environments where reward structure continually changes in response          to the actions of the individual. In contrast to approaches that treat learners as passive   observers, our goal in this work is to          develop theories of learning that can account for the active way in which we search, sample, and          explore our environment. On the computational side, this work applies the framework of          reinforcement learning (Sutton & Barto, 1998) to understand higher-level cognitive behavior and decision making.            Reinforcement learning is an agent-based approach to learning through interaction with the environment in          pursuit of goal-directed behavior. Reinforcement          learning is also a promising theoretical approach to studying higher-level human learning since it emphasizes the role of a         situated agent interacting with their environment. For example, RL models specify how learners should balance exploration          versus exploitation of resources in their environment, how they take into account delayed rewards while learning, and how          to assign credit to actions that later lead to successful outcomes.                    (barney photo notsogoodphotography )                                        Representative Publications:                    Markant, D. and Gureckis, T.M. (2010) \"Category Learning Through Active Sampling.\" in Proceedings of      the 32nd Annual Conference of the Cognitive Science Society                               Gureckis, T.M. and Markant, D. (2009) \"Modeling Information Search in a Spatial Concept Learning Game.\" in Proceedings of  the 31st Annual Conference of the Cognitive Science Society                               Gureckis, T.M. and Love, B.C. (2009) Learning in Noise: Dynamic          Decision-Making in an Uncertain Environment. Journal of Mathematical          Psychology                             Gureckis, T.M. and Love, B.C. (2009) Short Term Gains, Long Term          Pains: Reinforcement Learning in Dynamic Environments. Cognition                                                                                In recent years, there has been a growing interest in the neurobiological basis of our category learning          ability. In other words, how are categories represented in the brain, and how do different brain systems          influence what we learn? However, these efforts have tended to focus on the localization of particular          functions to particular brain regions in the absence of useful linking theories which detail how these          regions interact. One project we have been apart of over the past few years has been to consider the role          that models developed in cognitive science can play in helping to integrate these often disparate findings.          In general, we are very interested in how findings from cognitive neuroscience can constrain and improve theories of                behavior developed at the computational/behavioral level.                             Representative Publications:                             Gureckis, T.M., James, T.H., and Nosofsky, R.M. (2011).          Reevaluting the dissociation between implicit and explicit category learning: A fMRI Study. Journal of Cognitive Neuroscience.                             Love, B.C. and Gureckis, T.M. (2007). Models in Search of the Brain.          Cognitive and Affective Behavioral Neuroscience                                 Gureckis, T.M. and Love, B.C. (2006). Bridging Levels: Using a Cognitive Model to Connect Brain and Behavior in Category               Learning Proceedings of the 28th Annual Conference of Cognitive Science Society.                                 Gureckis, T.M. and Love, B.C. (2004). Common Mechanisms in Infant and  Adult Category Learning. Infancy, vol 5, no.2,                173-198.                                                                 From the ability to move our hands         when we are typing to our ability to sequence actions of our mouths to make words,          a key aspect of human cognition is our ability to learn about event arranged in time.          However, what are the representational primitives that support such behavior? In this line of         work, we have examined how the statistical structure of the environment interacts with the          representations of the world we adopt to determine which things are easier or harder for people          to learn.                                                 Representative Publications:                    Gureckis, T.M. and Love, B.C. (2009) Learning in Noise: Dynamic          Decision-Making in an Uncertain Environment. Journal of Mathematical          Psychology                             Gureckis, T.M. and Love, B.C. (2009) Short Term Gains, Long Term          Pains: Reinforcement Learning in Dynamic Environments. Cognition                             Gureckis, T.M. and Love, B.C. (2007) \"Behaviorism Reborn? Statistical          Learning as Simple Conditioning\" in Proceedings of the 29th Annual          Meeting of the Cognitive Science Society.                                 Gureckis, T.M. and Love, B.C. (2005). A Critical Look at the Mechanisms          Underlying Implicit Sequence Learning. Proceedings of the 27th Annual          Conference of Cognitive Science Society.                                                                            Our work is supported by grants from                 Copyright 2008-2013 - Todd Gureckis - New York University"}, {"content": "The M.S. Programs                                                                                  Courant Institute      New York University      FAS      CAS      GSAS                           :: CONTACT US                                                       go                                  Home     People     Administration     Research     Seminars     Courses     Ph.D. Programs     M.S. Programs     M.S. in Mathematics     M.S. in Scientific Computing     M.S. in Mathematics in Finance     M.S. in Data Science     Undergraduate Program     Visiting Member Program     Weekly Bulletin     Useful Links     Job Openings     Outreach                  The Master of Science in Scientific Computing           Notice to Fall 2015 MS applicants: The department has     started to review the applications. Please note that this     process can take several weeks. We cannot guarantee a decision     date; you will be contacted by the Graduate School when an     admission decision has been made on your application. If you     have any questions and concerns regarding your application,     please contact the Math Department at admissions@math.nyu.edu .                The Program      The departments of mathematics and computer science at NYU's     Courant Institute of Mathematical Sciences offer a master's     degree in scientific computing. The program provides broad yet     rigorous training in areas of mathematics and computer science     related to scientific computing. It aims to prepare people     with the right talents and background for a technical career     doing practical computing.      The program accommodates both full-time and part-time     students, with most courses meeting in the evening. The     program is self-contained and terminal, providing a complete     set of skills in a field where the need is greater than the     supply. The masters program focuses on computational science,     which includes modeling and numerical simulation as used in     engineering design, development, and optimization.           During the academic years of 2012 and 2013 a concentration in     data sciences existed within the scientific computing program;     this concentration has been discontinued as of 2014. Incoming     students interested in data sciences should consider the     recently-created Masters              of Science in Data Science within the NYU Center for Data     Science .           Starting Fall of 2014 the modified the program requirements     and guidelines listed below will apply to all incoming     students. The new list of required/approved courses includes     the previous list but gives additional flexibility for     students to tailor the list of courses to their background and     interests. Students presently enrolled in the Modeling and     Simulation track can choose to complete the program either     under the new or the old requirements. Students enrolled in     the Data Science concentration should consult the expanded     course options and modified requirements below since this     increases flexibility while maintaining consistency with the     previous requirements. These students should contact Professor     Esteban Tabak for help     in deciding on classes to take.                Scientific Computing:     Overview           Scientific computing is an indispensable part of almost all     scientific investigation and technological development at     universities, government laboratories, and within the private     sector. Typically a scientific computing team consists of     several people trained in some branch of mathematics, science,     statistics, or engineering. What is often lacking is expertise     in modern computing tools such as visualization, modern     programming paradigms, and high performance computing. The     master's program in scientific computing aims to satisfy these     needs, without omitting basic training in numerical analysis     and computer science. Many graduates of this program work at     technologically advanced institutions, especially in research     and development, where their skills and experience complement     those without interdisciplinary degrees. The program is also     open to students who will go on to pursue doctoral studies in     computer science, mathematics, or statistics.      The master's program in scientific computing focuses on the     mathematics and computer science related to advanced computer     modeling and simulation. The program is similar in structure     to terminal master's programs in engineering, combining     classroom training with practical experience. The coursework     ranges from foundational mathematics and fundamental     algorithms to such practical topics as data visualization and     software tools. Electives encourage the exploration of     specific application areas such as mathematical and     statistical finance, applications of machine learning, fluid     mechanics, finite element methods, and biomedical modeling.     The program culminates in a master's project, which serves to     integrate the classroom material.           Admission Requirements      The program requires least three semesters of Calculus     (including multivariate calculus), as well as linear algebra.     Experience with programming in a high-level language (e.g.,     Java, C, C++, Fortran. Python) as well as data structures,     equivalent to a first-year sequence in computer science,      is also required. It is highly desirable that applicants have     undergraduate major or significant experience in mathematics,     a quantitative science or engineering, or economics.      The deadline for application to the program is April 1st for     the fall semester. The program admits students both on a     full-time and on a part-time basis. The application process     takes place online via the Graduate School of Arts and     Sciences; please visit the Graduate     School Admissions site .      For more information, please contact us at      Office of Admissions and Student Affairs      Department of Mathematics      Courant Institute of Mathematical Sciences      251 Mercer Street      New York, NY 10012-1185                Tel. (212) 998-3238      Fax (212) 995-4121                e-Mail: admissions@math.nyu.edu           e-Mail: arnon@cims.nyu.edu           web page: http://www.math.nyu.edu                Degree Requirements      A candidate for a master's degree in scientific computing     must accrue the following:           33 points of course credit (11 courses)     comprised of            4 core courses (12 points) in mathematics       4 core courses (12 points) in computer science       3 elective courses (9 points)           3 points of credit from a master's capstone     project.          Students with exceptional backgrounds may petition the program    director for permission to substitute other appropriate courses    for core courses. Advanced students may be permitted to do a    masters thesis as an alternative to the master's capstone    project.           Core Courses      The following are the two required core courses      in mathematics:                 MATH-GA              2010 Numerical Methods I (fall semester)       MATH-GA              2020 Numerical Methods II (spring semester)          Students must also complete at least two of the following core    courses in mathematics:           MATH-GA              2701 Methods of Applied Mathematics (fall     semester)      MATH-GA-2490 Partial Differential Equations I (fall)       MATH-GA              2702 Fluid Dynamics (fall semester)      MATH-GA-2962 Mathematical Statistics (if offered)           MATH-GA-2704 Applied Stochastic Analysis (spring semester)      DS-GA-1002 Statistical and Mathematical Methods          Advanced students may also choose the two additional core    courses from the following advanced topics courses in applied    math and numerical analysis, typically offered bi-anually:           MATH-GA Advanced Topics: Optimization           MATH-GA Advanced Topics: Optimization and Data Analysis      MATH-GA Advanced Topics: Monte Carlo      MATH-GA Advanced Topics: Computational Fluid Dynamics      MATH-GA Advanced Topics: Finite Element Methods                The following are the two required core courses      in computer science:            CSCI-GA              1170 Fundamental Algorithms (fall, spring and     summer terms)       CSCI-GA              2110 Programming Languages (fall, spring, and     summer terms)          Students must also complete at least two of the following core    courses in computer science:            CSCI-GA              3033 Open Source Tools (fall term)       CSCI-GA              2270 Computer Graphics (spring term)       CSCI-GA              2565 Machine Learning (fall term)      CSCI-GA.2566 Foundations of Machine Learning      DS-GA-1001 Introduction to Data Science (fall)           DS-GA-1003 Machine Learning and Computational Statistics     (spring)      DS-GA-1004: Big Data (spring)          Advanced students may also choose the two additional core    courses from advanced topics courses in computer science of    relevance to Scientific Computing, such as:           CSCI-GA Graphics Processing Units (GPUs)      CSCI-GA Advanced Topics: High-Performance Computing          The departments of mathematics and computer science publish    annual brochures describing all courses offered each year.    Students should consult these lists of course offerings to    determine the availability of desired courses.     Concentration in Data     Science     This section is meant only for students presently enrolled in    the Data Science concentration; this concentration is no longer    offered. To graduate, students enrolled in the old concentration    are required to take the following core courses     in mathematics for the concentration in data science:                One of           MATH-GA              2962 Mathematical Statistics (spring semester),      DS-GA-1001 Introduction to Data Science (fall), or           DS-GA-1002 Statistical and Mathematical Methods                and either            MATH-GA              2043 Scientific Computing (fall and spring     semesters)                or both            MATH-GA              2010 Numerical Methods I (fall semester) and            MATH-GA              2020 Numerical Methods II (spring semester)           The following are the three required core courses      in computer science for the concentration in data science:            CSCI-GA              1170 Fundamental Algorithms (fall, spring and     summer terms)       CSCI-GA              3033 Open Source Tools (fall term)          and one of           CSCI-GA              2565 Machine Learning (fall term), or      DS-GA-1003 Machine Learning and Computational Statistics     (spring)          Students in the concentration in data science must complete 33    points points of course credit (11 courses), including core and    elective courses. They must also obtain 3 points from a master's    capstone project.          The Capstone Project      The master's program culminates in a capstone project. The     capstone project course is usually taken during the final year     of study. During the project, students go through the entire     process of solving a real-world problem, from collecting and     processing data to designing and fully implementing a     solution. The problems and data sets come from settings     identical to those encountered in industry, academia, or     government.           The following is a list of courses approved to meet the     capstone requirement:             MATH-GA.2011/ CSCI-GA.2945      Advanced topics: Data Science      CSCI-GA Advanced Topics: High-Performance Computing      DS-GA-1006 Capstone Project in Data Science      CSCI-GA Advanced Computer Graphics      CSCI-GA Multicore Processors: Architecture     &amp;Programming      CSCI-GA Software Engineering           Advanced students can obtain permission from the director of     the program to do an individual capstone project under the     supervision of a faculty member.                Computing Facilities      The Courant Institute makes available for graduate training     and coursework a network of workstations maintained by systems     administrators. All graduate students have computer accounts     for the duration of their studies. NYU also runs a     high-performance computing center with both shared-memory and     distributed-memory computers.           Faculty      Many members of the departments of mathematics and computer     science have research interests bearing on scientific     computing. The list includes           Marsha J.      Berger . B.S. 1974, Binghamton; M.S. 1978, Ph.D.     1982, Stanford. Research interests: computational fluid     dynamics, adaptive mesh refinement, parallel computing.      Yu Chen .     B.S. 1982, Tsinghua; M.S. 1988, Ph.D. 1991, Yale. Research     Interests: numerical scattering theory, ill-posed problems,     scientific computing.           Aleksandar Donev .     B.S. 2001, Michigan State; Ph.D. 2006, Princeton. Research     interests: multi-scale methods, fluctuating hydrodynamics,     coarse-grained particle methods, jamming and packing.           Davi Geiger .     B.S. 1980, Pontifica (Brazil); Ph.D. 1990, MIT. Research     interests: computer vision, information theory, medical     imaging, and neuroscience.      Jonathan              B. Goodman . B.S. 1977, MIT; Ph.D. 1982, Stanford.     Research interests: numerical analysis, fluid dynamics,     computational physics, partial differential equations.      Leslie      Greengard . B.A. 1979, Wesleyan; M.S. 1987, Yale     School of Medicine; Ph.D. 1987, Yale. Research interests:     scientific computing, fast algorithms, potential theory.      Yann LeCun . B.S.     1983, ESIEE (Paris); D.E.A. 1984, Ph.D. 1987, Pierre and Marie     Curie University (Paris). Research interests: machine     learning.      Andrew      Majda . B.S. 1970, M.S. 1971, Ph.D. 1973, Stanford.     Research interests: modern applied mathematics,     atmosphere/ocean science, turbulence, statistical physics.      Bhubaneswar              Mishra . B.S. 1980, India Institute of Technology,     Kharagpur; M.S. 1982, Ph.D. 1985, Carnegie-Mellon. Research     interests: robotics, mathematical and theoretical computer     science.      Michael      L. Overton . B.S. 1974, British Columbia; M.S.     1977, Ph.D. 1979, Stanford. Research interests: numerical     linear algebra, optimization, linear and semidefinite     programming.      Kenneth Perlin .     B.A. 1979, Harvard; M.S. 1984, Ph.D. 1986, NYU. Research     interests: computer graphics, simulation, computer-human     interfaces, multimedia.      Charles              S. Peskin . B.A. 1968, Harvard; Ph.D. 1972,     Yeshiva. Research interests: physiology, fluid dynamics,     numerical methods.      Aaditya V.      Rangan . B.A. 1999, Dartmouth; Ph.D. 2003,     Berkeley. Research interests: large-scale scientific modeling     of physical, biological, and neurobiological phenomena.      Tamar      Schlick . B.S. 1982, Wayne State; M.S. 1984, Ph.D.     1987, NYU. Research interests: mathematical biology, numerical     analysis, computational chemistry.      Michael              J. Shelley . B.S. 1981, Colorado; M.S. 1984, Ph.D.     1985, Arizona. Research interests: scientific computation,     fluid dynamics, neuroscience.      Eero Simoncelli .     B.A. 1984, Harvard; M.S. 1988, Ph.D. 1993, MIT. Research     interests: image processing, computational neuroscience,     computer vision.      Esteban              Tabak . Bach. 1988, Buenos Aires; Ph.D. 1992, MIT.     Research interests: fluid dynamics, conservation laws,     optimization and data analysis.      Olof B.      Widlund . C.E. 1960, Tekn. L. 1964, Technology     Institute, Stockholm; Ph.D. 1966, Uppsala. Research interests:     numerical analysis, partial differential equations, parallel     computing.      Margaret H. Wright .     B.S. 1964, M.S. 1965, Ph.D. 1976, Stanford. Research     interests: mathematical optimization, numerical methods,     nonlinear programming.      Denis Zorin .    B.S. 1991, Moscow Institute of Physics and Technology; M.S.    1993, Ohio State; Ph.D. 1997, Caltech. Research interests:    computer graphics, geometric modeling, subdivision surfaces,    multiresolution surface representations, perceptually based    methods for computer graphics.            Miranda Holmes-Cerfon ,    B.S. 2005 University of British Columbia, PhD 2010 NYU. Research    interests: soft-matter physics, fluid dynamics, oceanography,    stochastic methods.           Antoine Cerfon ,    B.S. 2003, M.S. 2005 Ecole des Mines de Paris, PhD 2010 MIT.    Research interests: Computational plasma physics, multi-scale    methods, fast algorithms.           Dimitris GIannakis ,    MSci 2001 Cambridge, PhD 2009 Chicago. Research interests:    geometrical data analysis, statistical modeling, climate    dynamics.                Academic Standards      To register for courses, students must maintain good academic     standing, fulfilling the following requirements:           Students must maintain an average of B or better over     their first twelve credits. Students who fail to achieve     this cannot continue in the master's program.      Students cannot obtain a master's degree unless they have     maintained an overall average of B or better. Students at     risk of failing to meet this requirement receive a warning     letter from the department.      Students cannot obtain more than four no-credit grades,     withdrawals, or unresolved incomplete grades during their     academic tenure, and no more than two such grades in the     first six courses for which they have registered.          Up to two core courses taken elsewhere can earn transfer credit,    subject to the normal NYU graduate school restrictions on    transfer of credit and the approval of the program director. At    least 30 credits must be taken at NYU. For further    administrative information please contact             Tamar    Arnon            arnon@cims.nyu.edu        Tel.    212 998-3257          For further academic information please contact                 Aleksandar Donev, Director of the Master's Program in Scientific    Computing            donev@courant.nyu.edu                          Revised summer 2013                                   NEW YORK UNIVERSITY"}]}]