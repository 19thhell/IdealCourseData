[{"detail": [{"content": "Layered Compositing of Facial Expression         Layered Compositing of Facial Expression       Ken Perlin   Media Research Laboratory   Department of Computer Science   New York University       How does one make an embodied agent react with appropriate facial expression, without resorting to repetitive prebuilt animations? How does one mix and transition between facial expressions to visually represent shifting moods and attitudes? How can authors of these agents relate lower level facial movements to higher level moods and intentions? We introduce a computational engine which addresses these questions with a stratified approach. We first define a low level movement model having a discrete number of degrees of freedom. Animators can combine and layer these degrees of freedom to create elements of autonomous facial motion. Animators can then recursively build on this movement model to construct higher level models.   In this way, animators can synthesize successively higher levels of autonomous facial expressiveness. A key feature of our computational approach is that composited movements tend to blend and layer in natural ways in the run time system. As the animator builds at higher levels, the correct layering priorities are always maintained at lower supporting levels. We have used this approach to create emotionally expressive autonomous facial agents.                     angry, daydreaming, disgusted, distrustful                   fiendish, haughty, head back, scolding, sad                 smiling, sneezing, surprised, suspicious     Building on our work in Improvisational Animation [Siggraph 96], we use a parallel layered approach. Our authoring system allows its user to relate lower level facial movements to higher level moods and intentions by using a model inspired by optical compositing. We first allow the animator to abstract facial motion into a discrete number of degrees of freedom. The system does not impose a particular model at this stage, but rather allows the animator to define the degrees of freedom that he or she finds useful.   Given a set of degrees of freedom, we allow the animator to specify time varying linear combinations and overlays of these degrees of freedom. Each definition becomes a new, derived, degree of freedom. Collectively, these derived degrees of freedom create a new abstraction layer. We allow animators to recursively create successive abstractions, each built upon the degrees of freedom defined by previous ones.   More specifically, we internally represent each of the model's K degrees of freedom by a K dimensional basis vector, which has a value of 1 in some dimension j, and a value of 0 in all other dimensions. The state space for the face consists of all linear combinations of these basis vectors.  We allow the animator to create time varying functions (coherent noise and splined curves) of these linear combinations, to create a simple vocabulary of \"basis motions\". We then provide a motion compositing engine, which allows the animator to specify a compositing structure for these basis motions. The engine gives motions varying \"opacities\" and does alpha blending, much as Photoshop composites layers of images. The animator can then encapsulate sets of time-varying weights for each of the basis motions in the above engine, and define each such set as a new motion basis. By doing this recursively, the animator can describe successively higher levels of an emotional vocabulary.   We have found that in this way, successively higher semantic levels of facial expressiveness can be effectively synthesized. In particular, we find that movements defined with this method tend to blend together and overlay in natural ways. As the animator works within higher abstractions, the system always maintains correct priorities at all lower supporting abstractions. The result is real-time interactive facial animation that can achieve a convincing degree of emotive expressiveness.   The eventual goal of this work is to give computer/human interfaces the ability to represent the subtleties we take for granted in face to face communication, so that they can function as agents for an emotional point of view. By automating the creation of facial expression in the run-time engine of an interactive agent, we enable such an agent to operate without the explicit intervention of a human operator."}]},
{"detail": [{"content": "Layered Compositing of Facial Expression         Layered Compositing of Facial Expression       Ken Perlin   Media Research Laboratory   Department of Computer Science   New York University       How does one make an embodied agent react with appropriate facial expression, without resorting to repetitive prebuilt animations? How does one mix and transition between facial expressions to visually represent shifting moods and attitudes? How can authors of these agents relate lower level facial movements to higher level moods and intentions? We introduce a computational engine which addresses these questions with a stratified approach. We first define a low level movement model having a discrete number of degrees of freedom. Animators can combine and layer these degrees of freedom to create elements of autonomous facial motion. Animators can then recursively build on this movement model to construct higher level models.   In this way, animators can synthesize successively higher levels of autonomous facial expressiveness. A key feature of our computational approach is that composited movements tend to blend and layer in natural ways in the run time system. As the animator builds at higher levels, the correct layering priorities are always maintained at lower supporting levels. We have used this approach to create emotionally expressive autonomous facial agents.                     angry, daydreaming, disgusted, distrustful                   fiendish, haughty, head back, scolding, sad                 smiling, sneezing, surprised, suspicious     Building on our work in Improvisational Animation [Siggraph 96], we use a parallel layered approach. Our authoring system allows its user to relate lower level facial movements to higher level moods and intentions by using a model inspired by optical compositing. We first allow the animator to abstract facial motion into a discrete number of degrees of freedom. The system does not impose a particular model at this stage, but rather allows the animator to define the degrees of freedom that he or she finds useful.   Given a set of degrees of freedom, we allow the animator to specify time varying linear combinations and overlays of these degrees of freedom. Each definition becomes a new, derived, degree of freedom. Collectively, these derived degrees of freedom create a new abstraction layer. We allow animators to recursively create successive abstractions, each built upon the degrees of freedom defined by previous ones.   More specifically, we internally represent each of the model's K degrees of freedom by a K dimensional basis vector, which has a value of 1 in some dimension j, and a value of 0 in all other dimensions. The state space for the face consists of all linear combinations of these basis vectors.  We allow the animator to create time varying functions (coherent noise and splined curves) of these linear combinations, to create a simple vocabulary of \"basis motions\". We then provide a motion compositing engine, which allows the animator to specify a compositing structure for these basis motions. The engine gives motions varying \"opacities\" and does alpha blending, much as Photoshop composites layers of images. The animator can then encapsulate sets of time-varying weights for each of the basis motions in the above engine, and define each such set as a new motion basis. By doing this recursively, the animator can describe successively higher levels of an emotional vocabulary.   We have found that in this way, successively higher semantic levels of facial expressiveness can be effectively synthesized. In particular, we find that movements defined with this method tend to blend together and overlay in natural ways. As the animator works within higher abstractions, the system always maintains correct priorities at all lower supporting abstractions. The result is real-time interactive facial animation that can achieve a convincing degree of emotive expressiveness.   The eventual goal of this work is to give computer/human interfaces the ability to represent the subtleties we take for granted in face to face communication, so that they can function as agents for an emotional point of view. By automating the creation of facial expression in the run-time engine of an interactive agent, we enable such an agent to operate without the explicit intervention of a human operator."}, {"content": "Bellomo Brendan: Tisch School of the Arts at NYU                           Skip to content        Undergraduate Film &amp; Television Admissions About the Program Course Offerings Summer Courses Showcase Internships Facilities Special Projects First Run &amp; Festivals Calendar of Events News TSOA FTV Wiki Admin &amp; Faculty Alumni Blog Facebook Twitter YouTube  home &gt; Undergraduate Film &amp; TV          Kanbar Institute of Film &amp; Television  Brendan Bellomo Adjunct Professor Email: bb612@nyu.edu Website: www.brendanbellomo.com Office: 721 Broadway, 8th Floor Office Hours: By Appointment  Courses Visual Effects & Compositing Education B.F.A. New York University, Tisch School of the Arts, Film/TV Biography Brendan Bellomo started filmmaking when he discovered his father\u2019s camera at age six. In 2009, he received the Student Academy Award for his sci-fi drama BOHEMIBOT involving a cast and crew of 200 people working in 11 countries. His films have been screened at over 30 film festivals in the US and abroad. He works as a music video director and feature VFX supervisor. He also created Tisch\u2019s first course in VFX and film compositing techniques, built around his 15 years of experience in digital animation, VFX and post-production. Most recently Brendan supervised effects on 2012 Sundance Grand Jury Prize winner BEASTS OF THE SOUTHERN WILD. Brendan is now preparing to adapt Charles Yu\u2019s TIME Magazine Top-10 fiction novel, HOW TO LIVE SAFELY IN A SCIENCE FICTIONAL UNIVERSE, to direct with NYU alum Chris Columbus and his company 1492 Pictures producing.           Undergraduate Admissions Graduate Admissions Academic Services Student Life Financial Aid and Scholarships Career Development Counseling Services International Students Incoming Students Graduation Forms What is Special Programs? Study Abroad Summer High School Summer in New York City Courses for Non-Majors Programs for Visiting Students Open Arts Minors Blog Newsletters Videos Admin &amp; Faculty Alumni Contact Us  Tisch Events Calendar Admin &amp; Faculty Calendar NYU Academic Deadlines Calendar Many Ways to Give Why Support Tisch Dean's Council Corporate Giving Foundation Giving Individual Giving Tisch Gala Who We Are Institute of Performing Arts Graduate Acting Dance Design for Stage &amp; Film Drama Graduate Musical Theatre Writing Performance Studies Maurice Kanbar Institute of Film &amp; Television Undergraduate Film &amp; Television Graduate Film Cinema Studies Moving Image Archiving &amp; Preservation (MIAP) Rita &amp; Burton Goldberg Department of Dramatic Writing Emerging Media Group Photography &amp; Imaging Interactive Telecommunications (ITP) Clive Davis Institute of Recorded Music NYU Game Center Art &amp; Public Policy / Arts Politics Open Arts Tisch Special Programs Tisch Asia"}]}]