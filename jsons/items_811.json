[{"detail": [{"content": "Suspension Representation Project | NYU School of Law                          Skip to main content                                     &rsaquo;               Quicklinks       Areas of Study   Calendar   Career Services   Colloquia   Courses   Departments   Directories   Docket   Housing   Library   News and Press   NYU Home   Student Links   Technology                                              JD Admissions   Faculty &amp; Scholarship   Global Opportunities   LLM/JSD Admissions   Academics &amp; Courses   Law &amp; Business   Executive Education   Current Students   Public Service   About NYU Law   Alumni &amp; Giving   Centers &amp; Institutes                               Home &rsaquo; Student Organizations &rsaquo; Suspension Representation Project             Suspension Representation Project                 Suspension Representation Project       Contact Us   Key Resources   Suspension Hearing Offices               Related Links           Student Bar Association Other Student Organizations                            New York City School Suspensions   In New York City, the number of superintendent\u2019s suspensions increased by more than 76 percent between 2000 and 2005, jumping from 8,567 to 15,090. A report by the National Economic and Social Rights Initiative (NESRI) found that the NYC schools with the most punitive disciplinary policies are overwhelmingly under-resourced, overcrowded, and primarily attended by low-income students of color. Youth who are suspended from school are more likely to fall behind in school, be retained a grade, and drop out.   Whenever a student faces a superintendent\u2019s suspension, a hearing is scheduled to determine whether the student engaged in the incident as charged and, if so, the appropriate disciplinary measures. Parents have the right to bring a representative to the hearing. However, currently, in NYC, there are very few professionals who represent students in suspension hearings. The vast majority of families are unrepresented at their students\u2019 suspension hearings.     Suspension Representation Project (SRP)   The Suspension Representation Project (SRP) is an advocacy group that trains law students to represent public school students in superintendent\u2019s suspension hearings and help safeguard their right to education. Students at NYU School of Law began SRP in 2007 as part of NYU\u2019s Education Law &amp; Policy Society (Ed Law) to help address the tremendous need to increase access to quality representation for low-income students and parents facing suspension hearings. In the spring of 2009, SRP became a stand-alone student organization, with built in leadership links to the Ed Law Board.   SRP pairs new law student advocates with experienced law student advocates in its attempt to provide excellent training for law students and excellent advocacy for clients. SRP enables law students to develop valuable legal skills including interviewing clients, conducting direct and cross examinations, and delivering closing arguments. SRP routinely helps to shorten the length of the suspension or eliminate the suspension, helping kids stay in school. SRP also participates in coalitions in NYC to try to improve the ways that schools respond to students\u2019 behavior.                                                  Facebook   Twitter   YouTube   Instagram   Visitor Information   Directories   Offices and Departments   Site Map   NYU School of Law                   Prospective Info   Admissions (JD) Admissions (LLM/JSD) Areas of Study Degrees Offered Faculty Profiles Campus Map  Academics   Academic Sitemap Course Descriptions Class Schedules NYU Classes Clinics Academic Calendars  Departments   Academic Services Career Services Financial Aid Graduate Affairs (LLM) Hospitality and Events Housing Human Resources Library Operations and Facilities Records and Registration Student Affairs  Tools and Resources   About the Law School Campus Map Directories Law School News Journals Picture Book                      \u00a9 2015 New York University School of Law. 40 Washington Sq. South, New York, NY 10012. Tel. (212) 998-6100"}]},
{"detail": [{"content": "Suspension Representation Project | NYU School of Law                          Skip to main content                                     &rsaquo;               Quicklinks       Areas of Study   Calendar   Career Services   Colloquia   Courses   Departments   Directories   Docket   Housing   Library   News and Press   NYU Home   Student Links   Technology                                              JD Admissions   Faculty &amp; Scholarship   Global Opportunities   LLM/JSD Admissions   Academics &amp; Courses   Law &amp; Business   Executive Education   Current Students   Public Service   About NYU Law   Alumni &amp; Giving   Centers &amp; Institutes                               Home &rsaquo; Student Organizations &rsaquo; Suspension Representation Project             Suspension Representation Project                 Suspension Representation Project       Contact Us   Key Resources   Suspension Hearing Offices               Related Links           Student Bar Association Other Student Organizations                            New York City School Suspensions   In New York City, the number of superintendent\u2019s suspensions increased by more than 76 percent between 2000 and 2005, jumping from 8,567 to 15,090. A report by the National Economic and Social Rights Initiative (NESRI) found that the NYC schools with the most punitive disciplinary policies are overwhelmingly under-resourced, overcrowded, and primarily attended by low-income students of color. Youth who are suspended from school are more likely to fall behind in school, be retained a grade, and drop out.   Whenever a student faces a superintendent\u2019s suspension, a hearing is scheduled to determine whether the student engaged in the incident as charged and, if so, the appropriate disciplinary measures. Parents have the right to bring a representative to the hearing. However, currently, in NYC, there are very few professionals who represent students in suspension hearings. The vast majority of families are unrepresented at their students\u2019 suspension hearings.     Suspension Representation Project (SRP)   The Suspension Representation Project (SRP) is an advocacy group that trains law students to represent public school students in superintendent\u2019s suspension hearings and help safeguard their right to education. Students at NYU School of Law began SRP in 2007 as part of NYU\u2019s Education Law &amp; Policy Society (Ed Law) to help address the tremendous need to increase access to quality representation for low-income students and parents facing suspension hearings. In the spring of 2009, SRP became a stand-alone student organization, with built in leadership links to the Ed Law Board.   SRP pairs new law student advocates with experienced law student advocates in its attempt to provide excellent training for law students and excellent advocacy for clients. SRP enables law students to develop valuable legal skills including interviewing clients, conducting direct and cross examinations, and delivering closing arguments. SRP routinely helps to shorten the length of the suspension or eliminate the suspension, helping kids stay in school. SRP also participates in coalitions in NYC to try to improve the ways that schools respond to students\u2019 behavior.                                                  Facebook   Twitter   YouTube   Instagram   Visitor Information   Directories   Offices and Departments   Site Map   NYU School of Law                   Prospective Info   Admissions (JD) Admissions (LLM/JSD) Areas of Study Degrees Offered Faculty Profiles Campus Map  Academics   Academic Sitemap Course Descriptions Class Schedules NYU Classes Clinics Academic Calendars  Departments   Academic Services Career Services Financial Aid Graduate Affairs (LLM) Hospitality and Events Housing Human Resources Library Operations and Facilities Records and Registration Student Affairs  Tools and Resources   About the Law School Campus Map Directories Law School News Journals Picture Book                      \u00a9 2015 New York University School of Law. 40 Washington Sq. South, New York, NY 10012. Tel. (212) 998-6100"}, {"content": "Inference and Representation, Fall 2014        Inference and Representation        DS-GA-1005 and CSCI-GA.2569 Fall   201 4                    Overview                     This course covers graphical models, causal inference, and     advanced topics in statistical machine learning. A     graphical model is a probabilistic model, where the     conditional dependencies between the random variables are     specified via a graph. Graphical models provide a flexible     framework for modeling large collections of variables with     complex interactions, as evidenced by their wide domain of     application, including for example machine learning,     computer vision, speech and computational biology. This     course will provide a comprehensive survey of learning and     inference methods in graphical models.                                  Prerequisite: DS-GA-1003/CSCI-GA.2567                                     ( Machine                                      Learning and Computational Statistics ),                                or permission of instructor. Please note        that seats are limited and exceptions to the        prerequisite are only likely to be granted        to PhD students.                             General information      Lecture : Tuesdays, 7:10-9:00pm, in     Warren Weaver Hall 101      Recitation/Laboratory      (required for all students) : Thursdays,     5:10-6:00pm in Silver                                  Center 401                              Instructor :                                     David Sontag                       dsontag {@ | at} cs.nyu.edu                    Lab instructor :                      Yacine Jernite                             jernite {@ | at} cs.nyu.edu                    Grader :                             Prasoon Goyal                             pg1338 {@ | at} nyu.edu                                   Office hours: Tuesdays,     10:30-11:30am. Location:           Center for Data Science, 726 Broadway, 7th Floor            Grading : problem sets (55%) + midterm     exam (20%) + final exam (20%) + participation (5%). Problem Set policy           Book (required) : Kevin Murphy, Machine                                                                   Learning: a Probabilistic Perspective , MIT Press,     2012. I recommend the latest (4th) printing, as the     earlier editions had many typos. You can     tell which printing you have as follows: check the     inside cover, below the \"Library of Congress\"     information. If it says \"10 9 8 ... 4\" you've got the     (correct) fourth print.                Mailing list: To subscribe to the class list,     follow instructions here .                      Draft Schedule   (Readings refer to the Murphy book)                     Week      Date      Topic      Readings      Assignments             1     Sept 2         Introduction, Bayesian networks     [ Slides ]         Chapter 10 (Bayesian networks)     Sec. 17.3, 17.4, 17.5.1 (HMMs)           How to write     a spelling corrector (optional)           Introduction                                     to Probabilistic Topic Models (optional)                   ps1 due Sept. 15 at     10pm      [ Solutions ]                      2     Sept 9         Undirected graphical models [ Slides ]          Conditional random fields         Sec. 9-9.2.5 (exponential family)     Sec. 19-19.4.4, 19.6 (MRFs, CRFs)           An Introduction     to Conditional Random Fields (section 2)           Original                                     paper introducing CRFs (optional)           Application                       to camera lens blur (optional)           Application                       to predicting wind flow (optional)                                   3         Sept 16          Learning and causal inference     [ Slides ] [ BN                      structure slides ]          Maximum likelihood learning, structure learning, causal    networks     Sec. 26-26.6 (structure learning &amp;    causal inference)     Sec. 2.8.2 (KL-divergence)           Overview                            of causal inference in statistics by Judea Pearl (sec.    3.2; optional)           Paper on     causal inference used for computational advertising     (optional)           Science                            paper on learning gene regulatory networks (optional)           Learning                   causal networks using interventions (optional)         ps2 due    Sept. 26 at 5pm     [ Solutions ]                 4         Sept 23         Exact inference [ Slides ]          Variable elimination, treewidth         Chapter 20              ps3 ( data ) due Oct. 10 at     5pm                 5         Sept 30         Belief propagation     [ Slides ] [ Notes ]         Sec. 22.2 (loopy belief propagation)           Using loopy BP     for stereo vision (optional)                              6         Oct 7         Integer linear programming      (MAP inference) [ Slides ]          Linear programming relaxations, dual decomposition     Sec. 22.6 (MAP inference)           Derivation     relating dual decomposition &amp; LP relaxations           Introduction                                     to Dual Decomposition for Inference (optional)           Integer               Programming for Bayesian Network Structure Learning     (optional)                                  Oct 21          (no class Oct 14, Fall Recess; will hold office    hours on 10/14)          Midterm exam      (in class)                                           7         Oct 28         Variational inference [ Slides ]          Mean-field approximation     Sec. 21.1-21.4           Graphical                                  models, exponential families, and variational inference     (optional)           Stochastic             Variational Inference (optional)              ps4 ( data ) due Nov. 17 at     5pm                      8         Nov 4         Monte-Carlo methods for inference [ Slides 1 ] [ Slides          2 ]          Importance sampling, Metropolis Hastings, Gibbs sampling         Sec. 23.1-23.4     Sec. 17.2.3     Sec. 24.1-24.4                                   9         Nov 11         More variational inference [ Slides ]           TRW, loopy BP         Sec. 22.3-22.4                                   10         Nov 18     Discovering latent variables using the     method-of-moments      [ Slides 1 ]    [ Slides 2 ]            Guest         lecture by Daniel         Hsu          A Method of     Moments for Mixture Models and Hidden Markov Models     (Sec. 1 &amp; 2)           Method        of Moments for Topic Modeling (optional)                         11     Nov 25     Structured prediction     [ Slides ]         Sec. 19.7 (also, review 19.6 again)           Tutorial       on Structured Prediction (optional)           Original                                  paper introducing structured perceptron (optional)           Cutting-Plane                                  Training of Structural SVMs (optional)           Block-Coordinate                                  Frank-Wolfe Optimization for Structural SVMs     (optional)          ps5 ( data ) due Dec. 8 at     5pm                      12     Dec 2         Learning Markov networks      [ Slides 1 ] [ Slides      2 ]          Pseudo-likelihood, deep generative models          Sec. 19.5, 26.7-26.8     Sec. 27.7, 28.2           Notes on     pseudo-likelihood           An Introduction     to Conditional Random Fields (section 4; optional)           Approximate maximum     entropy learning in MRFs (optional)                         13     Dec 9         Advanced topics in learning           Probabilistic programming                  Talk     for recent ICML 2014 paper on deep    generative models (optional)           Probabilistic     programming: overview and bibliography (optional)           Probabilistic     programming: BUGS (optional)           Probabilistic programming:     Church book (optional)                                  Dec 16          Final exam (in class)                                     Acknowledgements : Many thanks to the Toyota Technological  Institute, Hebrew University, UC Berkeley, and Stanford University  for sharing material used in slides and homeworks                          Reference materials           Optional course text: Probabilistic      Graphical Models: Principles and Techniques by     Daphne Koller and Nir Friedman, MIT Press (2009).      Machine learning books            Trevor Hastie, Rob Tibshirani, and Jerry Friedman, Elements of      Statistical Learning , Second Edition, Springer,      2009. (Can be downloaded as PDF file.)       David Barber, Bayesian                                                                   Reasoning and Machine Learning , Cambridge      University Press, 2012. (Can be downloaded as PDF      file.)           Probability            Chapter 2 of Murphy             Review                                                                    notes from Stanford's machine learning class       Sam Roweis's probability                                                                    review           Optimization            Convex                                                                    Optimization by Stephen Boyd and Lieven      Vandenberghe. (Can be downloaded as PDF file.)                                  Problem                                      Set policy            I expect you to try solving each problem set on your own.    However, when being stuck on a problem, I encourage     you to collaborate with other students in the class, subject    to the following rules:               You may discuss a problem with any student in this     class, and work together on solving it. This can involve     brainstorming and verbally discussing the problem, going     together through possible solutions, but should not      involve one student telling another a complete solution.                Once you solve the homework, you must write up      your solutions on your own , without looking at     other people's write-ups or giving your write-up to     others.                In your solution for each problem, you must write      down the names of any person with whom you     discussed it. This will not affect your grade.                Do not consult solution manuals or other people's     solutions from similar courses.           Late submission policy : During the    semester you are allowed at most two extensions on the    homework assignment. Each extension is for at most 48 hours    and carries a penalty of 25% off your assignment."}]},
{"detail": [{"content": "Suspension Representation Project | NYU School of Law                          Skip to main content                                     &rsaquo;               Quicklinks       Areas of Study   Calendar   Career Services   Colloquia   Courses   Departments   Directories   Docket   Housing   Library   News and Press   NYU Home   Student Links   Technology                                              JD Admissions   Faculty &amp; Scholarship   Global Opportunities   LLM/JSD Admissions   Academics &amp; Courses   Law &amp; Business   Executive Education   Current Students   Public Service   About NYU Law   Alumni &amp; Giving   Centers &amp; Institutes                               Home &rsaquo; Student Organizations &rsaquo; Suspension Representation Project             Suspension Representation Project                 Suspension Representation Project       Contact Us   Key Resources   Suspension Hearing Offices               Related Links           Student Bar Association Other Student Organizations                            New York City School Suspensions   In New York City, the number of superintendent\u2019s suspensions increased by more than 76 percent between 2000 and 2005, jumping from 8,567 to 15,090. A report by the National Economic and Social Rights Initiative (NESRI) found that the NYC schools with the most punitive disciplinary policies are overwhelmingly under-resourced, overcrowded, and primarily attended by low-income students of color. Youth who are suspended from school are more likely to fall behind in school, be retained a grade, and drop out.   Whenever a student faces a superintendent\u2019s suspension, a hearing is scheduled to determine whether the student engaged in the incident as charged and, if so, the appropriate disciplinary measures. Parents have the right to bring a representative to the hearing. However, currently, in NYC, there are very few professionals who represent students in suspension hearings. The vast majority of families are unrepresented at their students\u2019 suspension hearings.     Suspension Representation Project (SRP)   The Suspension Representation Project (SRP) is an advocacy group that trains law students to represent public school students in superintendent\u2019s suspension hearings and help safeguard their right to education. Students at NYU School of Law began SRP in 2007 as part of NYU\u2019s Education Law &amp; Policy Society (Ed Law) to help address the tremendous need to increase access to quality representation for low-income students and parents facing suspension hearings. In the spring of 2009, SRP became a stand-alone student organization, with built in leadership links to the Ed Law Board.   SRP pairs new law student advocates with experienced law student advocates in its attempt to provide excellent training for law students and excellent advocacy for clients. SRP enables law students to develop valuable legal skills including interviewing clients, conducting direct and cross examinations, and delivering closing arguments. SRP routinely helps to shorten the length of the suspension or eliminate the suspension, helping kids stay in school. SRP also participates in coalitions in NYC to try to improve the ways that schools respond to students\u2019 behavior.                                                  Facebook   Twitter   YouTube   Instagram   Visitor Information   Directories   Offices and Departments   Site Map   NYU School of Law                   Prospective Info   Admissions (JD) Admissions (LLM/JSD) Areas of Study Degrees Offered Faculty Profiles Campus Map  Academics   Academic Sitemap Course Descriptions Class Schedules NYU Classes Clinics Academic Calendars  Departments   Academic Services Career Services Financial Aid Graduate Affairs (LLM) Hospitality and Events Housing Human Resources Library Operations and Facilities Records and Registration Student Affairs  Tools and Resources   About the Law School Campus Map Directories Law School News Journals Picture Book                      \u00a9 2015 New York University School of Law. 40 Washington Sq. South, New York, NY 10012. Tel. (212) 998-6100"}, {"content": "Inference and Representation, Fall 2014        Inference and Representation        DS-GA-1005 and CSCI-GA.2569 Fall   201 4                    Overview                     This course covers graphical models, causal inference, and     advanced topics in statistical machine learning. A     graphical model is a probabilistic model, where the     conditional dependencies between the random variables are     specified via a graph. Graphical models provide a flexible     framework for modeling large collections of variables with     complex interactions, as evidenced by their wide domain of     application, including for example machine learning,     computer vision, speech and computational biology. This     course will provide a comprehensive survey of learning and     inference methods in graphical models.                                  Prerequisite: DS-GA-1003/CSCI-GA.2567                                     ( Machine                                      Learning and Computational Statistics ),                                or permission of instructor. Please note        that seats are limited and exceptions to the        prerequisite are only likely to be granted        to PhD students.                             General information      Lecture : Tuesdays, 7:10-9:00pm, in     Warren Weaver Hall 101      Recitation/Laboratory      (required for all students) : Thursdays,     5:10-6:00pm in Silver                                  Center 401                              Instructor :                                     David Sontag                       dsontag {@ | at} cs.nyu.edu                    Lab instructor :                      Yacine Jernite                             jernite {@ | at} cs.nyu.edu                    Grader :                             Prasoon Goyal                             pg1338 {@ | at} nyu.edu                                   Office hours: Tuesdays,     10:30-11:30am. Location:           Center for Data Science, 726 Broadway, 7th Floor            Grading : problem sets (55%) + midterm     exam (20%) + final exam (20%) + participation (5%). Problem Set policy           Book (required) : Kevin Murphy, Machine                                                                   Learning: a Probabilistic Perspective , MIT Press,     2012. I recommend the latest (4th) printing, as the     earlier editions had many typos. You can     tell which printing you have as follows: check the     inside cover, below the \"Library of Congress\"     information. If it says \"10 9 8 ... 4\" you've got the     (correct) fourth print.                Mailing list: To subscribe to the class list,     follow instructions here .                      Draft Schedule   (Readings refer to the Murphy book)                     Week      Date      Topic      Readings      Assignments             1     Sept 2         Introduction, Bayesian networks     [ Slides ]         Chapter 10 (Bayesian networks)     Sec. 17.3, 17.4, 17.5.1 (HMMs)           How to write     a spelling corrector (optional)           Introduction                                     to Probabilistic Topic Models (optional)                   ps1 due Sept. 15 at     10pm      [ Solutions ]                      2     Sept 9         Undirected graphical models [ Slides ]          Conditional random fields         Sec. 9-9.2.5 (exponential family)     Sec. 19-19.4.4, 19.6 (MRFs, CRFs)           An Introduction     to Conditional Random Fields (section 2)           Original                                     paper introducing CRFs (optional)           Application                       to camera lens blur (optional)           Application                       to predicting wind flow (optional)                                   3         Sept 16          Learning and causal inference     [ Slides ] [ BN                      structure slides ]          Maximum likelihood learning, structure learning, causal    networks     Sec. 26-26.6 (structure learning &amp;    causal inference)     Sec. 2.8.2 (KL-divergence)           Overview                            of causal inference in statistics by Judea Pearl (sec.    3.2; optional)           Paper on     causal inference used for computational advertising     (optional)           Science                            paper on learning gene regulatory networks (optional)           Learning                   causal networks using interventions (optional)         ps2 due    Sept. 26 at 5pm     [ Solutions ]                 4         Sept 23         Exact inference [ Slides ]          Variable elimination, treewidth         Chapter 20              ps3 ( data ) due Oct. 10 at     5pm                 5         Sept 30         Belief propagation     [ Slides ] [ Notes ]         Sec. 22.2 (loopy belief propagation)           Using loopy BP     for stereo vision (optional)                              6         Oct 7         Integer linear programming      (MAP inference) [ Slides ]          Linear programming relaxations, dual decomposition     Sec. 22.6 (MAP inference)           Derivation     relating dual decomposition &amp; LP relaxations           Introduction                                     to Dual Decomposition for Inference (optional)           Integer               Programming for Bayesian Network Structure Learning     (optional)                                  Oct 21          (no class Oct 14, Fall Recess; will hold office    hours on 10/14)          Midterm exam      (in class)                                           7         Oct 28         Variational inference [ Slides ]          Mean-field approximation     Sec. 21.1-21.4           Graphical                                  models, exponential families, and variational inference     (optional)           Stochastic             Variational Inference (optional)              ps4 ( data ) due Nov. 17 at     5pm                      8         Nov 4         Monte-Carlo methods for inference [ Slides 1 ] [ Slides          2 ]          Importance sampling, Metropolis Hastings, Gibbs sampling         Sec. 23.1-23.4     Sec. 17.2.3     Sec. 24.1-24.4                                   9         Nov 11         More variational inference [ Slides ]           TRW, loopy BP         Sec. 22.3-22.4                                   10         Nov 18     Discovering latent variables using the     method-of-moments      [ Slides 1 ]    [ Slides 2 ]            Guest         lecture by Daniel         Hsu          A Method of     Moments for Mixture Models and Hidden Markov Models     (Sec. 1 &amp; 2)           Method        of Moments for Topic Modeling (optional)                         11     Nov 25     Structured prediction     [ Slides ]         Sec. 19.7 (also, review 19.6 again)           Tutorial       on Structured Prediction (optional)           Original                                  paper introducing structured perceptron (optional)           Cutting-Plane                                  Training of Structural SVMs (optional)           Block-Coordinate                                  Frank-Wolfe Optimization for Structural SVMs     (optional)          ps5 ( data ) due Dec. 8 at     5pm                      12     Dec 2         Learning Markov networks      [ Slides 1 ] [ Slides      2 ]          Pseudo-likelihood, deep generative models          Sec. 19.5, 26.7-26.8     Sec. 27.7, 28.2           Notes on     pseudo-likelihood           An Introduction     to Conditional Random Fields (section 4; optional)           Approximate maximum     entropy learning in MRFs (optional)                         13     Dec 9         Advanced topics in learning           Probabilistic programming                  Talk     for recent ICML 2014 paper on deep    generative models (optional)           Probabilistic     programming: overview and bibliography (optional)           Probabilistic     programming: BUGS (optional)           Probabilistic programming:     Church book (optional)                                  Dec 16          Final exam (in class)                                     Acknowledgements : Many thanks to the Toyota Technological  Institute, Hebrew University, UC Berkeley, and Stanford University  for sharing material used in slides and homeworks                          Reference materials           Optional course text: Probabilistic      Graphical Models: Principles and Techniques by     Daphne Koller and Nir Friedman, MIT Press (2009).      Machine learning books            Trevor Hastie, Rob Tibshirani, and Jerry Friedman, Elements of      Statistical Learning , Second Edition, Springer,      2009. (Can be downloaded as PDF file.)       David Barber, Bayesian                                                                   Reasoning and Machine Learning , Cambridge      University Press, 2012. (Can be downloaded as PDF      file.)           Probability            Chapter 2 of Murphy             Review                                                                    notes from Stanford's machine learning class       Sam Roweis's probability                                                                    review           Optimization            Convex                                                                    Optimization by Stephen Boyd and Lieven      Vandenberghe. (Can be downloaded as PDF file.)                                  Problem                                      Set policy            I expect you to try solving each problem set on your own.    However, when being stuck on a problem, I encourage     you to collaborate with other students in the class, subject    to the following rules:               You may discuss a problem with any student in this     class, and work together on solving it. This can involve     brainstorming and verbally discussing the problem, going     together through possible solutions, but should not      involve one student telling another a complete solution.                Once you solve the homework, you must write up      your solutions on your own , without looking at     other people's write-ups or giving your write-up to     others.                In your solution for each problem, you must write      down the names of any person with whom you     discussed it. This will not affect your grade.                Do not consult solution manuals or other people's     solutions from similar courses.           Late submission policy : During the    semester you are allowed at most two extensions on the    homework assignment. Each extension is for at most 48 hours    and carries a penalty of 25% off your assignment."}, {"content": "Required Courses - NYU Center for Data Science                                                                                                                            About CDS       Overview    About CDS    What is Data Science?    News &#038; Events    Partnerships    Research    Opportunities    Moore-Sloan    Moore-Sloan Data Science Environment at NYU    Data Science Showcase    Data Science Lunch Seminar Series    Data Science Incubator       Programs &#038; Admissions       MS Programs    Overview    Curriculum    Non-Degree Program    Faculty &#038; Staff    Admissions    Requirements    Careers in Data Science    Apply    Frequently Asked Questions       Current Students       Welcome    Registration Information    Student Resources    Course Grid    Course Pages    Important Dates    Student Profiles    Current Student Questions       Contact Us                                                                 Center for Data Science                                                                                                                   About CDS       Overview    About CDS    What is Data Science?    News &#038; Events    Partnerships    Research    Opportunities    Moore-Sloan    Moore-Sloan Data Science Environment at NYU    Data Science Showcase    Data Science Lunch Seminar Series    Data Science Incubator       Programs &#038; Admissions       MS Programs    Overview    Curriculum    Non-Degree Program    Faculty &#038; Staff    Admissions    Requirements    Careers in Data Science    Apply    Frequently Asked Questions       Current Students       Welcome    Registration Information    Student Resources    Course Grid    Course Pages    Important Dates    Student Profiles    Current Student Questions       Contact Us                                          NYU Center for Data Science                                                                                                                                            Required Courses                                                                                                                                                                                                             DS-GA-1001: Introduction to Data Science   Course credits: 3  Year of the Curriculum: One  Semester: Fall / Spring     Introduces students to the fundamental principles of data science that\u00a0underlie the algorithms, processes, methods, and data-analytic\u00a0thinking. Introduces students to algorithms and tools based on these\u00a0principles. Introduces frameworks to support problem-focused data-analytic thinking.   Course aims and objectives:   After taking this class, student should:     Approach business problems data-analytically. Think carefully and\u00a0systematically about whether &amp; how data can improve a particular\u00a0application, to understand a phenomenon better and especially to\u00a0make better-informed decisions and automated decisions.   Understand fundamental principles of data science, such as using\u00a0data to get information about an unknown quantity of interest,\u00a0calculating and using data similarity, fitting models to data,\u00a0supervised and unsupervised modeling, overfitting and its avoidance,\u00a0evaluation and model analytics, visualization, predictive modeling,\u00a0causal inference, the data mining process, problem decomposition,\u00a0data science strategy, solution deployment, and more.   Be able to apply the most important data science methods, using\u00a0open-source tools.     Prerequisites:       Basic probability or Statistics (undergraduate level)   Linear Algebra   Some experience in programming: Java, C, C++, Python, Perl, or similar languages, equivalent to two introductory courses in programming, such as \u201cIntroduction to Programming\u201d and \u201cData Structures and Algorithms.\u201d     Co-requisite:     &#8220;Programming for Data Science&#8221;\u00a0(waived with adequate experience; as decided\u00a0by MSDS program administration)     DS-GA-1002: Statistical and Mathematical Methods   Course credits: 3  Year of the Curriculum: One  Semester: Fall     This course briefly introduces basic statistical and mathematical methods needed in the practice of data science. It covers basic methods in probability, statistics, linear algebra, and optimization.   Course aims and objectives:     Teach basics of statistics and probability   Teach basic methods for solving linear systems and eigensystems, and demonstrate their use in regression and data representation.   Teach basic methods for multivariate function optimization (e.g gradient descent), and demonstrate their use in non-linear regression.     Prerequisites:       Undergraduate level probability course or statistics   Calculus I   Linear Algebra   Some experience in programming: Java, C, C++, Python, R, Lua, Ruby, OCaml or similar languages, equivalent to two introductory courses in programming, such as \u201cIntroduction to Programming\u201d and \u201cData Structures and Algorithms.\u201d   Some prerequisites may be waived with permission from the instructor. \u00a0     DS-GA-1003: Machine Learning and Computational Statistics   Course credits: 3  Year of the Curriculum: One  Semester: Spring     The course covers a wide variety of topics in machine learning, pattern recognition, statistical modeling, and neural computation. It covers the mathematical methods and theoretical aspects, but primarily focuses on algorithmic and practical issues.   Course aims and objectives:     Teach intermediate topics in machine learning   Provide hands-on experience in designing and programming data science algorithms     Prerequisites:       DS-GA-1001: Introduction to Data Science, or undergraduate course in Machine Learning.   Some experience in programming: Java, C, C++, Python, R, Lua, Ruby, OCaml or similar languages, equivalent to two introductory courses in programming, such as \u201cIntroduction to Programming\u201d and \u201cData Structures and Algorithms.\u201d   Some prerequisites may be waived with permission from the instructor. \u00a0     DS-GA-1004: Big Data   Course credits: 3  Year of the Curriculum: One  Semester: Spring     Big Data requires the storage, organization, and processing of data at a scale and efficiency that go well beyond the capabilities of conventional information technologies. In this course, we will study the state of the art in big data management: we will learn about algorithms, techniques and tools needed to support big data processing. In addition, we will examine real applications that require massive data analysis and how they can be implemented on Big Data platforms. The course will consist of lectures based both on textbook material and scientific papers. It will also include programming assignments that will provide students with hands-on experience on building data-intensive applications using existing Big Data platforms, including Amazon AWS. Besides lectures given by the instructor, we will also have guest lectures by experts in some of the topics we will cover.   Prerequisites:       DS-GA-1001: Introduction to Data Science or equivalent undergraduate course   DS-GA-1002: Statistical and Mathematical Methods   Some prerequisites may be waived with permission from the instructor.     DS-GA-1005: Inference and Representation   Course number: Course credits: 3  Year of the Curriculum: Two  Semester: Fall     This course covers graphical models, causal inference, and advanced topics in statistical machine learning.   Course aims and objectives:     Teach exact and approximate inference methods in graphical models.   Teach learning techniques for graphical models and structured prediction.   Teach methods for causal inference.     Prerequisites:       DS-GA-1004: Machine Learning and Computational Statistics     DS-GA-1006: Capstone Project and Presentation in Data Science   Course Number: Course Credits: 3  Year of the Curriculum: Two  Semester: Fall   The purpose of the capstone project is for the students to apply theoretical knowledge acquired during the the program to a real project involving actual data in a realistic setting. During the project, students engage in the entire process of solving a real-world data science project: from collecting and processing actual data, to applying a suitable and appropriate analytic method to the problem. Both the problem statements for the project assignments and the datasets orginate from real-world domains similar to those that students might typically encounter within industry, government, NGO, or academic research.   Depending upon a project&#8217;s complexity, students work individually or in small teams on a problem statement typically specified by an industry or governmental sponsor employing data set provided by the sponsor. Academic, governmental and NGO research groups (both from within, as well as external to NYU) may also propose projects. A list of projects will be posted early in the semester, so students can align themselves  with problems statements corresponding to their individual interests. As the project and problem statements warrant, students may be permitted to organize into teams of two to three participants. Teams larger than three will be considered for approval on a case-by-case basis, as warranted. The final problem statements and the composition of the teams will be approved by the Course Director in coordination with any relevant faculty advisor and the sponsor&#8217;s assigned representative (i.e. the sponsoring Project Coach).   Each project team will be supervised by the Course Director (in some cases with a relevant faculty advisor) and advised by the Project Coach assigned from the academic, governmental, NGO or industry sponsor.   Here are\u00a0two examples of\u00a0illustrative projects:\u00a0     An large insurance company has an anonymized dataset of workers compensation claimants. The insurance claims dataset incorporates corresponding data, e.g. claimant demographics, claims payments, etc. A team comprised of capstone students, advised by the instructor in conjunction with a technical coach from the company, employ the dataset to develop and implement an analytic solution using software tools studied in previous courses.   A professor from the Department of Politics has a dataset about tweets from individuals, with some indication of the party affiliation of the individual. Students use text classification methods studied in class to build a system that can predict party affiliation and voting behavior from tweets in conjunction social network tools to sort tweeters.     Course aims and objectives:       Students will demonstrate an ability to handle a problem in data science from the point of problem definition through delivery of a solution. In doing so, they will demonstrate proficiency in collecting and processing real-world data, in designing the best methods to solve the problem, and in implementing a solution.   Students will demonstrate competence in presenting material by delivering two presentations: a proposal on how to approach the problem and their final solution.   Students will learn how to work in small teams by working with at least one other student on their project.   Students will write a report on their project for evaluation by the instructor in consultation with the project advisors. The report will be structured as a typical research paper, and hence will include three main sections: 1. motivation and problem definition, existing approaches to the problem; 2. proposed solution; 3. results, conclusion, and directions for future work.     Prerequisites:       Successful completion of DS-GA-1001: Introduction to Data Science, DS-GA-1002: Statistical and Mathematical Methods, DS-GA-1003: Machine Learning, and DS-GA-1004: Big Data; or permission of the instructor, based on having successfully completed similar course work or gained experience in hands-on projects                                                                                                                                                                                                                             Location     The Center for Data Science is located on the Washington Square campus in the heart of Greenwich Village.     726 Broadway, 7th Floor, New York, NY 10003   Email: datascience-group@nyu.edu         Map: View                                                                    CDS Communities       Twitter   \u200e  Facebook     LinkedIn                                                                          CDS Science     Blog                                                                                                                                                      About the NYU Center for Data Science   Opportunities   What is Data Science?   Programs &#038; Admissions   People                                           Copyright &copy; 2013 |         New York University"}]}]