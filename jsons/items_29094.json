[{"detail": [{"content": "NYU Course: E85.1810: Midi for Non-Majors                                                                                    E85.1810: Midi for Non-Majors   Copyright 1997-2002 John V. Gilbert All Rights Reserved                SOUND, SYNTHESIS, AND SYNTHESIZERS   Synthesizers have brought us to a new awareness of sound and our capacity to create sound. We can use techniques such as FM synthesis, additive, syntheises, sampling and signal processing as tools to create new sound resources. As you become involved with the electronic computer medium, you will find that you will always be searching for sounds, because to a very large extent each new composition can require its own special need for distinctive sounds. Thus the creation or synthesizing of sound becomes a part of the creative process.     Fundamentals of Sound  As musicians, we need to recognize that because the art that we are advocating and teaching is grounded in sound, we need to understand the materials of our art just as much as a visual arts teacher needs to understand color. We need to have a deep sense of sound and learn to sensitize ourselves to the sounds and around us and why different sounds sound as they do.   All sound comes from vibrations of materials or elements. These vibrations create waveforms which are usually quite complex. A sound wave is made up of oscillations which occur at frequency levels which are audible. Waves exist also well beyond the audible range of the human ear which is anywhere from 20 cycles per second to 20,000 (higher or lower depending on the individual). For example, microwaves (extremely high frequencies) are used to cook our food or transmit telephone communication. There are also huge wave forms (earthquakes...a slow \"vibration of the earth's crust, and even Keppler's idea of music of the spheres: ratio of frequencies generated in their cycles around the sun).    Everything around us is vibration. We are vibration: a pulse that also acts in harmony with the frequencies of the nervous system and brain waves. Thus, even though we do not \"see\" sound (although we can see an analog of it in an oscilloscope), sound is quite tangible and quantifiably present....and vibrations exist on a continuum. Our senses equip us to experience vibrations all the way from touch, to sight (frequencies the eye can detect, including color), to hearing (frequencies for the ear). We use various instruments to detect frequencies and waveforms that are below and beyond our human senses.   We use oscilloscopes to study waveforms and with the aid of digital technology conduct elaborate analyses of of waveforms through spectrum analysis. The World Wide Web provides some excellent resources about the phenomena of waveforms. A good introduction to the use of the oscilloscope to study waveforms is provided on the World Wide Web by Tektronix .  Today we describe cycles per second as \"hz\" in honor of Heinrich Hertz (1857-1894), a physicist who studied the phenomena of waves and discovered radio waves which later led to Marconi's invention of the wireless telegraph. Thus, today we might find that stereo speakers or amplifiers for sound systems might be rated as handling from 30 hz to 20,000 hz. And when we speak of pitches in music such as \"A=440\"...we mean 440 hz.        Principles of Synthesis   In early electronic music, the oscillators used in electronic testing equipment were used as a source for waveforms which were then shaped by voltages, combined (additive synthesis), filtered (subtractive synthesis), or shaped by modulation (modulating one wave form with the frequency another wave form or FM synthesis. Some have associated the sine wave with the flute, the sawtooth with double reeds such as the oboe or bassoon, and the square wave with the clarinet. Nevertheless, these electronically produced wave forms have their own characteristic and contribute greatly to the character of sound in early electronic music. It wasn't until sampling of acoustic instruments and analysis of their waveform spectra became widespread that the digital voices accurately imitated the acoustic instruments.    Synthesizing Sound  In sound synthesis, we can conceptualize the components that we will work with for shaping sound as sources, treatments, and controls. In the time of analog synthesis the distinction between controls and treatments were often different according to the synthesizer and its particular approach.   Sources have to do with the sounds that are created by the synthesizer. Initially, sources were wave forms such as sine, square, triangle, and saw-tooth waves. Wave shapers could change the wave-form through voltage control. Voltage control (raising and lower voltages) could affect any parameter of sound. Later, sources also become sampled sounds.   Treatments might be reverberation (creating echo effects by sending the signal down a spring or a metal plate), decay (recording the sound a playing back at controlled increments), filtering (passing the sound through devices which wipe out overtones in specific patterns), and envelope. In many synthesizers the envelope was designed as a control rather than a treatment. In general treatments are various effect modules that we patch the sound through. Controls are generally voltages which we use to affect the sound (frequency and amplitude modulation, for example).   The envelope is traditionally described as an ADSR configuration (A = attack, D = decay, S = sustain, R = release). The envelope of a sound, then, is a \"history\" of the sound as it comes into being, is present and then dies away. Envelopes of instruments, along with their overtones or partials, are what contribute to the unique sound of acoustic instruments. Through voltage controls (now represented digitally), we can control the speed of the Attack and Decay, control the duration (sustain) of the note, and control the speed of the release. This gives us tremendous variety of possibilities in building our own instrumental sounds. Some synthesizers attempt to create more nuance of sound by adding stages to the envelope, so that we can have ADSDSR, and so on. Adding additional Decays and Sustains can give more nuance to the sound.         SOURCES, TREATMENTS, AND CONTROLS  It can be helpful to conceptualize the process of synthesis as consisting of three elements:   Sources which are usually electronically generated sources but can also included recorded sound, and digital samples of sound. In general, sources are the starting point of synthesis. We begin with a sound and perhaps pass it through  Treatments such as a filter, envelope shaper, or a reverberation unit. Treaments are generally specially designed circuits (or in computers, specially configured programs) assigned to perform a specific treatment of the sound. Sources and Treatments can be subject to  Controls which can be used to afftect the timbre and pattern of sounds.   we can use the frequencies and amplitudes of waveforms to control sources and   treatments. In some synthesizer designs, the envelope is shaped by control voltages   which directly control the attack, sustain, and decay of sounds. In other synthesizers   those controls have been collected into a specially designed circuit and is   connected as a treatment (i.e., the sound passes through the treatment and the   treatment has voltage-control knobs or levers to affect the envelope).  Flowcharts can be used to describe the flow of the signal. A common approach   to flowcharts is to show the source passing through treatments on the horizontal   plane, while the vertical axis indicates the use of controls on specific modules.     In this flow chart, the source of the signal is a triangle wave, controlled   by a sine wave using frequency modulation to create a vibrato, and a keyboard   to control pitch of the triangle wave. The source is passed through a filter   which is controlled by a triangle wave to create a changing timbre over time.   This signal is routed through an envelope shaper to control the ADSR of the   signal and through a reverberation unit to create an echo effect. The signal   is then amplified and sent to the speaker.     Wave forms   In the Table below are some of the basic waves created by voltages as they   would appear on the screen of an oscilloscope. The sine wave is the most basic   wave form which has no overtones present in the sound as opposed to the sawtooth   which has odd and even harmonics present in the wave to give a sharp, biting   timbre. Sounds have their distinctions according to the overtones present   and how those overtones are reinforced.   Notice that the wave forms presented here have a positive and negative pulse   (above and below the line) which represent one complete cycle. The number   of cycles per second determines the frequency or pitch of the wave and the   type of wave form determines the timbre or color of the waveform.   The waveforms presented here were basic sound sources for early electronic   music.                    Common Wave Forms in Electronic Music                                                                                                                  Modulation   Waveforms can be used as sources, but they can also be used as controls.   Therefore Voltage Control Synthesizers (Analog Synthesizer) could use waveforms   as a source of sound and also as a control of sound. Oscillators producing   these waveforms would include extremely low frequencies, well below the audible   level. The last example in the above table illustrates a sine wave being modulated   by a sine wave. The \"carrier wave\" is the sine wave that is audible. The modulating   sine wave creates the pattern of the sine wave shape (slow curving up and   down pattern). This allows the waveform to create a pattern by using a very   low frequency of an oscillator to modulate the frequency of another oscillator.   This kind of control is known as frequency modulation. Modulation simply means   \"to change.\" Higher modulating frequencies would be used to create vibratos.   Amplitude modulation would increase the range of the modulation (the higher   the amplitude or volume the greater the range of the modulation). Listen to   this wide-range modulation using a square   carrier wave which gradually acquires vibrato and as the amplitude of the   modulating sine wave is increased, achieves a wider and wider oscillation   (vibrato, but the range is so wide we no longer perceive it as vibrato but   rapidly fluctuating sound) and finally returns to the original narrow vibrato.      Copyright by John V. Gilbert    Send feedback or questions to john.gilbert@nyu.edu"}]},
{"detail": [{"content": "NYU Course: E85.1810: Midi for Non-Majors                                                                                    E85.1810: Midi for Non-Majors   Copyright 1997-2002 John V. Gilbert All Rights Reserved                SOUND, SYNTHESIS, AND SYNTHESIZERS   Synthesizers have brought us to a new awareness of sound and our capacity to create sound. We can use techniques such as FM synthesis, additive, syntheises, sampling and signal processing as tools to create new sound resources. As you become involved with the electronic computer medium, you will find that you will always be searching for sounds, because to a very large extent each new composition can require its own special need for distinctive sounds. Thus the creation or synthesizing of sound becomes a part of the creative process.     Fundamentals of Sound  As musicians, we need to recognize that because the art that we are advocating and teaching is grounded in sound, we need to understand the materials of our art just as much as a visual arts teacher needs to understand color. We need to have a deep sense of sound and learn to sensitize ourselves to the sounds and around us and why different sounds sound as they do.   All sound comes from vibrations of materials or elements. These vibrations create waveforms which are usually quite complex. A sound wave is made up of oscillations which occur at frequency levels which are audible. Waves exist also well beyond the audible range of the human ear which is anywhere from 20 cycles per second to 20,000 (higher or lower depending on the individual). For example, microwaves (extremely high frequencies) are used to cook our food or transmit telephone communication. There are also huge wave forms (earthquakes...a slow \"vibration of the earth's crust, and even Keppler's idea of music of the spheres: ratio of frequencies generated in their cycles around the sun).    Everything around us is vibration. We are vibration: a pulse that also acts in harmony with the frequencies of the nervous system and brain waves. Thus, even though we do not \"see\" sound (although we can see an analog of it in an oscilloscope), sound is quite tangible and quantifiably present....and vibrations exist on a continuum. Our senses equip us to experience vibrations all the way from touch, to sight (frequencies the eye can detect, including color), to hearing (frequencies for the ear). We use various instruments to detect frequencies and waveforms that are below and beyond our human senses.   We use oscilloscopes to study waveforms and with the aid of digital technology conduct elaborate analyses of of waveforms through spectrum analysis. The World Wide Web provides some excellent resources about the phenomena of waveforms. A good introduction to the use of the oscilloscope to study waveforms is provided on the World Wide Web by Tektronix .  Today we describe cycles per second as \"hz\" in honor of Heinrich Hertz (1857-1894), a physicist who studied the phenomena of waves and discovered radio waves which later led to Marconi's invention of the wireless telegraph. Thus, today we might find that stereo speakers or amplifiers for sound systems might be rated as handling from 30 hz to 20,000 hz. And when we speak of pitches in music such as \"A=440\"...we mean 440 hz.        Principles of Synthesis   In early electronic music, the oscillators used in electronic testing equipment were used as a source for waveforms which were then shaped by voltages, combined (additive synthesis), filtered (subtractive synthesis), or shaped by modulation (modulating one wave form with the frequency another wave form or FM synthesis. Some have associated the sine wave with the flute, the sawtooth with double reeds such as the oboe or bassoon, and the square wave with the clarinet. Nevertheless, these electronically produced wave forms have their own characteristic and contribute greatly to the character of sound in early electronic music. It wasn't until sampling of acoustic instruments and analysis of their waveform spectra became widespread that the digital voices accurately imitated the acoustic instruments.    Synthesizing Sound  In sound synthesis, we can conceptualize the components that we will work with for shaping sound as sources, treatments, and controls. In the time of analog synthesis the distinction between controls and treatments were often different according to the synthesizer and its particular approach.   Sources have to do with the sounds that are created by the synthesizer. Initially, sources were wave forms such as sine, square, triangle, and saw-tooth waves. Wave shapers could change the wave-form through voltage control. Voltage control (raising and lower voltages) could affect any parameter of sound. Later, sources also become sampled sounds.   Treatments might be reverberation (creating echo effects by sending the signal down a spring or a metal plate), decay (recording the sound a playing back at controlled increments), filtering (passing the sound through devices which wipe out overtones in specific patterns), and envelope. In many synthesizers the envelope was designed as a control rather than a treatment. In general treatments are various effect modules that we patch the sound through. Controls are generally voltages which we use to affect the sound (frequency and amplitude modulation, for example).   The envelope is traditionally described as an ADSR configuration (A = attack, D = decay, S = sustain, R = release). The envelope of a sound, then, is a \"history\" of the sound as it comes into being, is present and then dies away. Envelopes of instruments, along with their overtones or partials, are what contribute to the unique sound of acoustic instruments. Through voltage controls (now represented digitally), we can control the speed of the Attack and Decay, control the duration (sustain) of the note, and control the speed of the release. This gives us tremendous variety of possibilities in building our own instrumental sounds. Some synthesizers attempt to create more nuance of sound by adding stages to the envelope, so that we can have ADSDSR, and so on. Adding additional Decays and Sustains can give more nuance to the sound.         SOURCES, TREATMENTS, AND CONTROLS  It can be helpful to conceptualize the process of synthesis as consisting of three elements:   Sources which are usually electronically generated sources but can also included recorded sound, and digital samples of sound. In general, sources are the starting point of synthesis. We begin with a sound and perhaps pass it through  Treatments such as a filter, envelope shaper, or a reverberation unit. Treaments are generally specially designed circuits (or in computers, specially configured programs) assigned to perform a specific treatment of the sound. Sources and Treatments can be subject to  Controls which can be used to afftect the timbre and pattern of sounds.   we can use the frequencies and amplitudes of waveforms to control sources and   treatments. In some synthesizer designs, the envelope is shaped by control voltages   which directly control the attack, sustain, and decay of sounds. In other synthesizers   those controls have been collected into a specially designed circuit and is   connected as a treatment (i.e., the sound passes through the treatment and the   treatment has voltage-control knobs or levers to affect the envelope).  Flowcharts can be used to describe the flow of the signal. A common approach   to flowcharts is to show the source passing through treatments on the horizontal   plane, while the vertical axis indicates the use of controls on specific modules.     In this flow chart, the source of the signal is a triangle wave, controlled   by a sine wave using frequency modulation to create a vibrato, and a keyboard   to control pitch of the triangle wave. The source is passed through a filter   which is controlled by a triangle wave to create a changing timbre over time.   This signal is routed through an envelope shaper to control the ADSR of the   signal and through a reverberation unit to create an echo effect. The signal   is then amplified and sent to the speaker.     Wave forms   In the Table below are some of the basic waves created by voltages as they   would appear on the screen of an oscilloscope. The sine wave is the most basic   wave form which has no overtones present in the sound as opposed to the sawtooth   which has odd and even harmonics present in the wave to give a sharp, biting   timbre. Sounds have their distinctions according to the overtones present   and how those overtones are reinforced.   Notice that the wave forms presented here have a positive and negative pulse   (above and below the line) which represent one complete cycle. The number   of cycles per second determines the frequency or pitch of the wave and the   type of wave form determines the timbre or color of the waveform.   The waveforms presented here were basic sound sources for early electronic   music.                    Common Wave Forms in Electronic Music                                                                                                                  Modulation   Waveforms can be used as sources, but they can also be used as controls.   Therefore Voltage Control Synthesizers (Analog Synthesizer) could use waveforms   as a source of sound and also as a control of sound. Oscillators producing   these waveforms would include extremely low frequencies, well below the audible   level. The last example in the above table illustrates a sine wave being modulated   by a sine wave. The \"carrier wave\" is the sine wave that is audible. The modulating   sine wave creates the pattern of the sine wave shape (slow curving up and   down pattern). This allows the waveform to create a pattern by using a very   low frequency of an oscillator to modulate the frequency of another oscillator.   This kind of control is known as frequency modulation. Modulation simply means   \"to change.\" Higher modulating frequencies would be used to create vibratos.   Amplitude modulation would increase the range of the modulation (the higher   the amplitude or volume the greater the range of the modulation). Listen to   this wide-range modulation using a square   carrier wave which gradually acquires vibrato and as the amplitude of the   modulating sine wave is increased, achieves a wider and wider oscillation   (vibrato, but the range is so wide we no longer perceive it as vibrato but   rapidly fluctuating sound) and finally returns to the original narrow vibrato.      Copyright by John V. Gilbert    Send feedback or questions to john.gilbert@nyu.edu"}, {"content": "Audio Workstation Production Lab: Tisch School of the Arts at NYU                               Skip to content        Recorded Music Program Courses Internships News &amp; Events Facilities Alumni Faculty Directory Recorded Music FAQs For High School Students Facebook Twitter YouTube Summer In Recorded Music Academic Advisement Resources 194 Recordings  home &gt; Recorded Music        Audio Workstation Production Lab   &lt;&lt; Loading... &gt;&gt;  *Please Note: You are using an older browser or a browser that doesn't support the styles and javascript used in this page. This browser will not display the photo gallery correctly.                       The Pro Tools Production Lab is available for use by students of all levels. The Lab is equipped with 19 digital audio workstations and a teaching position, and two modular analog synthesizers. Students begin to learn the art of music production in the Lab, as well as advanced music production and programming. In addition to the Lab, there are two personal edit suites available for student use. The digital audio workstations in the suites are equipped similarly to the studio\u2019s computers. Production Lab 502 Student workstations (14) \u2022 Intel iMac \u2022 Pro Tools LE \u2022 mBox 2 \u2022 microKorg or Roland SH-201 synthesizers \u2022 Logic Studio \u2022 Reason ...and plugins from Digidesign, Waves, Bomb Factory and more Synth stations (2) \u2022 Dave Smith PolyEvolver analog synthesizer \u2022 MOTM analog modular synthesizer \u2022 Vostok analog modular synthesizer \u2022 Elektron/Machine Drum SPS1 drum machine \u2022 Intel iMac \u2022 Pro Tools LE \u2022 mBox 2 \u2022 Logic Studio 8 \u2022 Reason ...and plugins from Digidesign, Waves, Bomb Factory and more Teacher workstation \u2022 Mac G5 \u2022 Pro Tools HD Accel system \u2022 Langevin DVC preamp/compressor/EQ \u2022 Full integration with student workstations \u2022 Projection capability \u2022 Logic Studio 8 \u2022 Reason ...and plugins from Digidesign, Waves, Bomb Factory and more Edit Suites 1 and 2 Edit Suites (2) Software \u2022 Pro Tools HD \u2022 Logic Studio \u2022 Reason Plugins \u2022 Waves Gold bundle \u2022 Antares AutoTune \u2022 Bomb Factory classic compressors \u2022 Fairchild 660/670 \u2022 Line 6 Amp Farm \u2022 Maxim \u2022 Amplitube \u2022 Pultec Eq bundle \u2022 Sound Replacer ...and more from Digidesign, Waves, Bomb Factory and Trillium Lane Labs   The Clive Davis Institute of Recorded Music 194 Mercer Street, 5th Floor New York, NY 10012               Phone:       212 992 8400 tisch.recorded.music@nyu.edu           Undergraduate Admissions Graduate Admissions Academic Services Student Life Financial Aid and Scholarships Career Development Counseling Services International Students Incoming Students Graduation Forms What is Special Programs? Study Abroad Summer High School Summer in New York City Courses for Non-Majors Programs for Visiting Students Open Arts Minors Blog Newsletters Videos Admin &amp; Faculty Alumni Contact Us  Tisch Events Calendar Admin &amp; Faculty Calendar NYU Academic Deadlines Calendar Many Ways to Give Why Support Tisch Dean's Council Corporate Giving Foundation Giving Individual Giving Tisch Gala Who We Are Institute of Performing Arts Graduate Acting Dance Design for Stage &amp; Film Drama Graduate Musical Theatre Writing Performance Studies Maurice Kanbar Institute of Film &amp; Television Undergraduate Film &amp; Television Graduate Film Cinema Studies Moving Image Archiving &amp; Preservation (MIAP) Rita &amp; Burton Goldberg Department of Dramatic Writing Emerging Media Group Photography &amp; Imaging Interactive Telecommunications (ITP) Clive Davis Institute of Recorded Music NYU Game Center Art &amp; Public Policy / Arts Politics Open Arts Tisch Special Programs Tisch Asia"}]},
{"detail": [{"content": "NYU Course: E85.1810: Midi for Non-Majors                                                                                    E85.1810: Midi for Non-Majors   Copyright 1997-2002 John V. Gilbert All Rights Reserved                SOUND, SYNTHESIS, AND SYNTHESIZERS   Synthesizers have brought us to a new awareness of sound and our capacity to create sound. We can use techniques such as FM synthesis, additive, syntheises, sampling and signal processing as tools to create new sound resources. As you become involved with the electronic computer medium, you will find that you will always be searching for sounds, because to a very large extent each new composition can require its own special need for distinctive sounds. Thus the creation or synthesizing of sound becomes a part of the creative process.     Fundamentals of Sound  As musicians, we need to recognize that because the art that we are advocating and teaching is grounded in sound, we need to understand the materials of our art just as much as a visual arts teacher needs to understand color. We need to have a deep sense of sound and learn to sensitize ourselves to the sounds and around us and why different sounds sound as they do.   All sound comes from vibrations of materials or elements. These vibrations create waveforms which are usually quite complex. A sound wave is made up of oscillations which occur at frequency levels which are audible. Waves exist also well beyond the audible range of the human ear which is anywhere from 20 cycles per second to 20,000 (higher or lower depending on the individual). For example, microwaves (extremely high frequencies) are used to cook our food or transmit telephone communication. There are also huge wave forms (earthquakes...a slow \"vibration of the earth's crust, and even Keppler's idea of music of the spheres: ratio of frequencies generated in their cycles around the sun).    Everything around us is vibration. We are vibration: a pulse that also acts in harmony with the frequencies of the nervous system and brain waves. Thus, even though we do not \"see\" sound (although we can see an analog of it in an oscilloscope), sound is quite tangible and quantifiably present....and vibrations exist on a continuum. Our senses equip us to experience vibrations all the way from touch, to sight (frequencies the eye can detect, including color), to hearing (frequencies for the ear). We use various instruments to detect frequencies and waveforms that are below and beyond our human senses.   We use oscilloscopes to study waveforms and with the aid of digital technology conduct elaborate analyses of of waveforms through spectrum analysis. The World Wide Web provides some excellent resources about the phenomena of waveforms. A good introduction to the use of the oscilloscope to study waveforms is provided on the World Wide Web by Tektronix .  Today we describe cycles per second as \"hz\" in honor of Heinrich Hertz (1857-1894), a physicist who studied the phenomena of waves and discovered radio waves which later led to Marconi's invention of the wireless telegraph. Thus, today we might find that stereo speakers or amplifiers for sound systems might be rated as handling from 30 hz to 20,000 hz. And when we speak of pitches in music such as \"A=440\"...we mean 440 hz.        Principles of Synthesis   In early electronic music, the oscillators used in electronic testing equipment were used as a source for waveforms which were then shaped by voltages, combined (additive synthesis), filtered (subtractive synthesis), or shaped by modulation (modulating one wave form with the frequency another wave form or FM synthesis. Some have associated the sine wave with the flute, the sawtooth with double reeds such as the oboe or bassoon, and the square wave with the clarinet. Nevertheless, these electronically produced wave forms have their own characteristic and contribute greatly to the character of sound in early electronic music. It wasn't until sampling of acoustic instruments and analysis of their waveform spectra became widespread that the digital voices accurately imitated the acoustic instruments.    Synthesizing Sound  In sound synthesis, we can conceptualize the components that we will work with for shaping sound as sources, treatments, and controls. In the time of analog synthesis the distinction between controls and treatments were often different according to the synthesizer and its particular approach.   Sources have to do with the sounds that are created by the synthesizer. Initially, sources were wave forms such as sine, square, triangle, and saw-tooth waves. Wave shapers could change the wave-form through voltage control. Voltage control (raising and lower voltages) could affect any parameter of sound. Later, sources also become sampled sounds.   Treatments might be reverberation (creating echo effects by sending the signal down a spring or a metal plate), decay (recording the sound a playing back at controlled increments), filtering (passing the sound through devices which wipe out overtones in specific patterns), and envelope. In many synthesizers the envelope was designed as a control rather than a treatment. In general treatments are various effect modules that we patch the sound through. Controls are generally voltages which we use to affect the sound (frequency and amplitude modulation, for example).   The envelope is traditionally described as an ADSR configuration (A = attack, D = decay, S = sustain, R = release). The envelope of a sound, then, is a \"history\" of the sound as it comes into being, is present and then dies away. Envelopes of instruments, along with their overtones or partials, are what contribute to the unique sound of acoustic instruments. Through voltage controls (now represented digitally), we can control the speed of the Attack and Decay, control the duration (sustain) of the note, and control the speed of the release. This gives us tremendous variety of possibilities in building our own instrumental sounds. Some synthesizers attempt to create more nuance of sound by adding stages to the envelope, so that we can have ADSDSR, and so on. Adding additional Decays and Sustains can give more nuance to the sound.         SOURCES, TREATMENTS, AND CONTROLS  It can be helpful to conceptualize the process of synthesis as consisting of three elements:   Sources which are usually electronically generated sources but can also included recorded sound, and digital samples of sound. In general, sources are the starting point of synthesis. We begin with a sound and perhaps pass it through  Treatments such as a filter, envelope shaper, or a reverberation unit. Treaments are generally specially designed circuits (or in computers, specially configured programs) assigned to perform a specific treatment of the sound. Sources and Treatments can be subject to  Controls which can be used to afftect the timbre and pattern of sounds.   we can use the frequencies and amplitudes of waveforms to control sources and   treatments. In some synthesizer designs, the envelope is shaped by control voltages   which directly control the attack, sustain, and decay of sounds. In other synthesizers   those controls have been collected into a specially designed circuit and is   connected as a treatment (i.e., the sound passes through the treatment and the   treatment has voltage-control knobs or levers to affect the envelope).  Flowcharts can be used to describe the flow of the signal. A common approach   to flowcharts is to show the source passing through treatments on the horizontal   plane, while the vertical axis indicates the use of controls on specific modules.     In this flow chart, the source of the signal is a triangle wave, controlled   by a sine wave using frequency modulation to create a vibrato, and a keyboard   to control pitch of the triangle wave. The source is passed through a filter   which is controlled by a triangle wave to create a changing timbre over time.   This signal is routed through an envelope shaper to control the ADSR of the   signal and through a reverberation unit to create an echo effect. The signal   is then amplified and sent to the speaker.     Wave forms   In the Table below are some of the basic waves created by voltages as they   would appear on the screen of an oscilloscope. The sine wave is the most basic   wave form which has no overtones present in the sound as opposed to the sawtooth   which has odd and even harmonics present in the wave to give a sharp, biting   timbre. Sounds have their distinctions according to the overtones present   and how those overtones are reinforced.   Notice that the wave forms presented here have a positive and negative pulse   (above and below the line) which represent one complete cycle. The number   of cycles per second determines the frequency or pitch of the wave and the   type of wave form determines the timbre or color of the waveform.   The waveforms presented here were basic sound sources for early electronic   music.                    Common Wave Forms in Electronic Music                                                                                                                  Modulation   Waveforms can be used as sources, but they can also be used as controls.   Therefore Voltage Control Synthesizers (Analog Synthesizer) could use waveforms   as a source of sound and also as a control of sound. Oscillators producing   these waveforms would include extremely low frequencies, well below the audible   level. The last example in the above table illustrates a sine wave being modulated   by a sine wave. The \"carrier wave\" is the sine wave that is audible. The modulating   sine wave creates the pattern of the sine wave shape (slow curving up and   down pattern). This allows the waveform to create a pattern by using a very   low frequency of an oscillator to modulate the frequency of another oscillator.   This kind of control is known as frequency modulation. Modulation simply means   \"to change.\" Higher modulating frequencies would be used to create vibratos.   Amplitude modulation would increase the range of the modulation (the higher   the amplitude or volume the greater the range of the modulation). Listen to   this wide-range modulation using a square   carrier wave which gradually acquires vibrato and as the amplitude of the   modulating sine wave is increased, achieves a wider and wider oscillation   (vibrato, but the range is so wide we no longer perceive it as vibrato but   rapidly fluctuating sound) and finally returns to the original narrow vibrato.      Copyright by John V. Gilbert    Send feedback or questions to john.gilbert@nyu.edu"}, {"content": "Audio Workstation Production Lab: Tisch School of the Arts at NYU                               Skip to content        Recorded Music Program Courses Internships News &amp; Events Facilities Alumni Faculty Directory Recorded Music FAQs For High School Students Facebook Twitter YouTube Summer In Recorded Music Academic Advisement Resources 194 Recordings  home &gt; Recorded Music        Audio Workstation Production Lab   &lt;&lt; Loading... &gt;&gt;  *Please Note: You are using an older browser or a browser that doesn't support the styles and javascript used in this page. This browser will not display the photo gallery correctly.                       The Pro Tools Production Lab is available for use by students of all levels. The Lab is equipped with 19 digital audio workstations and a teaching position, and two modular analog synthesizers. Students begin to learn the art of music production in the Lab, as well as advanced music production and programming. In addition to the Lab, there are two personal edit suites available for student use. The digital audio workstations in the suites are equipped similarly to the studio\u2019s computers. Production Lab 502 Student workstations (14) \u2022 Intel iMac \u2022 Pro Tools LE \u2022 mBox 2 \u2022 microKorg or Roland SH-201 synthesizers \u2022 Logic Studio \u2022 Reason ...and plugins from Digidesign, Waves, Bomb Factory and more Synth stations (2) \u2022 Dave Smith PolyEvolver analog synthesizer \u2022 MOTM analog modular synthesizer \u2022 Vostok analog modular synthesizer \u2022 Elektron/Machine Drum SPS1 drum machine \u2022 Intel iMac \u2022 Pro Tools LE \u2022 mBox 2 \u2022 Logic Studio 8 \u2022 Reason ...and plugins from Digidesign, Waves, Bomb Factory and more Teacher workstation \u2022 Mac G5 \u2022 Pro Tools HD Accel system \u2022 Langevin DVC preamp/compressor/EQ \u2022 Full integration with student workstations \u2022 Projection capability \u2022 Logic Studio 8 \u2022 Reason ...and plugins from Digidesign, Waves, Bomb Factory and more Edit Suites 1 and 2 Edit Suites (2) Software \u2022 Pro Tools HD \u2022 Logic Studio \u2022 Reason Plugins \u2022 Waves Gold bundle \u2022 Antares AutoTune \u2022 Bomb Factory classic compressors \u2022 Fairchild 660/670 \u2022 Line 6 Amp Farm \u2022 Maxim \u2022 Amplitube \u2022 Pultec Eq bundle \u2022 Sound Replacer ...and more from Digidesign, Waves, Bomb Factory and Trillium Lane Labs   The Clive Davis Institute of Recorded Music 194 Mercer Street, 5th Floor New York, NY 10012               Phone:       212 992 8400 tisch.recorded.music@nyu.edu           Undergraduate Admissions Graduate Admissions Academic Services Student Life Financial Aid and Scholarships Career Development Counseling Services International Students Incoming Students Graduation Forms What is Special Programs? Study Abroad Summer High School Summer in New York City Courses for Non-Majors Programs for Visiting Students Open Arts Minors Blog Newsletters Videos Admin &amp; Faculty Alumni Contact Us  Tisch Events Calendar Admin &amp; Faculty Calendar NYU Academic Deadlines Calendar Many Ways to Give Why Support Tisch Dean's Council Corporate Giving Foundation Giving Individual Giving Tisch Gala Who We Are Institute of Performing Arts Graduate Acting Dance Design for Stage &amp; Film Drama Graduate Musical Theatre Writing Performance Studies Maurice Kanbar Institute of Film &amp; Television Undergraduate Film &amp; Television Graduate Film Cinema Studies Moving Image Archiving &amp; Preservation (MIAP) Rita &amp; Burton Goldberg Department of Dramatic Writing Emerging Media Group Photography &amp; Imaging Interactive Telecommunications (ITP) Clive Davis Institute of Recorded Music NYU Game Center Art &amp; Public Policy / Arts Politics Open Arts Tisch Special Programs Tisch Asia"}, {"content": "Software Synthesizers                                                           Analysis of Four Software Synthesizers.                  I.          INTRODUCTION                The history of   software synthesizers begins in the 1950s with the first known use of a   computer to produce music. It took   place at Bell Laboratories in Murray Park ,    NJ .    The first music programming language called Music I was developed in the   1960s by Max Mathews. Development   continued as computers became both cheaper and more powerful. Then the MIDI   specification was released in 1982.    This provided a standard digital interface and networking protocol for   computers and electronic musical instruments.          The development of MIDI spawned a flurry of   musical software activity. This led   to an explosion of new companies and a new market. Most of the new commercial software   processed MIDI , but not audio. Academic software could also process   audio, but they required their users to be able to know a programming language,   usually C or Lisp. Then in 1994   some software products started integrating MIDI and   audio.  The first   commercial music synthesizer was called Reality and was released in 1995.        There are   currently hundreds of software synthesizers, but most were written after the   Pentium III, G3 and VST 2.0 were released.    The Pentium III and G3 are central processors with improved speed and   hardware acceleration for audio and video processing. This increased speed is critical for the   real-time, digital-signal-processing, software synthesizers. VST 2.0 is a plug-in protocol that was   created by Steinberg Media Technologies GmbH (in 1999, I believe). It allows MIDI   information to be passed to and from plug-ins which is necessary if a software   synthesizer is to function as a plug-in within a host application.          I chose four software synthesizers for this report based on their   popularity, extensive features and their ability to use a number of different   sound synthesis techniques. To give   this report greater breadth I chose two commercial and two academic   synthesizers. Its probably   obvious, but a commercial synthesizer is offered for sale and an academic   synthesizer is free and is usually affiliated with one or more universities or   research centers.              II.         COMMERCIAL SOFTWARE   SYNTHESIZERS                    Reaktor .        Reaktor version   4 is a very flexible software synthesizer thanks to its modular structure. It consists of more than one hundred   modules and function blocks (like macros) which can be connected in an almost   unlimited number of ways. This   flexibility enables users to design and build their own instruments, samplers   (record and playback sounds), effects, sequencers (record and playback musical   notes) and sound design tools. Here   is an example of modules connected to create a sound filter.                 Modules are included for a wide   variety of synthesis and processing techniques. Many of the modules will accept multiple   value ranges and connections. These   will be listed in more detail later in section IV, Synthesizer Comparison and   Contrast. But, the internal   organization is as follows from the Reaktor user manual:                  Top    level is the Ensemble.       An    Ensemble contains Instruments.       An    Instrument contains Macros.       A    Macro contains Modules.    Ensembles and    Instruments can have their own Panels with switches, knobs and faders.    Macros can have a    rectangular frame in the Panel.    (Kurz et al. 109)                 Reason .          Reason version 2.5 is the flagship application for Propellerhead Software   in Sweden . It cant really be considered a modular   synthesizer in the way that Reaktor is, but it can be called a modular sound   studio. This is because it contains   a number of components that are frequently found in a sound studio: a mixer, a subtractive synthesizer, a   granular/wavetable synthesizer, three different kinds of sample players, a      drum machine, eight effect units   and two types of sequencers. The   inputs and outputs of these components can be connected in almost any   order. Depending on your computers   speed, you can also have multiple instances of the different modules running at   the same time.       Reason also   includes an interface technology called ReWire which allows it to integrate   tightly with more full-featured sequencers such as Cakewalks Sonar or   Steinbergs Cubase. When Reason is   integrated with other audio software through ReWire it is able to both send and   receive audio and MIDI information. This greatly expands its   capabilities. For example, Reason's   sounds can be sent to Cubase where they can be altered by Cubases effects or by   any plug-in software that can run inside of Cubase. Going the other way, a guitar track   recorded into Sonar and patched through ReWire into Reason can be further   mutated by Reasons synthesizers, samplers and/or effects.           III.         ACADEMIC SOFTWARE   SYNTHESIZERS                    Csound .          Cooper calls Csound a software synthesizer; in fact it was probably the   first software program to be called that.    However, its really more of a musical computer language or a specialized   library of C language opcode's made for musical use and it can create entire   music compositions from basic audio building blocks. These opcodes are like modules that can   be used in any order or structure to create more complex sound-processing and   sound-generating objects. There   were more than 450 of these Csound        opcodes available in 2000 and there   appears to be a significantly larger number now. (Cooper 41)          Csound started out (about twenty years ago) as a language that had to be   completely scripted and then compiled (converted to binary form so it will run   on a computer). As computers became   faster, and MIDI interfacing opcodes were introduced, it   became possible to control at least some parts of a Csound script in   real-time. So now you can create a   musical instrument in Csound and play it in real time with a    MIDI instrument or some other means of computer   input.          Csound appears to have been created principally by Barry Vercoe at the   Massachusetts Institute of Technology.    He released the source code for Csound to the public and the language has   been expanded by programmers world-wide ever since. It is available for free download and   you can now find many graphic interfaces that make using it much easier than the   old method of writing C language text files              PD .                PD stands for   Pure Data. It is a visual   programming language created by Miller Puckette for making computer music and   graphics. It is in many ways a new   and improved version of his very popular Max/MSP programs. As such, there is much in common between   the two. But one big difference is   that PD is available for many platforms where Max/MSP is only available for the   Macintosh.          According to PDs manual, the main PD data file is called a patch or   canvas. It can consist of any   combination of boxes of which there are four types:  object, message, GUI, and    comment.  (Center) The example below taken from the online   manual gives a good idea of how PD works:            You make    objects by typing text into object boxes. The text is divided into    atoms separated by white space. The first atom specifies what type of   object Pd will make, and the other atoms, called creation arguments , tell   Pd how to initialize the object. If you type for example,            the \"+\" specifies the class    of the object. In this case the object will be the kind that carries out   addition, and the \"13\" initializes the amount to add. Atoms are either numbers   or symbols like \"+\". The text you type into an object box determines how   many and what kinds of inlets and outlets the object will have. Some classes   (like \"+\" always have a fixed arrangement of inlets and outlets, and in the case   of other classes, the inlets and outlets will depend on the creation arguments.         Here for example is a simple MIDI   synthesizer:         (Center)                  As you might expect for a programming language, PD is very deep with   hundreds of objects. New tools and   extended features are being added to it all the time. It is currently available for free   download and use, but since it is at version 0.39 there is the possibility that   once it reaches version 1.0 it will become a commercial product like its   predecessor Max/MSP.          IV.        SYNTHESIZER COMPARISON AND   CONTRAST               System and Sound Compatibility and Requirements                                    Reaktor         Reason         Csound         PD            Operating systems         MacOS 8.6 or higher; Windows 95,98, NT-XP         MacOS X; Windows     98, NT-XP         Macintosh OSX;     Windows 3.1-XP; MS-DOS; Unix; Linux; IRIX         Windows 98-XP;     Linux;     Macintosh OSX;     IRIX             CPUs          G3, G4; Pentium II, III, IV; Athlon         G3, G4; Pentium II, III, IV; Athlon         Theoretically any CPU         G3, G4; Pentium II, III, IV; Athlon; MIPS            DSP chips or cards         Digidesign DAE systems         None         Analog Devices Sharc; Motorola 5600x         None            Audio I/O         DirectX; ASIO         DirectX; ASIO         DirectX; OSS ;     ALSA         ASIO; OSS ;     ALSA            Sound Formats Imported          AIFF; WAV; SD II; Akai S1000, S3000         AIFF; WAV; SoundFonts; REX;          AIFF; WAV; SD II         AIFF; WAV            Sound Formats Exported          AIFF; WAV         Rewire         AIFF; WAV; SD II         AIFF; WAV            Instruments         Users can create         Users cannot create         Users can create         Users can create            Sound   Patches         Open and trade able.         Open and trade able.         Open and trade able.         Open and trade able.         Notes:           DirectX    is an audio driver architecture made by Microsoft to improve Windows    real-time performance       ASIO    is a widely supported audio driver architecture created by Steinberg that    works with Windows and Macintosh.        OSS    is an audio driver architecture that is included in the Linux kernel.       ALSA    (Advanced Linux Sound Architecture) is a newer architecture available from http://www.alsa-project.org/  (Center)       SD    II is short for Sound Designer II, a stereo audio file format by Digidesign.         Akai    S1000 and S3200 refer to the audio file format for those samplers produced by    Akai.          SoundFonts    is an audio file format created by Creative Labs in cooperation with Microsoft    and Roland explicitly for sound sampling.       REX    is an audio file format created by Propellerhead Software for their ReCycle    sample editing program and is supported by other manufacturers.       Rewire    is an audio and midi software interface that allows applications that support    it to communicate with each other in real-time.            Synthesis   Technology                                  Reaktor         Reason         Csound         PD            Subtractive Synthesis         32 types of oscillators         14 types of oscillators         Yes         Yes            Additive Synthesis         Yes with noise         No         Yes         Yes            FM Synthesis         7 types including sampled sounds.         No         Yes         Yes            Sampling         6 types          No, but 4 sample player types         Yes         Yes            Phase Distortion         Yes         Yes         Yes         Yes            Wave Sequencing          Yes         Yes         Yes         Yes                      Reaktor         Reason         Csound         PD            Granular Synthesis         Yes         One grain table instrument         Yes         Yes            Wave-shaping     Synthesis         Yes         Yes         Yes         Yes            Physical Modelling         Simple types only         Not user controllable         Yes         Yes            Vector Synthesis         Yes         Yes         Yes         Yes            Spectral Synthesis         Yes         No         Yes         Yes            Filters         Unlimited Multimode         14 Types, Unlimited Instances         Unlimited Multimode          Unlimited Multimode            Special Features         Pattern sequencers         Pattern and linear sequencers         Sequencing objects; Anyone can program new objects in C;                 Sequencing objects;     Anyone can program new objects in C; Extensive graphics     processing         Notes:            Jeff Pressing in \"Synthesizer Performance and   Real-Time Techniques\" gives this list of approaches to sound synthesis.             subtractive synthesis - filtering of complex   sounds to shape harmonic spectrum, typically starting with geometric waves.               additive synthesis - combining tones, typically   harmonics of varying amplitudes             frequency modulation synthesis - modulating a   carrier wave with one or more operators             sampling - using recorded sounds as sound sources   subject to modification             phase distortion - altering speed of waveforms   stored in wavetables during playback             resynthesis - modification of digitally sampled   sounds before playback             wave sequencing - linear combinations of several   small segments to create a new sound             granular synthesis - combining of several small   sound segments into a new sound             waveshaping - intentional distortion of a signal   to produce a modified result             physical modeling - mathematical equations of   acoustic characteristics of sound             vector synthesis - technique for fading between   any number of different sound sources                            (Pressing)             Spectral Synthesis also known as spectral   modeling, it analyzes the spectral (frequency) content of a sound, breaking it   into many components and then processing and resynthesizing the sound according   to the algorithm used.            Pattern Sequencers also know as step   sequencers, record and play a repeating pattern, usually of less than 32   steps.            Linear Sequencers have no length limit. Can be thought of as endless musical   staff paper.           V.         RECENT SOUND SYNTHESIS   DEVELOPMENTS                 Most   of the synthesis research and development in both academic and commercial   sectors has been focused on various types of physical modeling. This work is primarily divided into two   main areas: analog modeling and   acoustic instrument modeling. Most   of the commercial work has been centered on trying to emulate the   characteristics of various vintage, analog synthesizers. There are many hardware and software   examples of this trend. Often both   the traits of the analog circuits and the sounds they produce are modeled.       Academic   research is mostly focused on modeling acoustic instruments. In many ways this is more challenging   than modeling analog circuitry.    Most acoustic instruments produce a greater variety of noises (sounds   without a single dominant frequency).    There are also many more variables involved in the ways they can be   played. Each of these variables can   have an effect on the way sounds are produced and must be painstakingly modeled   in order to get a realistic simulation.    So while physical modeling itself isnt a recent development, there are   many more physical models available now than there was ten years ago.       Below   are some other recent developments in sound synthesis:          Formant    Filters not a new idea, but has been taken to new lengths in the last ten    years. This technique uses an    array of filters, often dozens at a time, to shape narrow bands of frequencies    over time. This can be very    useful for simulating sounds such as human speech.       Sound    Mapping this involves mapping some kind of non-sound data to frequency, time    or amplitude values to create new sounds. The biggest area of exploration right    now is mapping genetic information usually either chromosomal    representations or genetic algorithms to sound parameters.                  VI.        COMPARISON OF NEW SOUND SYNTHESIS WITH CURRENT   SOFTWARE SYNTHESIZERS                Most   commercial and academic synthesizers today use physical modeling to generate                their   sounds. However the commercial   synthesizers usually dont allow the user to change the values for the physical   models themselves. In these cases   the models are used to give greater realism or detail to the sound which is then   processed with other, less processor-intensive synthesis techniques. Academic synthesizers on the other hand,   do allow the user to change the settings for physical models or even to create   their own physical models. This is   often accomplished with the aid of a math application such as MATLAB.             Formant filters can be built in Reaktor and some other synthesizers but   are not available in most, including Reason. They can be built in PD, Csound and   other academic sound software. The   only commercially available software synthesizer that can do any kind of sound   mapping is Max/MSP. Most academic   sound software already has opcodes that can receive the data used for sound   mapping and can then be programmed to process it as the user desires.               VII.        CONCLUSION             In general,   commercial synthesizers give greater real-time functionality and are more   compatible with other commercial software and a greater variety of sound   formats. However, due to the fact   that their source-code is unavailable to the public, users cannot easily extend   their features or technology to take advantage of new developments. The user must wait for the company that   makes the software to release a new version.        Academic   software is usually open source and if the user knows how to program, can be   quickly customized to take advantage of new theories or developments in sound   synthesis. None of these programs   are as user friendly as most of the commercial products, but that seems to be an   unavoidable trade off. Reaktor is   probably the most flexible and powerful of the commercial synthesizers and it is   also the most difficult to master.    True, with all of its included ensembles and instruments the user can be   playing sounds on it a few seconds after installing it, but it takes a lot more   time to learn how to create your own ensembles. If a user wants to be able to use a   piece of software for rapidly advancing sound research they have little choice   but to use academic software.                       BIBLIOGRAPHY          Works Sited     Center for Research in Computing and the Arts, The    University of    California , San   Diego .    World Wide Web Site.  8 July 2003 .          &lt; http://crca.ucsd.edu/~msp/Pd_documentation/index.htm &gt;        Cooper, Robert L., and Richard Boulanger. Csound, True Virtual Synthesis. Keyboard.     Jan. 1997: 41-48.        Kurz, Michael, et al. Reaktor Version 3 Manual.    Berlin : Native Instruments Software    Synthesis GmbH. 2001.        Native Instruments Software Synthesis, GmbH. World Wide Web Site.  8 July 2003 .          &lt; http://www.native-instruments.com/ &gt;.        Pressing, Jeff.    Synthesizer Performance and   Real-time Techniques.    Middleton ,    WI :             A-R Editions, Inc. 1992.            Works Also Consulted          Boulanger, Richard.    The Csound Book: Perspectives in Software Synthesis, Sound         Design, Signal Processing, and   Programming .    Cambridge ,    MA :    MIT Press, 2000.        Cook, Perry R.    Real Sound Synthesis for Interactive Applications .  Natick ,    MA :    A K    Peters Ltd, 2002.        Garton, Brad.    Director of the Computer    Music    Center and Professor of Music at      Columbia    University . Personal interview.        Guttman, Newman, et al.    Computer Music Currents 13, The   Historical CD of Digital      Sound Synthesis 1995.        Miranda, Eduardo.    Computer Sound Design: Synthesis Techniques and Programming .      Burlington ,    MA :    Focal Press, 2002.        Roads, Curtis.  Microsound .    Cambridge ,    MA :    MIT Press, 2002.    Rowe, Robert.  Machine Musicianship . Cambridge, MA: MIT Press, 2001.    Serra, Xavier, et al.    Integrating complementary spectral models in the design of a    musical synthesizer.  Proceedings of the International Computer   Music Conference 1997 .        Schmitt, Stefan.    One of the founders of Native Instruments and co-authored the original         version of Reaktor. Personal interview.          Solenberg, Fredrik.    Software developer for Propellerhead Software. Email interview.        Tanner, Ted. Architectural Strategist for Microsoft   Corporation. Email   interview."}]}]