[{"detail": [{"content": "// Derived from an example in the Boogie distribution  class IntSet {  ghost var Contents: set ;  ghost var Repr: set ;   var root: TreeNode;   predicate Valid  reads this, Repr;  {  this in Repr &&  (root == null ==> Contents == {}) &&  (root != null ==>   root in Repr && root.Repr x in Contents;  {  if (root == null) {   present := false;  } else {   present := root.Find(x);  }  }   method Insert(x: int)  requires Valid;  modifies Repr;  ensures Valid && fresh(Repr - old(Repr));  ensures Contents == old(Contents) + {x};  {   if (root == null) {   root := new TreeNode.Init(x);  } else {   root.Insert(x);  }  Contents := root.Contents;  Repr := root.Repr + {this};  }   method Remove(x: int)  requires Valid;  modifies Repr;  ensures Valid && fresh(Repr - old(Repr));  ensures Contents == old(Contents) - {x};  {  if (root != null) {   var newRoot := root.Remove(x);   root := newRoot;   if (root == null) {   Contents := {};   Repr := {this};   } else {   Contents := root.Contents;   Repr := root.Repr + {this};   }  }  } }  class TreeNode {  ghost var Contents: set ;  ghost var Repr: set ;   var data: int;  var left: TreeNode;  var right: TreeNode;   predicate Valid  reads this, Repr;  {  this in Repr &&  null !in Repr &&  (left != null ==>   left in Repr &&   left.Repr y    right in Repr &&   right.Repr data left.Repr !! right.Repr) &&  Contents == (if left == null then {} else left.Contents) +     (if right == null then {} else right.Contents) +     {data}  }   constructor Init(x: int)  modifies this;  ensures Valid && fresh(Repr - {this});  ensures Contents == {x};  {  data := x;  left := null;  right := null;  Contents := {x};  Repr := {this};  }   method Insert(x: int)  requires Valid;  modifies Repr;  ensures Valid;  ensures Contents == old(Contents) + {x};  ensures fresh(Repr - old(Repr));  decreases Repr;  {  if (x == data) {   return;  }   if (x x in Contents;  decreases Repr;  {  if (x == data) {   present := true;  } else if (left != null && x TreeNode.Valid;  ensures TreeNode == null ==> old(Contents) TreeNode.Repr TreeNode.Valid;  ensures TreeNode == null ==> old(Contents) == {min};  ensures TreeNode != null ==> TreeNode.Repr min x == 12 || x == 24;  }   method Client1(s: IntSet, x: int)  requires s != null && s.Valid;  modifies s.Repr;  {  s.Insert(x);  s.Insert(24);  assert old(s.Contents) - {x,24} == s.Contents - {x,24};  } }"}]},
{"detail": [{"content": "// Derived from an example in the Boogie distribution  class IntSet {  ghost var Contents: set ;  ghost var Repr: set ;   var root: TreeNode;   predicate Valid  reads this, Repr;  {  this in Repr &&  (root == null ==> Contents == {}) &&  (root != null ==>   root in Repr && root.Repr x in Contents;  {  if (root == null) {   present := false;  } else {   present := root.Find(x);  }  }   method Insert(x: int)  requires Valid;  modifies Repr;  ensures Valid && fresh(Repr - old(Repr));  ensures Contents == old(Contents) + {x};  {   if (root == null) {   root := new TreeNode.Init(x);  } else {   root.Insert(x);  }  Contents := root.Contents;  Repr := root.Repr + {this};  }   method Remove(x: int)  requires Valid;  modifies Repr;  ensures Valid && fresh(Repr - old(Repr));  ensures Contents == old(Contents) - {x};  {  if (root != null) {   var newRoot := root.Remove(x);   root := newRoot;   if (root == null) {   Contents := {};   Repr := {this};   } else {   Contents := root.Contents;   Repr := root.Repr + {this};   }  }  } }  class TreeNode {  ghost var Contents: set ;  ghost var Repr: set ;   var data: int;  var left: TreeNode;  var right: TreeNode;   predicate Valid  reads this, Repr;  {  this in Repr &&  null !in Repr &&  (left != null ==>   left in Repr &&   left.Repr y    right in Repr &&   right.Repr data left.Repr !! right.Repr) &&  Contents == (if left == null then {} else left.Contents) +     (if right == null then {} else right.Contents) +     {data}  }   constructor Init(x: int)  modifies this;  ensures Valid && fresh(Repr - {this});  ensures Contents == {x};  {  data := x;  left := null;  right := null;  Contents := {x};  Repr := {this};  }   method Insert(x: int)  requires Valid;  modifies Repr;  ensures Valid;  ensures Contents == old(Contents) + {x};  ensures fresh(Repr - old(Repr));  decreases Repr;  {  if (x == data) {   return;  }   if (x x in Contents;  decreases Repr;  {  if (x == data) {   present := true;  } else if (left != null && x TreeNode.Valid;  ensures TreeNode == null ==> old(Contents) TreeNode.Repr TreeNode.Valid;  ensures TreeNode == null ==> old(Contents) == {min};  ensures TreeNode != null ==> TreeNode.Repr min x == 12 || x == 24;  }   method Client1(s: IntSet, x: int)  requires s != null && s.Valid;  modifies s.Repr;  {  s.Insert(x);  s.Insert(24);  assert old(s.Contents) - {x,24} == s.Contents - {x,24};  } }"}, {"content": "NYU Depth V1 &laquo; Nathan Silberman               Nathan Silberman                  NYU Depth V1   Nathan Silberman , Rob Fergus  Indoor Scene Segmentation using a Structured Light Sensor  ICCV 2011 Workshop on 3D Representation and Recognition\u00a0 [PDF] [Bib]        Samples of the RGB image, the raw depth image, and the class labels from the dataset.       Overview   The NYU-Depth data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect . The dataset has several components:     Labeled: A subset of the video data accompanied by dense multi-class labels. This data has also been preprocessed to fill in missing depth labels.   Raw: The raw rgb, depth and accelerometer data as provided by the Kinect.   Toolbox: Useful functions for manipulating the data and labels.   The train/test splits used for evaluation.     Downloads     Labeled dataset (~4 GB)   Raw dataset (~90 GB)   Toolbox   Original raw RGB and Depth filenames for each of the labeled images   Train/test splits for multi-class segmentation   Train/test splits for classification        Labeled Dataset     Output from the RGB camera (left), preprocessed depth (center) and a set of labels (right) for the image.   The labeled dataset is a subset of the Raw Dataset. It is comprised of pairs of RGB and Depth frames that have been synchronized and annotated with dense labels for every image. In addition to the projected depth maps, we have included a set of preprocessed depth maps whose missing values have been filled in using the colorization scheme of Levin et al . Unlike, the Raw dataset, the labeled dataset is provided as a Matlab .mat file with the following variables:     accelData &#8211; Nx4 matrix of accelerometer values indicated when each frame was taken. The columns contain the roll, yaw, pitch and tilt angle of the device.   depths &#8211; HxWxN matrix of depth maps where H and W are the height and width, respectively and N is the number of images. The values of the depth elements are in meters.   images &#8211; HxWx3xN matrix of RGB images where H and W are the height and width, respectively, and N is the number of images.   labels &#8211; HxWxN matrix of label masks where H and W are the height and width, respectively and N is the number of images. The labels range from 1..C where C is the total number of classes. If a pixel&#8217;s label value is 0, then that pixel is &#8216;unlabeled&#8217;.   names &#8211; Cx1 cell array of the english names of each class.   namesToIds &#8211; map from english label names to IDs (with C key-value pairs)   rawDepths &#8211; HxWxN matrix of depth maps where H and W are the height and width, respectively, and N is the number of images. These depth maps are the raw output from the kinect.   scenes &#8211; Cx1 cell array of the name of the scene from which each image was taken.     Raw Dataset      Output from the RGB camera (left) and depth camera (right). Missing values in the depth image are a result of (a) shadows caused by the disparity between the infrared emitter and camera, (b) random missing or spurious values caused by specular or low albedo surfaces or (c) the projection of the depth coordinate frame.       The raw dataset contains the raw image and accelerometer dumps from the kinect. The RGB and Depth camera sampling rate lies between 20 and 30 FPS (variable over time). While the frames are not synchronized, the timestamps for each of the RGB, depth and accelerometer files are included as part of each filename.   The dataset is divided into different folders which correspond to each &#8217;scene&#8217; being filmed, such as &#8216;living_room_0012&#8242; or &#8216;office_0014&#8242;. The file hierarchy is structured as follows:   / ../bedroom_0001/ ../bedroom_0001/a-1294886363.011060-3164794231.dump ../bedroom_0001/a-1294886363.016801-3164794231.dump      ... ../bedroom_0001/d-1294886362.665769-3143255701.pgm ../bedroom_0001/d-1294886362.793814-3151264321.pgm      ... ../bedroom_0001/r-1294886362.238178-3118787619.ppm ../bedroom_0001/r-1294886362.814111-3152792506.ppm   Files that begin with the prefix a- are the accelerometer dumps. These dumps are written to disk in binary and can be read with file get_accel_data.mex. Files that begin with the prefix r- and d- are the frames from the RGB and depth cameras, respectively. Since no preprocessing has been performed, the raw depth images must be projected onto the RGB coordinate space into order to align the images.   Toolbox   The matlab toolbox has several useful functions for handling the data. To run the unit tests, you must have matlab&#8217;s xUnit on your path.     demo_synched_projected_frames.m &#8211; Demos synchronization of the raw rgb and depth images as well as alignment of the rgb and raw depth.   eval_seg.m &#8211; Evaluates the predicted segmentation against the ground truth label map.   get_projected_depth.m &#8211; Projects the raw depth image onto the rgb image plane. This file contains the calibration parameters that are specific to the kinect used to gather the data.   get_accel_data.m &#8211; Extracts the accelerometer data from the binary files in the raw dataset.   get_projection_mask.m &#8211; Gets a mask for the images that crops areas whose depth had to be inferred, rather than directly projected from the Kinect signal.   get_synched_frames.m &#8211; Returns a list of synchronized rgb and depth frames from a given scene in the raw dataset.   get_train_test_split.m &#8211; Splits the data in a way that ensures that images from the same scene are not found in both train and test sets.                               Pages: Home    Publications    Datasets    Code    About"}]},
{"detail": [{"content": "// Derived from an example in the Boogie distribution  class IntSet {  ghost var Contents: set ;  ghost var Repr: set ;   var root: TreeNode;   predicate Valid  reads this, Repr;  {  this in Repr &&  (root == null ==> Contents == {}) &&  (root != null ==>   root in Repr && root.Repr x in Contents;  {  if (root == null) {   present := false;  } else {   present := root.Find(x);  }  }   method Insert(x: int)  requires Valid;  modifies Repr;  ensures Valid && fresh(Repr - old(Repr));  ensures Contents == old(Contents) + {x};  {   if (root == null) {   root := new TreeNode.Init(x);  } else {   root.Insert(x);  }  Contents := root.Contents;  Repr := root.Repr + {this};  }   method Remove(x: int)  requires Valid;  modifies Repr;  ensures Valid && fresh(Repr - old(Repr));  ensures Contents == old(Contents) - {x};  {  if (root != null) {   var newRoot := root.Remove(x);   root := newRoot;   if (root == null) {   Contents := {};   Repr := {this};   } else {   Contents := root.Contents;   Repr := root.Repr + {this};   }  }  } }  class TreeNode {  ghost var Contents: set ;  ghost var Repr: set ;   var data: int;  var left: TreeNode;  var right: TreeNode;   predicate Valid  reads this, Repr;  {  this in Repr &&  null !in Repr &&  (left != null ==>   left in Repr &&   left.Repr y    right in Repr &&   right.Repr data left.Repr !! right.Repr) &&  Contents == (if left == null then {} else left.Contents) +     (if right == null then {} else right.Contents) +     {data}  }   constructor Init(x: int)  modifies this;  ensures Valid && fresh(Repr - {this});  ensures Contents == {x};  {  data := x;  left := null;  right := null;  Contents := {x};  Repr := {this};  }   method Insert(x: int)  requires Valid;  modifies Repr;  ensures Valid;  ensures Contents == old(Contents) + {x};  ensures fresh(Repr - old(Repr));  decreases Repr;  {  if (x == data) {   return;  }   if (x x in Contents;  decreases Repr;  {  if (x == data) {   present := true;  } else if (left != null && x TreeNode.Valid;  ensures TreeNode == null ==> old(Contents) TreeNode.Repr TreeNode.Valid;  ensures TreeNode == null ==> old(Contents) == {min};  ensures TreeNode != null ==> TreeNode.Repr min x == 12 || x == 24;  }   method Client1(s: IntSet, x: int)  requires s != null && s.Valid;  modifies s.Repr;  {  s.Insert(x);  s.Insert(24);  assert old(s.Contents) - {x,24} == s.Contents - {x,24};  } }"}, {"content": "NYU Depth V1 &laquo; Nathan Silberman               Nathan Silberman                  NYU Depth V1   Nathan Silberman , Rob Fergus  Indoor Scene Segmentation using a Structured Light Sensor  ICCV 2011 Workshop on 3D Representation and Recognition\u00a0 [PDF] [Bib]        Samples of the RGB image, the raw depth image, and the class labels from the dataset.       Overview   The NYU-Depth data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect . The dataset has several components:     Labeled: A subset of the video data accompanied by dense multi-class labels. This data has also been preprocessed to fill in missing depth labels.   Raw: The raw rgb, depth and accelerometer data as provided by the Kinect.   Toolbox: Useful functions for manipulating the data and labels.   The train/test splits used for evaluation.     Downloads     Labeled dataset (~4 GB)   Raw dataset (~90 GB)   Toolbox   Original raw RGB and Depth filenames for each of the labeled images   Train/test splits for multi-class segmentation   Train/test splits for classification        Labeled Dataset     Output from the RGB camera (left), preprocessed depth (center) and a set of labels (right) for the image.   The labeled dataset is a subset of the Raw Dataset. It is comprised of pairs of RGB and Depth frames that have been synchronized and annotated with dense labels for every image. In addition to the projected depth maps, we have included a set of preprocessed depth maps whose missing values have been filled in using the colorization scheme of Levin et al . Unlike, the Raw dataset, the labeled dataset is provided as a Matlab .mat file with the following variables:     accelData &#8211; Nx4 matrix of accelerometer values indicated when each frame was taken. The columns contain the roll, yaw, pitch and tilt angle of the device.   depths &#8211; HxWxN matrix of depth maps where H and W are the height and width, respectively and N is the number of images. The values of the depth elements are in meters.   images &#8211; HxWx3xN matrix of RGB images where H and W are the height and width, respectively, and N is the number of images.   labels &#8211; HxWxN matrix of label masks where H and W are the height and width, respectively and N is the number of images. The labels range from 1..C where C is the total number of classes. If a pixel&#8217;s label value is 0, then that pixel is &#8216;unlabeled&#8217;.   names &#8211; Cx1 cell array of the english names of each class.   namesToIds &#8211; map from english label names to IDs (with C key-value pairs)   rawDepths &#8211; HxWxN matrix of depth maps where H and W are the height and width, respectively, and N is the number of images. These depth maps are the raw output from the kinect.   scenes &#8211; Cx1 cell array of the name of the scene from which each image was taken.     Raw Dataset      Output from the RGB camera (left) and depth camera (right). Missing values in the depth image are a result of (a) shadows caused by the disparity between the infrared emitter and camera, (b) random missing or spurious values caused by specular or low albedo surfaces or (c) the projection of the depth coordinate frame.       The raw dataset contains the raw image and accelerometer dumps from the kinect. The RGB and Depth camera sampling rate lies between 20 and 30 FPS (variable over time). While the frames are not synchronized, the timestamps for each of the RGB, depth and accelerometer files are included as part of each filename.   The dataset is divided into different folders which correspond to each &#8217;scene&#8217; being filmed, such as &#8216;living_room_0012&#8242; or &#8216;office_0014&#8242;. The file hierarchy is structured as follows:   / ../bedroom_0001/ ../bedroom_0001/a-1294886363.011060-3164794231.dump ../bedroom_0001/a-1294886363.016801-3164794231.dump      ... ../bedroom_0001/d-1294886362.665769-3143255701.pgm ../bedroom_0001/d-1294886362.793814-3151264321.pgm      ... ../bedroom_0001/r-1294886362.238178-3118787619.ppm ../bedroom_0001/r-1294886362.814111-3152792506.ppm   Files that begin with the prefix a- are the accelerometer dumps. These dumps are written to disk in binary and can be read with file get_accel_data.mex. Files that begin with the prefix r- and d- are the frames from the RGB and depth cameras, respectively. Since no preprocessing has been performed, the raw depth images must be projected onto the RGB coordinate space into order to align the images.   Toolbox   The matlab toolbox has several useful functions for handling the data. To run the unit tests, you must have matlab&#8217;s xUnit on your path.     demo_synched_projected_frames.m &#8211; Demos synchronization of the raw rgb and depth images as well as alignment of the rgb and raw depth.   eval_seg.m &#8211; Evaluates the predicted segmentation against the ground truth label map.   get_projected_depth.m &#8211; Projects the raw depth image onto the rgb image plane. This file contains the calibration parameters that are specific to the kinect used to gather the data.   get_accel_data.m &#8211; Extracts the accelerometer data from the binary files in the raw dataset.   get_projection_mask.m &#8211; Gets a mask for the images that crops areas whose depth had to be inferred, rather than directly projected from the Kinect signal.   get_synched_frames.m &#8211; Returns a list of synchronized rgb and depth frames from a given scene in the raw dataset.   get_train_test_split.m &#8211; Splits the data in a way that ensures that images from the same scene are not found in both train and test sets.                               Pages: Home    Publications    Datasets    Code    About"}, {"content": "ExamReporter :: Login                                                NYU School of Law                                     Tools and Resources:                              Home        NYU Law        NYUHome                                                                                  Exam Report Navigation Links:                         About       Admissions       Academics       Faculty       Students       Centers       LL.M. &amp; J.S.D.       Global       Alumni                                                              Exam Reporter                                              Related                                         Exams                              Laptop Requirements             NYU Law ITS           NYU Law Academic Services                                                                                                                                    ExamReporter                 &#160; Login     NetID:       Password:       &#160;                                                   General Information:                            Home                                         &copy; 2015 NYU School of Law Helpdesk, 40 Washington Square South, NY, NY 10012"}]}]