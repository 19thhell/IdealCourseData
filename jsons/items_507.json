[{"detail": [{"content": "Foundations of Machine Learning -- CSCI-GA.2566-001             Fall 2014    Foundations of Machine Learning      Course#: CSCI-GA.2566-001    Instructor: Mehryar Mohri   Graders/TAs:   Vitaly Kuznetsov ,  Andres Munoz Medina ,  Scott Yang .     Mailing List         Course Description       This course introduces the fundamental concepts and methods of machine learning, including the description and analysis of several modern algorithms, their theoretical basis, and the illustration of their applications. Many of the algorithms described have been successfully used in text and speech processing, bioinformatics, and other areas in real-world products and services. The main topics covered are:      Probability tools, concentration inequalities  PAC model  Rademacher complexity, growth function, VC-dimension  Perceptron, Winnow  Support vector machines (SVMs)  Kernel methods  Decision trees  Boosting  Density estimation, maximum entropy models  Logistic regression  Regression problems and algorithms  Ranking problems and algorithms  Halving algorithm, weighted majority algorithm, mistake bounds  Learning automata and transducers  Reinforcement learning, Markov decision processes (MDPs)     It is strongly recommended to those who can to also attend the  Machine Learning Seminar .        Location and Time      Warren Weaver Hall Room 109,  251 Mercer Street.  Mondays 5:10 PM - 7:00 PM.        Prerequisite     Familiarity with basics in linear algebra, probability, and analysis of algorithms.       Projects and Assignments     There will be 3 to 4 assignments and a project. The final grade is essentially the average of the assignment and project grades. The standard high level of integrity  is expected from all students, as with all CS courses.       Lectures        Lecture 00 : Introduction to convex optimization.   Lecture 01 : Introduction to machine learning, basic definitions, probability tools.   Lecture 02 : PAC model, guarantees for learning with finite hypothesis sets.   Lecture 03 : Rademacher complexity, growth function, VC-dimension, learning bounds for infinite hypothesis sets.   Lecture 04 : Support vector machines (SVMs), margin bounds.   Lecture 05 : Kernel methods.   Lecture 06 : Boosting.   Lecture 07 : Density estimation, Maxent models, multinomial logistic regression.   Lecture 08 : On-line learning.   Lecture 09 : Ranking.   Lecture 10 : Multi-class classification.   Lecture 11 : Regression.   Lecture 12 : Reinforcement learning.   Lecture 13: Learning languages.            Textbook    The following is the required textbook for the class. It covers all the material presented (and a lot more):      Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.  Foundations of Machine Learning . MIT Press, 2012.          Technical Papers    An extensive list of recommended papers for further reading is provided in the lecture slides.        Homework         Homework 1 [ solution ]  Homework 2 [ solution ]  Homework 3 [ solution ]  Projects           Previous years         2005   2006   2007   2008   2009   2010   2011   2012   2013   2014 (Spring)"}]},
{"detail": [{"content": "Foundations of Machine Learning -- CSCI-GA.2566-001             Fall 2014    Foundations of Machine Learning      Course#: CSCI-GA.2566-001    Instructor: Mehryar Mohri   Graders/TAs:   Vitaly Kuznetsov ,  Andres Munoz Medina ,  Scott Yang .     Mailing List         Course Description       This course introduces the fundamental concepts and methods of machine learning, including the description and analysis of several modern algorithms, their theoretical basis, and the illustration of their applications. Many of the algorithms described have been successfully used in text and speech processing, bioinformatics, and other areas in real-world products and services. The main topics covered are:      Probability tools, concentration inequalities  PAC model  Rademacher complexity, growth function, VC-dimension  Perceptron, Winnow  Support vector machines (SVMs)  Kernel methods  Decision trees  Boosting  Density estimation, maximum entropy models  Logistic regression  Regression problems and algorithms  Ranking problems and algorithms  Halving algorithm, weighted majority algorithm, mistake bounds  Learning automata and transducers  Reinforcement learning, Markov decision processes (MDPs)     It is strongly recommended to those who can to also attend the  Machine Learning Seminar .        Location and Time      Warren Weaver Hall Room 109,  251 Mercer Street.  Mondays 5:10 PM - 7:00 PM.        Prerequisite     Familiarity with basics in linear algebra, probability, and analysis of algorithms.       Projects and Assignments     There will be 3 to 4 assignments and a project. The final grade is essentially the average of the assignment and project grades. The standard high level of integrity  is expected from all students, as with all CS courses.       Lectures        Lecture 00 : Introduction to convex optimization.   Lecture 01 : Introduction to machine learning, basic definitions, probability tools.   Lecture 02 : PAC model, guarantees for learning with finite hypothesis sets.   Lecture 03 : Rademacher complexity, growth function, VC-dimension, learning bounds for infinite hypothesis sets.   Lecture 04 : Support vector machines (SVMs), margin bounds.   Lecture 05 : Kernel methods.   Lecture 06 : Boosting.   Lecture 07 : Density estimation, Maxent models, multinomial logistic regression.   Lecture 08 : On-line learning.   Lecture 09 : Ranking.   Lecture 10 : Multi-class classification.   Lecture 11 : Regression.   Lecture 12 : Reinforcement learning.   Lecture 13: Learning languages.            Textbook    The following is the required textbook for the class. It covers all the material presented (and a lot more):      Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.  Foundations of Machine Learning . MIT Press, 2012.          Technical Papers    An extensive list of recommended papers for further reading is provided in the lecture slides.        Homework         Homework 1 [ solution ]  Homework 2 [ solution ]  Homework 3 [ solution ]  Projects           Previous years         2005   2006   2007   2008   2009   2010   2011   2012   2013   2014 (Spring)"}, {"content": "Machine Learning and Computational Statistics, Spring 2014    Machine Learning and Computational Statistics        DS-GA-1003 and CSCI-GA.2567,    Spring 2014                    Overview      Machine learning is an exciting and fast-moving field at     the intersection of computer science, statistics, and     optimization with many recent consumer          applications (e.g., Microsoft Kinect, Google Translate,     Iphone's Siri, digital camera face detection, Netflix     recommendations, Google news). Machine learning and     computational statistics also play a central role in data     science. In this graduate-level class, students will learn     about the theoretical foundations of machine learning and     computational statistics and how to apply these to solve     new problems. This is a required course for the MS in Data     Science and should be taken in the first year of study; it     is also suitable for MS and Ph.D. students in Computer     Science and related fields (see pre-requisites below).           This course is part of a two course series, although it     can be taken individually. The second course, DS-GA-1005:      Inference and Representation , will be offered in     Fall 2014 (see an approximate     list of topics).           For registration information, please contact Varsha      Tiger &lt;varsha.tiger@nyu.edu&gt; or Katie Laugel      &lt;laugel@cs.nyu.edu&gt;.                                General information      Lecture : Tuesdays, 5:10-7pm, in Warren     Weaver Hall 109.      Recitation/Laboratory (required for all students):     Thursdays, 8:10-9pm in Warren     Weaver Hall 109.                                   Instructor :                      Prof. David Sontag                       dsontag {@ | at} cs.nyu.edu          Office hours: Tuesdays, 10-11am in 715 Broadway, 12th floor, Room 1204. [still subject to change]              Lab instructor:                             Yoni       Halpern                             halpern {@ | at} cs.nyu.edu                 Office hours: Thursdays, 9:10-9:40pm in Warren Weaver Hall 109 (right after lab)              Project advisers:                             Dr.        David Rosenberg        Dr.       Kurt Miller        Dr.        Alex Simma                                                       Graders:                             Akshay Kumar, Mick Jermsurawong                              akshaykumar, jj1192 {@ | at} nyu.edu                            Pre-requisites: There are two     different sets of pre-requisites to accommodate both     Computer Science and Data Science MS students. Students     are required to have taken either:           Fundamental Algorithms (CSCI-GA.1170) and      Mathematical Techniques for Computer Science     Applications (CSCI-GA.1180), or      Intro to Data Science (DS-GA-1001) and       Statistical and Mathematical Methods (DS-GA-1002).                Students should be familiar with linear algebra,     probability and statistics, and multi-variable calculus,     in addition to having good programming skills.           Grading : problem sets (45%) + midterm     exam (25%) + project (25%) + participation (5%). Problem Set policy                     Books : No textbook is required     (readings will come from freely available online     material). If an additional reference is desired, a good     option is the following book by Kevin Murphy: Machine     Learning: a Probabilistic Perspective (2012). A good reference on linear algebra and probability is Ernest Davis's Linear Algebra and Probability for Computer Science Applications .        Mailing list: To subscribe to the class list,     follow instructions here .          Project      information                   Schedule               Lecture      Date      Topic      Required reading      Assignments             1     Jan 28         Introduction to learning [ Slides ]       Chapter 1 of Murphy's book     Notes     on perceptron mistake bound (just section 1)         ps1 ( data ) due Feb 6 at 8pm.                 2     Feb 4         Support vector machines (SVMs) [ Slides ]          Notes     on support vector machines   Optional : Second reference on SVM dual and kernel methods (sec. 3-8)   Optional : For more on SVMs, see Hastie, Sections 12.1-12.3 (pg. 435). For more on cross-validation see Hastie, Section 7.10 (pg. 250).        ps2 due Feb 14 at 5pm. [ Solutions ]             3     Feb 11         Kernel methods [ Slides ] Optimization, Mercer's theorem     Notes on linear algebra, convexity, kernels, and Mercer's theorem      Optional: For more advanced kernel methods, see    chapter 3 of this book     (free online from NYU libraries)      ps3 ( data ) due Feb 25 at 3pm.             4     Feb 18         Learning theory [ Slides ]     Notes     on learning theory   Notes     on gap-tolerant classifiers (section 7.1, pg. 29-31)  Pedro Domingos's A     Few Useful Things to Know About Machine Learning                     5     Feb 25         Decision trees [ Slides ] Ensemble methods, Random forests       Mitchell Ch. 3    Hastie et al ., Section 8.7 (bagging)      Optional: Rudin's lecture notes (on decision trees)      Optional: Hastie et al. Chapter 15 (on random    forests)      ps4 ( data ) due Mar 7 at 5pm.             6     March 4         Midterm review Lab: deep learning (guest lecture by Yann LeCun)                       7     March 11      (no class, office hours, or lab March 18/20, Spring break)          Midterm exam  Lab: project advisers          Project proposal , due March 27 at 3pm.             8     March 25         Clustering [ Slides ] K-means, hierarchical, spectral      Hastie et al. , Sections 14.3.6,    14.3.8, 14.3.9, 14.3.12, 14.5.3           Optional: Tutorial     on spectral clustering                    9     April 1         Dimensionality reduction [ Slides ]       Notes      on PCA      More      notes on PCA           Optional: Barber, Chapter 15      Optional: Roweis and Saul, Science 2000 ,      Tenenbaum et al., Science 2000 ,     van der Maaten and Hinton, JMLR '08         ps5 ( data ) due Apr 15 at 3pm.             10     April 8         Bayesian methods [ Slides ] Maximum likelihood estimation, naive Bayes     Notes     on naive Bayes and logistic regression   Optional: Notes on probability and statistics                       11     April 15         Graphical models [ Slides ]          Tutorial     on HMMs      Introduction     to Bayesian networks       ps6 due Apr 28 at 5pm             12     April 22         Unsupervised learning [ Slides ]       Notes     on mixture models                    13     April 29         EM algorithm [ Slides 1 , Slides 2 ] Mixture models, topic models, latent Dirichlet allocation        Notes     on Expectation Maximization      The Expectation     Maximization Algorithm: A short tutorial   Review     article on topic modeling  Explore topic models of: state-of-the-union addresses , literary studies (see also this blog ), evolution of science , Wikipedia                   14     May 6      (no class Tuesday May 13)         Advanced topics       Optional:   Introduction to learning to rank  Joachims' Training Linear SVMs in Linear Time   Slides on collaborative filtering   Slides on victim identification using Bayesian networks ( Video )                  15     Thu. May 15, 7:10-9:40pm          Project presentations (WWH 13th floor)                           Acknowledgements : Many thanks to the University of  Washington, Carnegie Mellon University, UT Dallas, Stanford, UC  Irvine, Princeton, and MIT for sharing material used in slides and  homeworks.                          Reference materials           Machine learning books                  Trevor Hastie, Rob Tibshirani, and Jerry Friedman, Elements of      Statistical Learning , Second Edition, Springer,      2009. (Can be downloaded as PDF file.)       David Barber,            Bayesian                                       Reasoning and Machine Learning ,            Cambridge University Press, 2012. (Can be downloaded      as PDF file.)                 Probability            Chapter 2 of either Murphy or Bishop (see also      Bishop Appendix B)       Review                                        notes from Stanford's machine learning class       Sam Roweis's probability                                        review                 Linear algebra            Bishop Appendix C       Online                                        class from MIT       Review                                        notes from Stanford's machine learning class       Sam Roweis's linear                                        algebra review                 Calculus          Online calculus textbook (MIT Open Courseware)       Bishop Appendix D and E (Lagrange multipliers)       Notes       from MIT on Lagrange multipliers       Dan Klein's Lagrange                                        Multipliers without Permanent Scarring           Optimization                        Convex                                        Optimization by Stephen Boyd and Lieven      Vandenberghe. (Can be downloaded as PDF file.)                                  Problem       Set policy            I expect you to try solving each problem set on your own.    However, when being stuck on a problem, I encourage     you to collaborate with other students in the class, subject    to the following rules:               You may discuss a problem with any student in this     class, and work together on solving it. This can involve     brainstorming and verbally discussing the problem, going     together through possible solutions, but should not      involve one student telling another a complete solution.                Once you solve the homework, you must write up      your solutions on your own , without looking at     other people's write-ups or giving your write-up to     others.                In your solution for each problem, you must write      down the names of any person with whom you     discussed it. This will not affect your grade.                Do not consult solution manuals or other people's     solutions from similar courses.           Late submission policy : During the    semester you are allowed at most two extensions on the    homework assignment. Each extension is for at most 48 hours    and carries a penalty of 25% off your assignment."}]},
{"detail": [{"content": "Foundations of Machine Learning -- CSCI-GA.2566-001             Fall 2014    Foundations of Machine Learning      Course#: CSCI-GA.2566-001    Instructor: Mehryar Mohri   Graders/TAs:   Vitaly Kuznetsov ,  Andres Munoz Medina ,  Scott Yang .     Mailing List         Course Description       This course introduces the fundamental concepts and methods of machine learning, including the description and analysis of several modern algorithms, their theoretical basis, and the illustration of their applications. Many of the algorithms described have been successfully used in text and speech processing, bioinformatics, and other areas in real-world products and services. The main topics covered are:      Probability tools, concentration inequalities  PAC model  Rademacher complexity, growth function, VC-dimension  Perceptron, Winnow  Support vector machines (SVMs)  Kernel methods  Decision trees  Boosting  Density estimation, maximum entropy models  Logistic regression  Regression problems and algorithms  Ranking problems and algorithms  Halving algorithm, weighted majority algorithm, mistake bounds  Learning automata and transducers  Reinforcement learning, Markov decision processes (MDPs)     It is strongly recommended to those who can to also attend the  Machine Learning Seminar .        Location and Time      Warren Weaver Hall Room 109,  251 Mercer Street.  Mondays 5:10 PM - 7:00 PM.        Prerequisite     Familiarity with basics in linear algebra, probability, and analysis of algorithms.       Projects and Assignments     There will be 3 to 4 assignments and a project. The final grade is essentially the average of the assignment and project grades. The standard high level of integrity  is expected from all students, as with all CS courses.       Lectures        Lecture 00 : Introduction to convex optimization.   Lecture 01 : Introduction to machine learning, basic definitions, probability tools.   Lecture 02 : PAC model, guarantees for learning with finite hypothesis sets.   Lecture 03 : Rademacher complexity, growth function, VC-dimension, learning bounds for infinite hypothesis sets.   Lecture 04 : Support vector machines (SVMs), margin bounds.   Lecture 05 : Kernel methods.   Lecture 06 : Boosting.   Lecture 07 : Density estimation, Maxent models, multinomial logistic regression.   Lecture 08 : On-line learning.   Lecture 09 : Ranking.   Lecture 10 : Multi-class classification.   Lecture 11 : Regression.   Lecture 12 : Reinforcement learning.   Lecture 13: Learning languages.            Textbook    The following is the required textbook for the class. It covers all the material presented (and a lot more):      Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.  Foundations of Machine Learning . MIT Press, 2012.          Technical Papers    An extensive list of recommended papers for further reading is provided in the lecture slides.        Homework         Homework 1 [ solution ]  Homework 2 [ solution ]  Homework 3 [ solution ]  Projects           Previous years         2005   2006   2007   2008   2009   2010   2011   2012   2013   2014 (Spring)"}, {"content": "Machine Learning and Computational Statistics, Spring 2014    Machine Learning and Computational Statistics        DS-GA-1003 and CSCI-GA.2567,    Spring 2014                    Overview      Machine learning is an exciting and fast-moving field at     the intersection of computer science, statistics, and     optimization with many recent consumer          applications (e.g., Microsoft Kinect, Google Translate,     Iphone's Siri, digital camera face detection, Netflix     recommendations, Google news). Machine learning and     computational statistics also play a central role in data     science. In this graduate-level class, students will learn     about the theoretical foundations of machine learning and     computational statistics and how to apply these to solve     new problems. This is a required course for the MS in Data     Science and should be taken in the first year of study; it     is also suitable for MS and Ph.D. students in Computer     Science and related fields (see pre-requisites below).           This course is part of a two course series, although it     can be taken individually. The second course, DS-GA-1005:      Inference and Representation , will be offered in     Fall 2014 (see an approximate     list of topics).           For registration information, please contact Varsha      Tiger &lt;varsha.tiger@nyu.edu&gt; or Katie Laugel      &lt;laugel@cs.nyu.edu&gt;.                                General information      Lecture : Tuesdays, 5:10-7pm, in Warren     Weaver Hall 109.      Recitation/Laboratory (required for all students):     Thursdays, 8:10-9pm in Warren     Weaver Hall 109.                                   Instructor :                      Prof. David Sontag                       dsontag {@ | at} cs.nyu.edu          Office hours: Tuesdays, 10-11am in 715 Broadway, 12th floor, Room 1204. [still subject to change]              Lab instructor:                             Yoni       Halpern                             halpern {@ | at} cs.nyu.edu                 Office hours: Thursdays, 9:10-9:40pm in Warren Weaver Hall 109 (right after lab)              Project advisers:                             Dr.        David Rosenberg        Dr.       Kurt Miller        Dr.        Alex Simma                                                       Graders:                             Akshay Kumar, Mick Jermsurawong                              akshaykumar, jj1192 {@ | at} nyu.edu                            Pre-requisites: There are two     different sets of pre-requisites to accommodate both     Computer Science and Data Science MS students. Students     are required to have taken either:           Fundamental Algorithms (CSCI-GA.1170) and      Mathematical Techniques for Computer Science     Applications (CSCI-GA.1180), or      Intro to Data Science (DS-GA-1001) and       Statistical and Mathematical Methods (DS-GA-1002).                Students should be familiar with linear algebra,     probability and statistics, and multi-variable calculus,     in addition to having good programming skills.           Grading : problem sets (45%) + midterm     exam (25%) + project (25%) + participation (5%). Problem Set policy                     Books : No textbook is required     (readings will come from freely available online     material). If an additional reference is desired, a good     option is the following book by Kevin Murphy: Machine     Learning: a Probabilistic Perspective (2012). A good reference on linear algebra and probability is Ernest Davis's Linear Algebra and Probability for Computer Science Applications .        Mailing list: To subscribe to the class list,     follow instructions here .          Project      information                   Schedule               Lecture      Date      Topic      Required reading      Assignments             1     Jan 28         Introduction to learning [ Slides ]       Chapter 1 of Murphy's book     Notes     on perceptron mistake bound (just section 1)         ps1 ( data ) due Feb 6 at 8pm.                 2     Feb 4         Support vector machines (SVMs) [ Slides ]          Notes     on support vector machines   Optional : Second reference on SVM dual and kernel methods (sec. 3-8)   Optional : For more on SVMs, see Hastie, Sections 12.1-12.3 (pg. 435). For more on cross-validation see Hastie, Section 7.10 (pg. 250).        ps2 due Feb 14 at 5pm. [ Solutions ]             3     Feb 11         Kernel methods [ Slides ] Optimization, Mercer's theorem     Notes on linear algebra, convexity, kernels, and Mercer's theorem      Optional: For more advanced kernel methods, see    chapter 3 of this book     (free online from NYU libraries)      ps3 ( data ) due Feb 25 at 3pm.             4     Feb 18         Learning theory [ Slides ]     Notes     on learning theory   Notes     on gap-tolerant classifiers (section 7.1, pg. 29-31)  Pedro Domingos's A     Few Useful Things to Know About Machine Learning                     5     Feb 25         Decision trees [ Slides ] Ensemble methods, Random forests       Mitchell Ch. 3    Hastie et al ., Section 8.7 (bagging)      Optional: Rudin's lecture notes (on decision trees)      Optional: Hastie et al. Chapter 15 (on random    forests)      ps4 ( data ) due Mar 7 at 5pm.             6     March 4         Midterm review Lab: deep learning (guest lecture by Yann LeCun)                       7     March 11      (no class, office hours, or lab March 18/20, Spring break)          Midterm exam  Lab: project advisers          Project proposal , due March 27 at 3pm.             8     March 25         Clustering [ Slides ] K-means, hierarchical, spectral      Hastie et al. , Sections 14.3.6,    14.3.8, 14.3.9, 14.3.12, 14.5.3           Optional: Tutorial     on spectral clustering                    9     April 1         Dimensionality reduction [ Slides ]       Notes      on PCA      More      notes on PCA           Optional: Barber, Chapter 15      Optional: Roweis and Saul, Science 2000 ,      Tenenbaum et al., Science 2000 ,     van der Maaten and Hinton, JMLR '08         ps5 ( data ) due Apr 15 at 3pm.             10     April 8         Bayesian methods [ Slides ] Maximum likelihood estimation, naive Bayes     Notes     on naive Bayes and logistic regression   Optional: Notes on probability and statistics                       11     April 15         Graphical models [ Slides ]          Tutorial     on HMMs      Introduction     to Bayesian networks       ps6 due Apr 28 at 5pm             12     April 22         Unsupervised learning [ Slides ]       Notes     on mixture models                    13     April 29         EM algorithm [ Slides 1 , Slides 2 ] Mixture models, topic models, latent Dirichlet allocation        Notes     on Expectation Maximization      The Expectation     Maximization Algorithm: A short tutorial   Review     article on topic modeling  Explore topic models of: state-of-the-union addresses , literary studies (see also this blog ), evolution of science , Wikipedia                   14     May 6      (no class Tuesday May 13)         Advanced topics       Optional:   Introduction to learning to rank  Joachims' Training Linear SVMs in Linear Time   Slides on collaborative filtering   Slides on victim identification using Bayesian networks ( Video )                  15     Thu. May 15, 7:10-9:40pm          Project presentations (WWH 13th floor)                           Acknowledgements : Many thanks to the University of  Washington, Carnegie Mellon University, UT Dallas, Stanford, UC  Irvine, Princeton, and MIT for sharing material used in slides and  homeworks.                          Reference materials           Machine learning books                  Trevor Hastie, Rob Tibshirani, and Jerry Friedman, Elements of      Statistical Learning , Second Edition, Springer,      2009. (Can be downloaded as PDF file.)       David Barber,            Bayesian                                       Reasoning and Machine Learning ,            Cambridge University Press, 2012. (Can be downloaded      as PDF file.)                 Probability            Chapter 2 of either Murphy or Bishop (see also      Bishop Appendix B)       Review                                        notes from Stanford's machine learning class       Sam Roweis's probability                                        review                 Linear algebra            Bishop Appendix C       Online                                        class from MIT       Review                                        notes from Stanford's machine learning class       Sam Roweis's linear                                        algebra review                 Calculus          Online calculus textbook (MIT Open Courseware)       Bishop Appendix D and E (Lagrange multipliers)       Notes       from MIT on Lagrange multipliers       Dan Klein's Lagrange                                        Multipliers without Permanent Scarring           Optimization                        Convex                                        Optimization by Stephen Boyd and Lieven      Vandenberghe. (Can be downloaded as PDF file.)                                  Problem       Set policy            I expect you to try solving each problem set on your own.    However, when being stuck on a problem, I encourage     you to collaborate with other students in the class, subject    to the following rules:               You may discuss a problem with any student in this     class, and work together on solving it. This can involve     brainstorming and verbally discussing the problem, going     together through possible solutions, but should not      involve one student telling another a complete solution.                Once you solve the homework, you must write up      your solutions on your own , without looking at     other people's write-ups or giving your write-up to     others.                In your solution for each problem, you must write      down the names of any person with whom you     discussed it. This will not affect your grade.                Do not consult solution manuals or other people's     solutions from similar courses.           Late submission policy : During the    semester you are allowed at most two extensions on the    homework assignment. Each extension is for at most 48 hours    and carries a penalty of 25% off your assignment."}, {"content": "NYU Computer Science Department &gt; Machine Learning and Knowledge Representation                                                                                                                                 Search                                                  go                         Location          Contacts      Directions      NYC Information            Admissions          Undergraduate Admissions      Graduate Admissions           People           Faculty        Researchers/Visitors      Administration/Staff      Students: PhD / MS      Alumni / In Memoriam             Research           Research Areas      Tech Reports      Theses: PhD / MS        Faculty Recognition      Student Recognition           Education           Undergraduate Program      Graduate Program : PhD / MS        Courses      Office Hours           News / Events           Colloquia        Calendar: Grad / Undergrad           Job Openings           Faculty Positions             Links           Libraries      Student Organizations      CIMS Computing                                                                                                                                                                               Machine Learning and Knowledge Representation   Edit Title                Edit Body      Faculty    Ernest Davis  Yann LeCun  Mehryar Mohri  Foster Provost  David Sontag    Machine learning is concerned with developing of mathematical foundations and algorithm design needed for computers to learn, that is, to adapt their responses based on information extracted from data. For example, learning algorithms may allow a robot to navigate an unknown environment, improving its performance as it acquires more and more data, or a voice-controlled system to improve its recognition of a person's speech after analysis of a sufficient number of samples. Machine learning techniques draw on many fundamental areas from statistics to theoretical computer science, and are used in a broad variety applications: robotics, speech analysis, finance, computer games, handwriting recognition to name just a few.    Ernest Davis studies the problem of representing commonsense knowledge: that is, the problem of taking the basic knowledge about the real world that is common to all humans; expressing it in a form that is systematic enough to be used by a computer program; and providing the program with techniques for effectively using that knowledge. The primary focus of his work is on spatial and physical reasoning (for example, a formalization of commonsense physical reasoning must capture our intuition about qualitative object behavior under gravity). Other directions of work include reasoning about knowledge, belief, plans, and goals, and their interaction with physical reasoning. Development of formal representations of this type has many potential applications in robotics, product and process design and automated training.    Yann LeCun 's research interests include the fundamental and practical aspects of machine learning. The main goal of his research is to devise methods through which computers can extract knowledge and automatically acquire \"skills\" from massive datasets or from experience. Application of learning to perception is a central theme of his work: how can we teach machines to detect and recognize everyday objects in images, how to teach robots to navigate and avoid obstacles solely from visual input. LeCun's group works on a number of fundamental techniques (energy-based models, \"deep learning\", relational graphical models, and others) which are applied solve problems in computer vision, robotics, image and signal processing, bioinformatics, medical informatics, and economics. LeCun's group also works with the Center for Neural Science on computational models of biological learning.    Mehryar Mohri's primary research areas are machine learning, theory and algorithms, text and speech processing, and computational biology. This includes in particular the study of the theoretical aspects of machine learning, the design of general and accurate learning algorithms, and their applications to large-scale learning problems such as those found in bioinformatics and language processing.    Foster Provost studies data mining, knowledge systems, and machine learning and their alignment with business problems. He has applied advanced technologies to a variety of business problems, including fraud detection and customer contact management. His research directions include robust modeling in the face of imprecision in the business environment, and profiling/monitoring on-line activity.    David Sontag's research interests include theoretical and practical aspects of machine learning and probabilistic inference, with applications to medicine, natural language processing, and information retrieval. In particular, he studies structured prediction, approximate inference in graphical models, unsupervised learning of latent variable models, and computational trade-offs between learning and inference.    Related Web Pages    Computational and Biological Learning Lab  ML Research Group  Machine Learning Seminar          Edit All                   top | contact webmaster"}]}]